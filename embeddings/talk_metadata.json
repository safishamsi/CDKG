[
  {
    "title": "(DataCatalog)_-[poweredBy]-_(KnowledgeGraph)",
    "description": "Your data catalog, ergo your company knowledge, must be powered by a knowledge graph.\n\nBut most organizations don't have a clear picture of their data assets, how they are connected, or how they're being used.\n\nBryon and Juan will discuss why a successful journey from metadata to data to knowledge management depends on a graph foundation that enables flexibility, extensibility and agility in order to future-proof your enterprise data investment.\n\nThey'll share their experiences on building large scale data catalogs and knowledge graphs and how you can get started on this journey.",
    "category": "Knowledge Graphs",
    "transcript": "Hello, and welcome welcome back, everyone, to Knowledge Connections twenty twenty. I'm Jozana Diotis, and, I'm your host here on the Innovators track. And, we keep we keep pushing through with more great talks. We're here today with, Juan Sequeira and Brian Jacob from data dot org. Who are here to talk to us about data catalogs, knowledge graphs, and how these two are connected, and how you can use knowledge graphs to to power your data. The floor is yours, Juan and Brian. Take it away. Great. Thank you very much, George, and we're excited to be here. So, this talk when when we wanted to have a talk here, Brian and I were thinking about something to be having a a a different bold point of view, and and that's the goal of this talk. So, just a little bit about us. My name is Juan Cicada. I'm I'm the principal scientist at data dot world. I've been in this, knowledge graph space, which goes back to the original semantic web vision for almost fifteen years. This is where I started doing my research, and I have, I did my PhD at the University of Texas at Austin. We spun out our research into a company called CapCenta, which we did, kind of pioneered a lot of the semantic graph virtualization on relational databases, developed Grapho, our visual collaborative real time knowledge graph scheme editor. And and we we sold the company. We're acquired by by Brian's company, data dot world, and I've been part of kind of bridging the world between academic and industry, being on many standards committees, and and now working on the property graph schema working group. And, I'll pass it on to Brian to introduce himself. Great. Thanks, Juan. I as Juan said, I'm the the one of the cofounders, the CTO for data dot world. I I think, you know, maybe my story is a little bit similar, but inverted to one's. I, you know, grad school twenty years ago in AI machine learning, but really have spent the last twenty years, you know, starting and working for a number of of large companies kinda more in mainstream data management. And the idea behind data dot world was to really bring the power of semantic web and knowledge graph technology into that world. And so, that's how I got connected up with Juan and ultimately, acquired his company so that, you know, we can kinda work on this together. So And this is a side note I always tell people. Brian at HomeAway, there that was a company that acquired over thirty companies in ten years. So if there's somebody who knows about data integration, it's Brian Jacob. He has integrated over thirty different systems in just ten years. That's people spend integrating one two different systems in just two years. So to take that in account. So this is this is the kind of your typical example that you see. Right? You have your sources. Right? And you have your data producers, the personas, like data engineers and data stewards. They have all this data from all these different sources. And the goal is to be able to integrate this data, to be able to provide it to your consumers of data, like your data analysts and your data scientists who wanna be able to go answer your questions. Right? And there's this big gap. And the typical marketing or or architecture slides, it's that you put a knowledge graph here in the middle. But in reality, what you start seeing in here, there's very specific boxes. Right? There's things like we need to do data governance. We need to have a a business glossary, a data glossary. We need to have data virtualization and federation technology. We need to do data quality. We need to do data security, data preparation, data lineage, data dot dot dot data, more data data. Tomorrow, there's gonna be more data things that we need to go do. And all these different aspects are needed to be able to go integrate data. So just simply kinda slapping on that, oh, a knowledge graph is gonna solve us here. The devil's in the details, and there's a lot of details in here. And this is the big question is how do you figure out what you need? Our position is that you wanna start on two important pillars. The first is a pillar on metadata management, and the next is a pillar on data management. So you want to be able to start build your strong foundation on top of metadata management and data management, And that is our definition of what a data catalog should be. Now a data catalog wants need you need to be able to go do things with quality and security and preparation and so forth. But depending on your use case, data quality may be more important than data lineage. Data preparation might be more important than data security. So that's why you don't need to really think about boiling the ocean and bringing all these things together. You wanna start on these very important pillars and then bring in the other tools that you need. And how do you bring in these other tools? The knowledge graph is what really helps put to put this stuff in together. Because what we're gonna go discuss here is that because everything is driven by connections, and and and we can start putting connecting all these different systems together to ultimately be able to create your knowledge graph. So I've introduced two terms here, data catalog and the knowledge graph. A data catalog is a metadata and data management tool that helps users inventory and organize and access the data within their systems. And the knowledge graph, I know there are just so many different definitions and stuff, and and and this is what I call Juan's, nonscientific, non pedantic definition of a knowledge graph. You wanna do data integration where you're integrating internal and also external data sources. And second is that you want your real world concepts and relationships to be first class citizens, And those concepts end up being nodes in a graph, and the relations that end up being edges in the graph. And that's a simple definition of what a knowledge graph is. So with that, let me go pass it on to Brian because I want him to go tell us why your data catalog should be powered by a knowledge graph. So I'm gonna start by talking about the relationship in this direction. The data catalog is powered by your knowledge graph. So a graph data model is, by definition, flexible and agile. It's about entities and relationships, and it can be open open world in the sense that you can bring in new entities and relationships. So that as you move across your organization, acquire new data sources, uncover new data sources, you can bring those into the model in a flexible way. Data quality, security, lineage, all of these kind of use cases of of a metadata management system are represented in the knowledge graph as relationships between the between entities. And it's you know, as with any application, it's not that you can't build this application on top of tabular, you know, relational SQL databases. It's that when you think about the kind of expansiveness and the openness of this problem by modeling it at where the entities and the relationships are first class citizens, you end up putting the data, the actual meaning of what you're representing first, and you build a more flexible, usable, and and and powerful implementation to power discovery, search, recommendations across those data assets. And and one thing to add here, if you look at other companies, like the Airbnbs of the world, right, that they built their own data catalogs, guess what they built the catalog on? They built it on graphs. Sure. So thinking about the use cases that we talked about for for data catalog, for metadata management system. For when when you think about the first thing you wanna think about is cataloging your data assets. What data assets do exist and how are they related to one another? This is this could be represented by, you know, by go back one one slide one. This represented by, you know, thinking about those database tables and databases as nodes in the graph and the relationship of containment as as a relationship between them. Going forward to discovery, you wanna understand what data assets are related to a topic, what physical assets like this table are related to, what logical concepts like the concept of order from my business glossary or from my taxonomy of my organization. You wanna think about governance. You wanna understand what agents and policies are related to those data assets. You know, again, here is a simple example. Reuben is a person who is the steward of this database. Notice how each of these statements are a real world use case of something I'd like to understand, but they are but but but, ultimately, they are represented by modeling the entities and the relationships as first class citizen just like in any graph based solution. Provenance. How are data assets related to one another and and over time and and and in a in a derivation, in a in a, you know, in a state change representation manner. And collaboration. What people are related to all the data and analytical artifacts? If I want to understand all of the data the the the data assets in my system, it's one thing to have kind of all the relate all the all the data represented in the catalog, but you also wanna understand who are the other people who have additional information, who have actually where is the prior art and how who else can I contact who has actually touched this data or has has a relationship These things, these are all concepts in the real world? Your databases, your reports, your your your policies and people, they're all real world concepts. They contain information about other real world concepts and relationships, but they themselves are concepts that have relationships to one another and the information they represent. Regardless of how you actually implement it, your data catalog is a knowledge graph. It's a graph of of of of of knowledge about these assets, and it should be the starting point for your enterprise knowledge graph that kind of speaks in the domain language of your business. So stepping through those use cases again. You know, first, I represent that this orders table is part of this particular database. I understand that that orders table is related logically to the concept of orders. I understand that Ruben is the steward of the sales database and that there's a net sales table derived from that orders table and a report derived from that, which was actually produced by by Sally. I all of these individual statements are things that are collected together by introspecting and understanding the metadata about about these data assets. And these give you when you ping them bring them all together, a graph over which you can reason and analyze your data infrastructure to understand where where the where where the actual data and know and information about your real world entities lives. So when you build this graph, ultimately, you need a model that helps you make sense of this. Your nodes are gonna represent concepts like data assets, databases, tables, and columns, the agents, topics, etcetera. The edges are gonna represent things like containment, things like like logical relation, things like derivation for provenance and lineage. And so what you end up what you end up with is an ontology that describes these that this this interconnected network of of data assets, of your of of the of the metadata about your data assets. What we what we have built internally is not told you we call Dweck, but, ultimately, it is the majority of that of that is vocabulary from these open standards. It uses things like DCAP for the basic catalog structure, Dublin Core for basic, metadata about digital assets, Provo for lineage, SCOS for concepts, etcetera. The this is the this is the ontology that you start with to make sense of the data assets themselves, and it builds into the next section that Juan is gonna talk about where you actually then use that as the foundation to build your enterprise knowledge graph that's in the language of your of your business. And I'll just kind of close with, you know, thinking about one specific use case and how this kinda really makes it makes it makes it real, makes it visceral. You know, a a common use case for a data catalog is to understand lineage for things like impact analysis. And when you when you do that, what you're really doing is saying, I want to go into the graph that represents all the ways that all of my data assets are interrelated, and I wanna pull out a subgraph that looks at the looks at all of these things by along the the derived from dimension. I wanna basically pull out all of the the ways that things are derived from in a prominent sense so that I look all the way off to the right here and say, I have these several Tableau reports, and I would like to understand where the data that powers these reports come from. That is a query against that graph that says, give me transitively everything from which these things are are derived. This is just one quick example that, you know, I think is is is a common visual implementation in a in a cataloging and and lineage solution to show you kind of how you what you're doing is really querying across this graph database and pulling out an interesting subgraph to address a particular query. With that, I'll hand it back to Juan. Great. So we the point here is that your data catalog is about managing and understanding your metadata. And if you're gonna do that, it ends up being a graph. So it ends up being a knowledge graph itself. But I know everybody here is really excited about knowledge graphs, and we're trying to learn about that. And our position is that your knowledge graph should be powered by a data catalog. So let's think about it. When within organizations, you don't know what data you have. This is the common problem everybody has. Right? We We don't know what data we need to go search for data, so let's go catalog it. Let's go literally crawl and bring in all that metadata together. But when you do that, you wanna be able to start connecting it together to understand how things are connected, everything that Brian just mentioned. Once you start cataloging that data, you wanna go find the data, the next thing is, like, I wanna go access that data. I found the data that I needed. Right? Regard I talked to somebody. I figured out how things were connected. I now wanna go access that data. Let it be give me access to the raw data because it's being sent to me, or I wanna be able to go query that data. And at this point is when you start realizing that I I I I can go query it using data virtualization technology, data federation technology. I wanna take a table in one database and the table in another database and go join them together. Right? We wanna be able to provide users with that quick and easy access to the data. But this continues to be what we call the application centric view of the data. Right? When when we talk about real world enterprise databases that you're cataloging, you're cataloging databases that have hundreds, if not thousands, of tables and thousands, if not ten thousand columns. And at this point, you want I mean, our position is that you want people to realize that their data is a mess, and you want them to kind of crash into the wall and realize, wait. My data I don't cataloging also the usage of the people, and you start realizing what are the different the most important data, assets that people are querying. You can start cataloging also the queries that people are doing, and you realize, wow. People are querying for orders in all these three, four different ways. And when you start talking to people, you realize, wow. You use the word order, and it means three different things to different people. So you start realizing these things once you start cataloging the data. And then when you start cataloging the data and accessing the data as it is, people realize, I don't understand my data. And at that point is when they realize, I need to add my knowledge on top of that data. I don't want to go write these really complicated application centric view queries to get what is in order. I would like to just have a concept called order and just access the data through that concept and get it. But if we I think a tendency that we see a lot is people wanna jump into this knowledge aspect first. We call this to crawl, walk, and run. You wanna be able to first crawl, catalog your data, walk, go access the data, understand how that data is today, and realize that there there are a lot of pains, and I understand it. And when you start cataloging the usage, you start realizing which are the aspects most important to people. And at that stage, you start you can go go into the running phase, which is let's go create add the knowledge to it and go create that knowledge graph. And by doing this crawl, walk, and run, it helps us focus on what people are actually using and needing, and that avoids boiling the ocean. Because this is something that we have seen over and over again, that people want to jump in and create their knowledge graph, and they start building this big knowledge graph with having a clear view of success. So the crawl and walk is what we call the the data catalog, and the knowledge graph aspect is when you're starting to go run, create that knowledge graph, and then actually using that knowledge graph. That's why we say the knowledge graph should be powered by your data catalog in order to be successful. So to get a little bit more of an example, when you're starting to go catalog, this is an example of a real world enterprise database. Right? This is something that you can go catalog and understand how things are what things exist in the real world. Right? This is in your real world enterprise databases. But this is what you if you give this to your data consumers, right, your data scientists, your data analysts, your BI developers, They're gonna scratch their head because they don't understand this, and that's why you go back and forth with your data data producers to understand it. So you wanna be able to go create your knowledge graph as adding that layer of of knowledge, the layer of semantics. So imagine viewing the data this way. You see the main the the real world business concepts, orders, customers, billing address, order lines. Right? A per customer purchases an order and so forth. You wanna be able to go access this data. Access the data in terms of, in terms of this knowledge graph view. But when you're creating that first catalog, you're starting to come up, for example, with the business glossary terms, and that helps you bootstrap what this is going to look like. Now the big differentiation between having this kind of, the the the the knowledge graph view versus what I what we call the application centric view is, if I have this particular question. Right? What are the orders and their net sales in a given time period per their status? To answer that question, I would have to write one SQL query over that database, and it would be a very complicated SQL query. Something that's much harder to to to maintain, something that a small amount of people can be able to understand because they need to know understand the application database. But when you're starting to talk about what I what we call the knowledge and data centric view of it, your queries I'll be I understand they're in graph and sparkle that may be a new thing, but just think about that. The queries are now in terms of the knowledge view. This it's a data centric view of this. And this everything now starts to have a complete lineage. So you if you're asking what is an order, I have the connection of what that concept order is all the way back to the original application database. That's why you wanna be able to start with the metadata first. So another aspect that I really want to push here is that we all think it's just a technology problem. It's not just a technology problem. It's a that's a fallacy. It's about culture too, and I think this is the other aspect that we want you to take away. First of all, we talk about tools, but what about the people? Ask yourself, what does success look like? How do you know if you're going to build a knowledge graph you're being successful? Who is responsible for the data? Assume that, you generate data and and the data is being used for to run some machine learning models to go make a decision, and that and the decision that was made was a failure. It it sales went down. Who's responsible for that? On the contrary, what if sales just increased dramatically because of the data that was generated? Who's responsible gets promoted for that? And, additionally, who does the actual knowledge work? Who understands what these concepts are and understands how they're supposed to what all the different meanings and how they're supposed to be connected to those physical databases? I think this is this is this is again, this is not a problem solved just by technology. This is understanding the people who are involved, understanding who is actually going to be consuming this data and asking them what success looks like. Therefore, we need to be able to understand what are the business needs, the business problems, and tie everything that we're doing with the data to business value. Otherwise, we're just having we're we're we're defining success from a technical point of view. And I think this is one of the key problems. We have been in the same kind of rut in in enterprise data management because we define success from a technical point of view and not from a social business point of view. Bringing the data into one place, into one lake does not solve my problem. It solve the problem is if the business users are able to go answer their questions. That's how we need to go define success. Who is responsible for the data? We we treat software with the respect it deserves. It has a team. It has a manager, a product manager that communicates with the with the with the users of the software to get the requirements and translates those requirements to the engineers and defines tests. And we don't do that for data. We need to have this notion of a data product manager, and they're the ones who are managing the this the datasets that are coming out of that. And who does this knowledge work? We all confuse we're all confused, and then we say that the data scientists, right, are the ones who's supposed to do it, and they complain of the eighty twenty rule. Right? Eighty percent of the data scientists spend their time cleaning the data, and then only twenty percent of the time they do the analysis. And that's true. But now we treat this eighty percent almost with disrespect. We say it's a data janitorial work when it's the crucial knowledge work, that knowledge of what things mean in our company. So this needs to be kind of we need to separate it and make it into its own diff own, role and what we're calling the knowledge scientist. Now people work together, and the question is, how do they work together? What are the processes that people need to go to? How do you avoid avoid boiling the ocean? We don't wanna have the old school waterfall approach. Yeah. How do people work together? Right? We need to have agile agile methodologies, what we're calling agile data governance. And one of the things I always tell people is, like, why do we have brakes in a car? And the immediate response usually is is so we can go slow. It's like, but no. We have brakes in a car so we can drive fast safely. And that's the mentality that we need to go think about when we're thinking about data. And how do people work together? The data product manager needs to set up sprints. We need to think about how to document data. We would never dare release code into a master branch without any comments. Why do we do that for data? We don't we don't document it. We don't document how it's being transformed, the transformations, the queries. We don't do that. We would never dare right now to go release code without anybody, code reviewing it. Right? We do pull requests. People go review our codes. Right? We we even do peer programming. Why don't we do that for data? These are the these are the social aspects that we need to go consider. So to take away, frankly, I think it's just time to transform enterprise data integration. The way we have been integrating data has been the same. So your data catalog powered by a knowledge graph. Your first knowledge graph should be of your metadata, of your tables and columns. Why? Because you need to know what data you have. And by know means you need to have a knowledge. That's what we're talking about. The knowledge graph of your first knowledge graph should be should be of your of your metadata. And then your knowledge graph should be powered by the data catalog. To build your enterprise knowledge graph, it needs to be done to solve business pain points. And I know people are gonna be so tempted to run, to jump and start creating their knowledge graph. Please don't do that. Why not? Because you are basically diving into the deep end, and you will most probably drown, meaning you're probably gonna fail. And what's gonna happen is that when people start looking at this knowledge graph, they're gonna ask, where does this data come from? How do I know I can trust it? And the answer to that, it's the metadata. So that's why you should start with the data catalog as a strong foundation of metadata. Crawl and walk first in order to succeed with your knowledge graph to run. And I know Brian has some final words here. No. I think yep. Just just just as you said, your data catalog is a knowledge graph. That knowledge graph is actually going to be a fundamental and critical part of your enterprise knowledge graph of your enterprise knowledge graph. The the knowledge in your knowledge graph isn't knowledge if you can't kind of state where it comes from. And that is exactly what having a model of your of of your data assets, the metadata about your your your data, your policies, your organization, and how it all hangs together. It is it is it is the foundation of your knowledge graph, and it is fundamentally a part of an integral part of having a a real and powerful knowledge graph for your enterprise. And and one final point is we have to be very honest. We're people who are here who are attending this the this this conference, we're all very excited, but we even though we see Gartner and we see where they're putting knowledge graphs and all that stuff, this is frankly still very early. Right? People are not companies are not knocking on their door on on and say, I want a knowledge graph. They do not have a budget item, the budget line item that says go buy a knowledge graph. They're not there yet. So knowledge graph is still new in the industry. There needs a lot of education for it. So we believe that a way to get knowledge graph in is just focus on a data catalog powered by a knowledge graph to sneak in the knowledge graph within an organization. The value of a data catalog is evident right now to the business. Right? There's magic quadrants for data catalogs. Right? There is a line item to go buy a data catalog. The market is asking for a data catalog now. So if you wanna help the knowledge graph industry grow, start with the data catalog powered by a knowledge graph. And with that, thank you very much. I think, George, you're you're on to moderate questions? Yep. Yeah. Yeah. And, actually, you've been quite active in responding while, you were giving the talk, which is pretty unusual, but also a little bit unfair if I compare to the other people who gave talks because there's two of you. So well done. And, CEO also started the conversation with, Dean Alemanc in the comments, which I've been following with with great interest. And so I'll just relay, the insulators comment, basically. So he's saying for the benefit of, the rest of the people who may not, have the chance to, to to follow the conversation. So he's asking, how do Juan's comments about data governance relate to the fact that we'd like to reuse data from outside organization? We don't know if the external data has undergone the governance that he talks about. Does that mean that we can't use it? And he he hopes not. And it's it's a long comment, and it it includes a number of questions, actually. And then it he concludes by asking how do we reconcile the need to manage data in the way Juan is talking about with the desire to use external data. Yeah. I mean, you know, I can jump in on that. That's what I I think it's a it's a great question. And, you know, I think, you know, the answer is you can you can represent that external data as as concepts and entities. You can represent what governance has taken place if you know about it externally and what governance may have taken place on that internally. Right? You know, the so, folks, something we could ask a lot is, like, how do we deal with data quality? How do you kind of know that only data only quality data is within the within the catalog? And the answer is, well, we we don't, but we do in the sense that data quality isn't an absolute isn't an absolute thing. Right? It's always kind of use case and need dependent. So what you can do is you can represent facts about data. What what what assessments have been made? What ascertainments have been made about this data? And then within a particular context, you can assess you can assert policies, which are what ascertainments need to have been made about this data in order for it to be useful for this this use case. And so the same piece of data might be completely usable for some kinds of research courses purposes within your organization. It might be totally unusable if you're putting together, you know, your s a c SEC filings because it hasn't gone through proper proper vetting. That's not two different pieces of data. It's it's not two different it it is two different ascertainments of quality for different purposes. And and, you know, third party data is is absolutely, you know, part of part of your catalog. You just need to understand where did this data come from and what what governance has has been been you know, I would say, though, like, if you know, to to kind of, like, back to the framing of the question, if you just bring in a catalog of here's a bunch of open data that we haven't really looked at or or or scrutinized or made any ascertainments about, you probably shouldn't use that for most for for most critical use cases because you actually don't you don't actually know anything about about the quality. So you have to really kinda have a what factual quality as ascertained to the name and how does that and how does that relate to the policies of use that I have? So so a couple of things I wanna add. One is you first need to go catalog your own stuff to go know what I have before you wanna go bring other things. Otherwise, you're just, like, throwing stuff in the wall and see what sticks. Second, this should be use case driven. Right? Why why are you going to bring in this other data? What is the problem that you're gonna try to solve with that? So I think that's how you wanna be able to go bring understand that. And then once you realize that I have this particular use case, this is the problem I'm gonna go solve, you're gonna bring in that external data inside, and and then you're gonna have to add the governance to it and figure out, well, can I actually use this or not? Right? I don't I mean, we we're so kind of entrenched now with these magic wands of AI and stuff. It's like, you put people in the room, they won't even agree. Why the heck do we believe that AI is gonna come up with the right answer when the humans won't even agree? We need to stop thinking about that for for for I mean, we need to kinda really put our our feet on the ground on that aspect. Now the I think the the way the market is going to get involved, and this is much bigger than knowledge graphs, is a data marketplace. A data marketplace is going to be a data catalog of your internal and your external data. And you're gonna see and so you're you have data catalogs for for folks to go use it internally in their in their enterprise, and we're gonna start seeing data catalogs of open data that are really starting to add that governance. And guess what? So I think the winners are be are the ones are the data catalogs that are gonna be able to do both. So take a look and watch out for data dot world for sure. And I think I'm looking at one of the chats there, from Janine Hill asking about, the somebody was asking about the ontology, that that we that we were talking about. Frankly, the ontology we have is is is out there. Right? It's it's it's DCAD. It's DC. It's it's all that stuff. We internally have taken all that stuff and added some glue together. Right? We need to go connect that stuff. And I I think part of our plan is to, we're cleaning it all up, and we just wanna go release it. So we will be open sourcing that the Datadog World enterprise catalog. But note that it's all open standards plus a bunch of glue that we just added to that. So we're gonna I think that's something valuable that the that the industry needs. Well, we can't We don't have a paper that discusses the glue, but we probably should. That'd be a good that'd be a good thing for us to do and prepare for the engagement. I am looking at that. We'll get we'll have that published soon. Yep. Okay. So thanks to both of you. So thanks once more, and we're wrapping up, and, see you in a bit. Thanks.",
    "transcript_length": 33723,
    "speaker": "Bryon Jacob",
    "tags": [
      "brian jacob",
      "george",
      "jozana diotis",
      "juan sequeira",
      "rubin",
      "sally"
    ]
  },
  {
    "title": "(DataCatalog)_-[poweredBy]-_(KnowledgeGraph)",
    "description": "Your data catalog, ergo your company knowledge, must be powered by a knowledge graph.\n\nBut most organizations don't have a clear picture of their data assets, how they are connected, or how they're being used.\n\nBryon and Juan will discuss why a successful journey from metadata to data to knowledge management depends on a graph foundation that enables flexibility, extensibility and agility in order to future-proof your enterprise data investment.\n\nThey'll share their experiences on building large scale data catalogs and knowledge graphs and how you can get started on this journey.",
    "category": "Knowledge Graphs",
    "transcript": "Hello, and welcome welcome back, everyone, to Knowledge Connections twenty twenty. I'm Jozana Diotis, and, I'm your host here on the Innovators track. And, we keep we keep pushing through with more great talks. We're here today with, Juan Sequeira and Brian Jacob from data dot org. Who are here to talk to us about data catalogs, knowledge graphs, and how these two are connected, and how you can use knowledge graphs to to power your data. The floor is yours, Juan and Brian. Take it away. Great. Thank you very much, George, and we're excited to be here. So, this talk when when we wanted to have a talk here, Brian and I were thinking about something to be having a a a different bold point of view, and and that's the goal of this talk. So, just a little bit about us. My name is Juan Cicada. I'm I'm the principal scientist at data dot world. I've been in this, knowledge graph space, which goes back to the original semantic web vision for almost fifteen years. This is where I started doing my research, and I have, I did my PhD at the University of Texas at Austin. We spun out our research into a company called CapCenta, which we did, kind of pioneered a lot of the semantic graph virtualization on relational databases, developed Grapho, our visual collaborative real time knowledge graph scheme editor. And and we we sold the company. We're acquired by by Brian's company, data dot world, and I've been part of kind of bridging the world between academic and industry, being on many standards committees, and and now working on the property graph schema working group. And, I'll pass it on to Brian to introduce himself. Great. Thanks, Juan. I as Juan said, I'm the the one of the cofounders, the CTO for data dot world. I I think, you know, maybe my story is a little bit similar, but inverted to one's. I, you know, grad school twenty years ago in AI machine learning, but really have spent the last twenty years, you know, starting and working for a number of of large companies kinda more in mainstream data management. And the idea behind data dot world was to really bring the power of semantic web and knowledge graph technology into that world. And so, that's how I got connected up with Juan and ultimately, acquired his company so that, you know, we can kinda work on this together. So And this is a side note I always tell people. Brian at HomeAway, there that was a company that acquired over thirty companies in ten years. So if there's somebody who knows about data integration, it's Brian Jacob. He has integrated over thirty different systems in just ten years. That's people spend integrating one two different systems in just two years. So to take that in account. So this is this is the kind of your typical example that you see. Right? You have your sources. Right? And you have your data producers, the personas, like data engineers and data stewards. They have all this data from all these different sources. And the goal is to be able to integrate this data, to be able to provide it to your consumers of data, like your data analysts and your data scientists who wanna be able to go answer your questions. Right? And there's this big gap. And the typical marketing or or architecture slides, it's that you put a knowledge graph here in the middle. But in reality, what you start seeing in here, there's very specific boxes. Right? There's things like we need to do data governance. We need to have a a business glossary, a data glossary. We need to have data virtualization and federation technology. We need to do data quality. We need to do data security, data preparation, data lineage, data dot dot dot data, more data data. Tomorrow, there's gonna be more data things that we need to go do. And all these different aspects are needed to be able to go integrate data. So just simply kinda slapping on that, oh, a knowledge graph is gonna solve us here. The devil's in the details, and there's a lot of details in here. And this is the big question is how do you figure out what you need? Our position is that you wanna start on two important pillars. The first is a pillar on metadata management, and the next is a pillar on data management. So you want to be able to start build your strong foundation on top of metadata management and data management, And that is our definition of what a data catalog should be. Now a data catalog wants need you need to be able to go do things with quality and security and preparation and so forth. But depending on your use case, data quality may be more important than data lineage. Data preparation might be more important than data security. So that's why you don't need to really think about boiling the ocean and bringing all these things together. You wanna start on these very important pillars and then bring in the other tools that you need. And how do you bring in these other tools? The knowledge graph is what really helps put to put this stuff in together. Because what we're gonna go discuss here is that because everything is driven by connections, and and and we can start putting connecting all these different systems together to ultimately be able to create your knowledge graph. So I've introduced two terms here, data catalog and the knowledge graph. A data catalog is a metadata and data management tool that helps users inventory and organize and access the data within their systems. And the knowledge graph, I know there are just so many different definitions and stuff, and and and this is what I call Juan's, nonscientific, non pedantic definition of a knowledge graph. You wanna do data integration where you're integrating internal and also external data sources. And second is that you want your real world concepts and relationships to be first class citizens, And those concepts end up being nodes in a graph, and the relations that end up being edges in the graph. And that's a simple definition of what a knowledge graph is. So with that, let me go pass it on to Brian because I want him to go tell us why your data catalog should be powered by a knowledge graph. So I'm gonna start by talking about the relationship in this direction. The data catalog is powered by your knowledge graph. So a graph data model is, by definition, flexible and agile. It's about entities and relationships, and it can be open open world in the sense that you can bring in new entities and relationships. So that as you move across your organization, acquire new data sources, uncover new data sources, you can bring those into the model in a flexible way. Data quality, security, lineage, all of these kind of use cases of of a metadata management system are represented in the knowledge graph as relationships between the between entities. And it's you know, as with any application, it's not that you can't build this application on top of tabular, you know, relational SQL databases. It's that when you think about the kind of expansiveness and the openness of this problem by modeling it at where the entities and the relationships are first class citizens, you end up putting the data, the actual meaning of what you're representing first, and you build a more flexible, usable, and and and powerful implementation to power discovery, search, recommendations across those data assets. And and one thing to add here, if you look at other companies, like the Airbnbs of the world, right, that they built their own data catalogs, guess what they built the catalog on? They built it on graphs. Sure. So thinking about the use cases that we talked about for for data catalog, for metadata management system. For when when you think about the first thing you wanna think about is cataloging your data assets. What data assets do exist and how are they related to one another? This is this could be represented by, you know, by go back one one slide one. This represented by, you know, thinking about those database tables and databases as nodes in the graph and the relationship of containment as as a relationship between them. Going forward to discovery, you wanna understand what data assets are related to a topic, what physical assets like this table are related to, what logical concepts like the concept of order from my business glossary or from my taxonomy of my organization. You wanna think about governance. You wanna understand what agents and policies are related to those data assets. You know, again, here is a simple example. Reuben is a person who is the steward of this database. Notice how each of these statements are a real world use case of something I'd like to understand, but they are but but but, ultimately, they are represented by modeling the entities and the relationships as first class citizen just like in any graph based solution. Provenance. How are data assets related to one another and and over time and and and in a in a derivation, in a in a, you know, in a state change representation manner. And collaboration. What people are related to all the data and analytical artifacts? If I want to understand all of the data the the the data assets in my system, it's one thing to have kind of all the relate all the all the data represented in the catalog, but you also wanna understand who are the other people who have additional information, who have actually where is the prior art and how who else can I contact who has actually touched this data or has has a relationship These things, these are all concepts in the real world? Your databases, your reports, your your your policies and people, they're all real world concepts. They contain information about other real world concepts and relationships, but they themselves are concepts that have relationships to one another and the information they represent. Regardless of how you actually implement it, your data catalog is a knowledge graph. It's a graph of of of of of knowledge about these assets, and it should be the starting point for your enterprise knowledge graph that kind of speaks in the domain language of your business. So stepping through those use cases again. You know, first, I represent that this orders table is part of this particular database. I understand that that orders table is related logically to the concept of orders. I understand that Ruben is the steward of the sales database and that there's a net sales table derived from that orders table and a report derived from that, which was actually produced by by Sally. I all of these individual statements are things that are collected together by introspecting and understanding the metadata about about these data assets. And these give you when you ping them bring them all together, a graph over which you can reason and analyze your data infrastructure to understand where where the where where the actual data and know and information about your real world entities lives. So when you build this graph, ultimately, you need a model that helps you make sense of this. Your nodes are gonna represent concepts like data assets, databases, tables, and columns, the agents, topics, etcetera. The edges are gonna represent things like containment, things like like logical relation, things like derivation for provenance and lineage. And so what you end up what you end up with is an ontology that describes these that this this interconnected network of of data assets, of your of of the of the metadata about your data assets. What we what we have built internally is not told you we call Dweck, but, ultimately, it is the majority of that of that is vocabulary from these open standards. It uses things like DCAP for the basic catalog structure, Dublin Core for basic, metadata about digital assets, Provo for lineage, SCOS for concepts, etcetera. The this is the this is the ontology that you start with to make sense of the data assets themselves, and it builds into the next section that Juan is gonna talk about where you actually then use that as the foundation to build your enterprise knowledge graph that's in the language of your of your business. And I'll just kind of close with, you know, thinking about one specific use case and how this kinda really makes it makes it makes it real, makes it visceral. You know, a a common use case for a data catalog is to understand lineage for things like impact analysis. And when you when you do that, what you're really doing is saying, I want to go into the graph that represents all the ways that all of my data assets are interrelated, and I wanna pull out a subgraph that looks at the looks at all of these things by along the the derived from dimension. I wanna basically pull out all of the the ways that things are derived from in a prominent sense so that I look all the way off to the right here and say, I have these several Tableau reports, and I would like to understand where the data that powers these reports come from. That is a query against that graph that says, give me transitively everything from which these things are are derived. This is just one quick example that, you know, I think is is is a common visual implementation in a in a cataloging and and lineage solution to show you kind of how you what you're doing is really querying across this graph database and pulling out an interesting subgraph to address a particular query. With that, I'll hand it back to Juan. Great. So we the point here is that your data catalog is about managing and understanding your metadata. And if you're gonna do that, it ends up being a graph. So it ends up being a knowledge graph itself. But I know everybody here is really excited about knowledge graphs, and we're trying to learn about that. And our position is that your knowledge graph should be powered by a data catalog. So let's think about it. When within organizations, you don't know what data you have. This is the common problem everybody has. Right? We We don't know what data we need to go search for data, so let's go catalog it. Let's go literally crawl and bring in all that metadata together. But when you do that, you wanna be able to start connecting it together to understand how things are connected, everything that Brian just mentioned. Once you start cataloging that data, you wanna go find the data, the next thing is, like, I wanna go access that data. I found the data that I needed. Right? Regard I talked to somebody. I figured out how things were connected. I now wanna go access that data. Let it be give me access to the raw data because it's being sent to me, or I wanna be able to go query that data. And at this point is when you start realizing that I I I I can go query it using data virtualization technology, data federation technology. I wanna take a table in one database and the table in another database and go join them together. Right? We wanna be able to provide users with that quick and easy access to the data. But this continues to be what we call the application centric view of the data. Right? When when we talk about real world enterprise databases that you're cataloging, you're cataloging databases that have hundreds, if not thousands, of tables and thousands, if not ten thousand columns. And at this point, you want I mean, our position is that you want people to realize that their data is a mess, and you want them to kind of crash into the wall and realize, wait. My data I don't cataloging also the usage of the people, and you start realizing what are the different the most important data, assets that people are querying. You can start cataloging also the queries that people are doing, and you realize, wow. People are querying for orders in all these three, four different ways. And when you start talking to people, you realize, wow. You use the word order, and it means three different things to different people. So you start realizing these things once you start cataloging the data. And then when you start cataloging the data and accessing the data as it is, people realize, I don't understand my data. And at that point is when they realize, I need to add my knowledge on top of that data. I don't want to go write these really complicated application centric view queries to get what is in order. I would like to just have a concept called order and just access the data through that concept and get it. But if we I think a tendency that we see a lot is people wanna jump into this knowledge aspect first. We call this to crawl, walk, and run. You wanna be able to first crawl, catalog your data, walk, go access the data, understand how that data is today, and realize that there there are a lot of pains, and I understand it. And when you start cataloging the usage, you start realizing which are the aspects most important to people. And at that stage, you start you can go go into the running phase, which is let's go create add the knowledge to it and go create that knowledge graph. And by doing this crawl, walk, and run, it helps us focus on what people are actually using and needing, and that avoids boiling the ocean. Because this is something that we have seen over and over again, that people want to jump in and create their knowledge graph, and they start building this big knowledge graph with having a clear view of success. So the crawl and walk is what we call the the data catalog, and the knowledge graph aspect is when you're starting to go run, create that knowledge graph, and then actually using that knowledge graph. That's why we say the knowledge graph should be powered by your data catalog in order to be successful. So to get a little bit more of an example, when you're starting to go catalog, this is an example of a real world enterprise database. Right? This is something that you can go catalog and understand how things are what things exist in the real world. Right? This is in your real world enterprise databases. But this is what you if you give this to your data consumers, right, your data scientists, your data analysts, your BI developers, They're gonna scratch their head because they don't understand this, and that's why you go back and forth with your data data producers to understand it. So you wanna be able to go create your knowledge graph as adding that layer of of knowledge, the layer of semantics. So imagine viewing the data this way. You see the main the the real world business concepts, orders, customers, billing address, order lines. Right? A per customer purchases an order and so forth. You wanna be able to go access this data. Access the data in terms of, in terms of this knowledge graph view. But when you're creating that first catalog, you're starting to come up, for example, with the business glossary terms, and that helps you bootstrap what this is going to look like. Now the big differentiation between having this kind of, the the the the knowledge graph view versus what I what we call the application centric view is, if I have this particular question. Right? What are the orders and their net sales in a given time period per their status? To answer that question, I would have to write one SQL query over that database, and it would be a very complicated SQL query. Something that's much harder to to to maintain, something that a small amount of people can be able to understand because they need to know understand the application database. But when you're starting to talk about what I what we call the knowledge and data centric view of it, your queries I'll be I understand they're in graph and sparkle that may be a new thing, but just think about that. The queries are now in terms of the knowledge view. This it's a data centric view of this. And this everything now starts to have a complete lineage. So you if you're asking what is an order, I have the connection of what that concept order is all the way back to the original application database. That's why you wanna be able to start with the metadata first. So another aspect that I really want to push here is that we all think it's just a technology problem. It's not just a technology problem. It's a that's a fallacy. It's about culture too, and I think this is the other aspect that we want you to take away. First of all, we talk about tools, but what about the people? Ask yourself, what does success look like? How do you know if you're going to build a knowledge graph you're being successful? Who is responsible for the data? Assume that, you generate data and and the data is being used for to run some machine learning models to go make a decision, and that and the decision that was made was a failure. It it sales went down. Who's responsible for that? On the contrary, what if sales just increased dramatically because of the data that was generated? Who's responsible gets promoted for that? And, additionally, who does the actual knowledge work? Who understands what these concepts are and understands how they're supposed to what all the different meanings and how they're supposed to be connected to those physical databases? I think this is this is this is again, this is not a problem solved just by technology. This is understanding the people who are involved, understanding who is actually going to be consuming this data and asking them what success looks like. Therefore, we need to be able to understand what are the business needs, the business problems, and tie everything that we're doing with the data to business value. Otherwise, we're just having we're we're we're defining success from a technical point of view. And I think this is one of the key problems. We have been in the same kind of rut in in enterprise data management because we define success from a technical point of view and not from a social business point of view. Bringing the data into one place, into one lake does not solve my problem. It solve the problem is if the business users are able to go answer their questions. That's how we need to go define success. Who is responsible for the data? We we treat software with the respect it deserves. It has a team. It has a manager, a product manager that communicates with the with the with the users of the software to get the requirements and translates those requirements to the engineers and defines tests. And we don't do that for data. We need to have this notion of a data product manager, and they're the ones who are managing the this the datasets that are coming out of that. And who does this knowledge work? We all confuse we're all confused, and then we say that the data scientists, right, are the ones who's supposed to do it, and they complain of the eighty twenty rule. Right? Eighty percent of the data scientists spend their time cleaning the data, and then only twenty percent of the time they do the analysis. And that's true. But now we treat this eighty percent almost with disrespect. We say it's a data janitorial work when it's the crucial knowledge work, that knowledge of what things mean in our company. So this needs to be kind of we need to separate it and make it into its own diff own, role and what we're calling the knowledge scientist. Now people work together, and the question is, how do they work together? What are the processes that people need to go to? How do you avoid avoid boiling the ocean? We don't wanna have the old school waterfall approach. Yeah. How do people work together? Right? We need to have agile agile methodologies, what we're calling agile data governance. And one of the things I always tell people is, like, why do we have brakes in a car? And the immediate response usually is is so we can go slow. It's like, but no. We have brakes in a car so we can drive fast safely. And that's the mentality that we need to go think about when we're thinking about data. And how do people work together? The data product manager needs to set up sprints. We need to think about how to document data. We would never dare release code into a master branch without any comments. Why do we do that for data? We don't we don't document it. We don't document how it's being transformed, the transformations, the queries. We don't do that. We would never dare right now to go release code without anybody, code reviewing it. Right? We do pull requests. People go review our codes. Right? We we even do peer programming. Why don't we do that for data? These are the these are the social aspects that we need to go consider. So to take away, frankly, I think it's just time to transform enterprise data integration. The way we have been integrating data has been the same. So your data catalog powered by a knowledge graph. Your first knowledge graph should be of your metadata, of your tables and columns. Why? Because you need to know what data you have. And by know means you need to have a knowledge. That's what we're talking about. The knowledge graph of your first knowledge graph should be should be of your of your metadata. And then your knowledge graph should be powered by the data catalog. To build your enterprise knowledge graph, it needs to be done to solve business pain points. And I know people are gonna be so tempted to run, to jump and start creating their knowledge graph. Please don't do that. Why not? Because you are basically diving into the deep end, and you will most probably drown, meaning you're probably gonna fail. And what's gonna happen is that when people start looking at this knowledge graph, they're gonna ask, where does this data come from? How do I know I can trust it? And the answer to that, it's the metadata. So that's why you should start with the data catalog as a strong foundation of metadata. Crawl and walk first in order to succeed with your knowledge graph to run. And I know Brian has some final words here. No. I think yep. Just just just as you said, your data catalog is a knowledge graph. That knowledge graph is actually going to be a fundamental and critical part of your enterprise knowledge graph of your enterprise knowledge graph. The the knowledge in your knowledge graph isn't knowledge if you can't kind of state where it comes from. And that is exactly what having a model of your of of your data assets, the metadata about your your your data, your policies, your organization, and how it all hangs together. It is it is it is the foundation of your knowledge graph, and it is fundamentally a part of an integral part of having a a real and powerful knowledge graph for your enterprise. And and one final point is we have to be very honest. We're people who are here who are attending this the this this conference, we're all very excited, but we even though we see Gartner and we see where they're putting knowledge graphs and all that stuff, this is frankly still very early. Right? People are not companies are not knocking on their door on on and say, I want a knowledge graph. They do not have a budget item, the budget line item that says go buy a knowledge graph. They're not there yet. So knowledge graph is still new in the industry. There needs a lot of education for it. So we believe that a way to get knowledge graph in is just focus on a data catalog powered by a knowledge graph to sneak in the knowledge graph within an organization. The value of a data catalog is evident right now to the business. Right? There's magic quadrants for data catalogs. Right? There is a line item to go buy a data catalog. The market is asking for a data catalog now. So if you wanna help the knowledge graph industry grow, start with the data catalog powered by a knowledge graph. And with that, thank you very much. I think, George, you're you're on to moderate questions? Yep. Yeah. Yeah. And, actually, you've been quite active in responding while, you were giving the talk, which is pretty unusual, but also a little bit unfair if I compare to the other people who gave talks because there's two of you. So well done. And, CEO also started the conversation with, Dean Alemanc in the comments, which I've been following with with great interest. And so I'll just relay, the insulators comment, basically. So he's saying for the benefit of, the rest of the people who may not, have the chance to, to to follow the conversation. So he's asking, how do Juan's comments about data governance relate to the fact that we'd like to reuse data from outside organization? We don't know if the external data has undergone the governance that he talks about. Does that mean that we can't use it? And he he hopes not. And it's it's a long comment, and it it includes a number of questions, actually. And then it he concludes by asking how do we reconcile the need to manage data in the way Juan is talking about with the desire to use external data. Yeah. I mean, you know, I can jump in on that. That's what I I think it's a it's a great question. And, you know, I think, you know, the answer is you can you can represent that external data as as concepts and entities. You can represent what governance has taken place if you know about it externally and what governance may have taken place on that internally. Right? You know, the so, folks, something we could ask a lot is, like, how do we deal with data quality? How do you kind of know that only data only quality data is within the within the catalog? And the answer is, well, we we don't, but we do in the sense that data quality isn't an absolute isn't an absolute thing. Right? It's always kind of use case and need dependent. So what you can do is you can represent facts about data. What what what assessments have been made? What ascertainments have been made about this data? And then within a particular context, you can assess you can assert policies, which are what ascertainments need to have been made about this data in order for it to be useful for this this use case. And so the same piece of data might be completely usable for some kinds of research courses purposes within your organization. It might be totally unusable if you're putting together, you know, your s a c SEC filings because it hasn't gone through proper proper vetting. That's not two different pieces of data. It's it's not two different it it is two different ascertainments of quality for different purposes. And and, you know, third party data is is absolutely, you know, part of part of your catalog. You just need to understand where did this data come from and what what governance has has been been you know, I would say, though, like, if you know, to to kind of, like, back to the framing of the question, if you just bring in a catalog of here's a bunch of open data that we haven't really looked at or or or scrutinized or made any ascertainments about, you probably shouldn't use that for most for for most critical use cases because you actually don't you don't actually know anything about about the quality. So you have to really kinda have a what factual quality as ascertained to the name and how does that and how does that relate to the policies of use that I have? So so a couple of things I wanna add. One is you first need to go catalog your own stuff to go know what I have before you wanna go bring other things. Otherwise, you're just, like, throwing stuff in the wall and see what sticks. Second, this should be use case driven. Right? Why why are you going to bring in this other data? What is the problem that you're gonna try to solve with that? So I think that's how you wanna be able to go bring understand that. And then once you realize that I have this particular use case, this is the problem I'm gonna go solve, you're gonna bring in that external data inside, and and then you're gonna have to add the governance to it and figure out, well, can I actually use this or not? Right? I don't I mean, we we're so kind of entrenched now with these magic wands of AI and stuff. It's like, you put people in the room, they won't even agree. Why the heck do we believe that AI is gonna come up with the right answer when the humans won't even agree? We need to stop thinking about that for for for I mean, we need to kinda really put our our feet on the ground on that aspect. Now the I think the the way the market is going to get involved, and this is much bigger than knowledge graphs, is a data marketplace. A data marketplace is going to be a data catalog of your internal and your external data. And you're gonna see and so you're you have data catalogs for for folks to go use it internally in their in their enterprise, and we're gonna start seeing data catalogs of open data that are really starting to add that governance. And guess what? So I think the winners are be are the ones are the data catalogs that are gonna be able to do both. So take a look and watch out for data dot world for sure. And I think I'm looking at one of the chats there, from Janine Hill asking about, the somebody was asking about the ontology, that that we that we were talking about. Frankly, the ontology we have is is is out there. Right? It's it's it's DCAD. It's DC. It's it's all that stuff. We internally have taken all that stuff and added some glue together. Right? We need to go connect that stuff. And I I think part of our plan is to, we're cleaning it all up, and we just wanna go release it. So we will be open sourcing that the Datadog World enterprise catalog. But note that it's all open standards plus a bunch of glue that we just added to that. So we're gonna I think that's something valuable that the that the industry needs. Well, we can't We don't have a paper that discusses the glue, but we probably should. That'd be a good that'd be a good thing for us to do and prepare for the engagement. I am looking at that. We'll get we'll have that published soon. Yep. Okay. So thanks to both of you. So thanks once more, and we're wrapping up, and, see you in a bit. Thanks.",
    "transcript_length": 33723,
    "speaker": "Juan Sequeda",
    "tags": [
      "brian jacob",
      "george",
      "jozana diotis",
      "juan sequeira",
      "rubin",
      "sally"
    ]
  },
  {
    "title": "Applying Semantic Web Standards for Knowledge Representation at Elsevier",
    "description": "In our talk we will touch on our mission at Elsevier is to deliver data to improve our customer\u2019s efficiency, may they be researchers, students, geo professionals or medical professionals.\n\nTo achieve this goal, we are working simultaneously on keeping our data up to date, accurate and on streamlining the delivery of the knowledge representation structures (KRS) backing our tools and products such as ScienceDirect, - Topic Pages, Engineering Village and more.\n\nWe align our quality assurance (QA) and delivery methods with Semantic Web standards, relying on SHACL, SPARQL for QA, RDF standard serializations for releases. We also align with international vocabulary standards, to deliver standard identifiers to our customers to support interoperability.\n\nTo make our data searchable, we also aim at dereferencing 100% of our entities on an internal platform based on a triple store and have achieved this goal for over 20 KRSs serving 5 different products.\n\nWe also work on mappings and cross-references across the KRSs to increase the efficiency of data management, and boost product interoperability.",
    "category": "Semantic Technology",
    "transcript": "Good morning. Good afternoon. Good evening, everyone. Welcome to the talk, Applying Semantic Web Standards for Knowledge Representation at Elsevier. I'm Veronique Moore, Manager of Taxonomy Development and the slide deck was prepared together with Anna Tordai, who is Senior Architect also at Elsevier. The outline will be quite short. First, I'll walk you through a bird's eye view what Elsevier is, from publisher to technology organizations, so what we used to do and what we are doing now, which will lead to the question, why do we need linked data standards? And then more importantly, what are we doing? We'll link data at Elsevier. I'll start at the global level, and then we'll do a deeper down presentation of how we use it for managing vocabularies. So Elsevier, maybe you know us as a publishing company, and that is still true. As in two thousand eighteen, we were still publishing four hundred seventy thousand articles, throughout the whole year. But even so, if we're publishing articles, we're adding, data. We're adding value to it. For example, the United Nations, sustainable development goals classification. So the articles are tagged, with these different goals, so you can see which articles, which research contributes to sustainable developments. The, funding references, that are also in the articles are tagged also. So a funding body can, look how many articles were published based on a a specific grant, and we add other values. So publishing, yes, but, doing data analytics on top of it. Elsevier is a global company with seven thousand nine hundred employees all over the world. Yeah, people in my team are working from Australia, for example. And our CEO is comes out by asset. So eighty three percent of our revenues right now comes from digital products. In a nutshell, who are we? We are global information analytics business, and we help scientists, researchers, students, doctors, nurses, engineers, a wide range of customers find actual answers to their most critical needs. And, one aspect in which we helped out, the past global pandemic was to build, a coronavirus research hub, where we helped out researchers, get quick and targeted access to, every research that we could tag that was relevant for coronavirus in terms of impact, or in terms of research. And this was declined in a whole set of tools that we provide at Elsevier. Mendeley, you might be aware of for managing your bibliographical records, but it also provides recommenders. So Mendeley data had a special COVID nineteen feature. Scopus and ScienceDirect are a publishing platform, so, you could be accessing the COVID nineteen relevant parts of these huge platforms. HyAxis, for example, is a tool that is targeting on chemical reactions. So if your domain is chemistry and you were interested in research around COVID nineteen within chemical reactions, you could access this spot. So that was into the, coronavirus researcher. The hub now is still available and the papers that were made publicly accessible via this specific hub are still accessible for research. So Elsevierian numbers. I'm not gonna go through all these numbers, but this gives you an idea of the volume of data, information, documents that we're dealing with. And, you can also see some of the different products that we're providing. So Sharepath is for nursing education. It's, a recommended system that helps, nurses get into their journey for, training and getting their, how do you call it, certificates. Clinical key, serves medical doctors, provides them access to clinical overviews and actual answers to questions they might have when facing a patient. So it's not only static data, but it's a lot of dynamic and on the spot data that, we also deliver through our products. And what is the big challenge in research? What is the big challenge for our customers? What do they need? They need data. And one of the big pain points is around data preparation, of course. It's time consuming, computationally heavy. We have lots of data, and data is coming in every single day. It's repetitive. It's not fun. Yeah. But it has to be done. It's like, bunny tight, like, yeah, loving you is a dirty job, but someone's got to do it. And it is brittle, if it's done in an ad hoc manner. What we need then, is to have pipelines that get data through standard processes, but that convert it into a format that is, easily curable because it's understand that it's understood by, the wide set of tools at Elsevier of our data platforms. What we need is standardization. What we need is link data. The same same problem, and, then we can hop on to the, great design model that the link data provides, because here too, there's, enormous number of datasets that appear every day that were created by different people, by different organizations. They talk about different type of datasets. They serve different purposes, but they're all linked. They're they're referenceable. They are standardized, and thus, they can be interoperable and standard QA. So quality assurance tools can be applied throughout. And pipelines can be designed to fit the global unified model instead of having, a processing So for that, the architecture team where Anatolia is, designed the Elsevier Linked Data Standard, which is a place where you can find references to, existing w three c, so World War Web Consortium, recommendations that we're using. It defines when we're using the recommendations as is and when, some specific model has been extended for our needs, because yeah. You'll see in the next slide. First, we start with the w three c models. So we're using a shackle, and it's a huge expressive power, for restricting and helping out auto populating some of the metadata around the vocabularies. In the management system, we use PROV and PAV to define the provenance of data, that we have there. So entities, it can be the provenance of a vocabulary, a provenance of a paper, a provenance of a given concept in a specific graph. So we can go at all sorts of different levels. But because we're using standard vocabularies, the semantics is defined in a standard way, and we can also interact with vocabularies that are external datasets that are external, and we know that they will be compatible because they use the same semantics. Vocabularies that I mentioned once or twice already, are designed in Scos and more specifically in SCOS Excel. And the annotations, that we produce follow the web annotation data model. So we're compatible and data can be used, by people who use other linked data. We can use other linked data, and this way we can interact with the latest progress in science. Because we have lots of very specific types of data, Elsevier has designed an Elsevier data model to be able to express information and relationships for these different types of entities. So here you see a quick snapshot, around reactions that would be for tools like reaccess, clinical trials, treatments, diseases, because, yeah, Elsevier focuses a lot on the health side of things, your drugs, patients. But there's also another side, which is, represented by funders, researchers, research institutions. So researchers and funding institutions are part of the people, we need to express information for, because they're interested in outcomes of what has been published. So that's part of metadata we add using standards. And a side that's not present in this slide is everything related to engineering and sustainable development, renewable energy, which is a big, growing facet. We have Vocabrio's ecosystem to support, these different products to support our customers, and we have a dozen vocabulary management teams across the globe. They are not all, yeah. It's not just one big team that serves everybody across the globe, but we have different teams that are spread up. And the way to, be able to interact and share not only vocabularies, sources, but also processes is to have a link data again and follow the same standards. So we follow the Elsevier data model, which is linked to international IDF standards. And that way, we can grow our data but benefit from each other also. So, we have two big knowledge graphs. We have thirty vocabularies and, of course, these vocabularies have some overlap and, of course, some new products, projects, proof of concepts, development need, new vocabularies to be built quickly. And, because we link them and we use the same semantics and properties across them, then we can spin off, yeah, some quick vocabulary or subgraph or branches based on what we already have and can share across the globe. The use cases that we serve are quite varied. It's basically anything you can do with the vocabulary. So browsing, autocomplete in Mendeley data, for example. Recommender in ScienceDirect, Mendeley. Classification, I was mentioning the, United Nation sustainable goals classification. There's also topic classification. We do annotation and search. And I will, talk more about annotation in a minute when I'll show you an actual tool and, yeah, an actual product that we produce, which is the ScienceDirect topic pages. So we have time for it, a little spotlight. What is a topic page? A topic page is, some sort of like a Wikipedia if you want but for scientific concepts. The idea is to give a researcher who doesn't know much about one specific notion a definition if we have it, related concepts or some sort of, yeah, cloud of related notions and relevant snippets from our content so that they could grasp what this notion is about and get resources to do a deep dive from there if they're interested in learning more. So, for example, you can access it using a Google search. So what is the cosine similarity distance in NLP gives you as first results, topic page or at least did at the time of this screenshot. Not certain it's still the case. From our content, from ScienceDirect, when you get an article, there's some annotation that happened, and, the hyperlink redirects you to the topic page. This is what it looks like. So here a definition would appear if it would have one for cosine similarity. And as you can see, the process is still a work in progress. So, we do try to populate definitions for as many pages as we can, and we are investigating more resources, yeah, and more sources for filling them in. There's a set of related terms, and here are the snippets I mentioned. So if you go down the page, you get book chapters that you can read, that gives you some information about what cosine similarity is, what its use, and there is a feedback box. So for example, here the feedback could be, there's no definition. I don't need a topic page if there's no definition. So that's good feedback for us to be spotting what are the critical places where we would need to, add data or information. In the back end, how does it work? Well, it works based on linked data because we need to, have a taxonomy, annotating all of science direct publications, but also book publication and all sorts of other tools. We also integrate with high access reactions. So on some topic pages, you can see reactions. We integrate with clinical key, to have medical recommendations also show up in topic pages. So there's an interaction, for a whole range of document type that used to be serving different products. So if you keep all the documents separate in the way they're processed, if you keep all the documents targeted to one single product, then you cannot build such an aggregation . So there is data mining from different sources that are aggregated using relevancy ranking to get the, topic page having the most, useful information, focusing on entry point information, about a given topic. And then there are automated scripts, but also a thorough SME review of the topic pages after every release. There's sorry. There's a quarterly release, of topic pages. So, four times a year, you get a new set of topics that are published. And the topic pages are using OmniSciences as a source for annotation. The OmniSciences is a Scos Excel vocabulary that was started in two thousand fourteen. Its first, use case, like, its first goal, was to link the different standards that we were already using for annotating content at Elsevier and, that needed harmonization, but also to be able to do, cross platform m and gams. So we created, we created Omniscience for that first purpose, but we didn't base it on the Elsevier standards themselves. We build it on linked data. So we used, DBpedia anthologies to see how the international community in research was representing the world of research. We use some principles as atomization of concepts, so that we could link out two vocabularies that regroup some entities in different ways. So for example, if Elsevier were classifying, articles, in agriculture and agrobiology and food sciences as one topic. We were creating three different topics to link out to these because other vocabularies might regroup fisheries together with, other concepts. So we didn't want to be relying on one specific grouping, but we wanted to have, only atomic entries that we could link out for making end to end crosswalks. As I said it's using ScosXcels. ScosXcel also for mappings. We keep external unique identifiers to be able to get back to the references , and we use provenance and, DCT source information to link our vocabularies and and keep in our vocabularies as much metadata as possible that we could use for either making a subset based on copyrights, of course, or, yeah, making crosswalks between two specific resources. So Omniscience was created as a bridge, and then it expanded. We needed one single single vocabulary for, the paper submission system, which at time was revised. And so we added to, the top levels that we had created using only broad levels of science scientific description. We added all the controlled vocabularies that we use for all the, individual journal submission systems that was now gathered into one. We went from, four thousand concept to fifty five thousand in two years. And now for, building the topic pages, willing that to a piece that is built on automatically extracted concepts from the back of the book index, from glossaries, and and other resources, and we're up to seven hundred thousand concepts. We use for crosswalks, classification, annotation, and recommendation. And, well, these are the fields of science that I covered. So medicine still represents twenty percent of omniscience because our content is still mostly targeting, yeah, health sciences and life science. But engineering and renewable energy or energy in general are moving into the, the proportion that went from one percent to, yeah, like seven percent or five percent. And I'm sure that it's still that the strand is still gonna, be followed. So the main challenge is how to grow, the vocabulary and keep it accurate because we want the topic pages to be, representing truthful information, but we want the topic pages to be on the latest, topics of research. So we need to have the latest information, and we need to be sure that it's put at the right place with the right set of, synonyms and the right set of relationship to make sure that the topic page wouldn't be, yeah, incorrect and not that useful for our customers. So we grow topic, we grow OmniScience based on, customers' communities, so mostly by, getting updates from, public vocabularies that are updated clearly or at a higher frequency. So we keep up to date with what most scientists are using as reference vocabularies. We, get feedback from our customers' needs based on search logs, and we try to make sure that these are covered. We also have system of extraction for the, trending and influential topics that are automatically extracted, from last month's research and curated by SMEs and placed at the proper place in the taxonomy. That would be a topic on its own to see how can we streamline the process of injection of new entries. And then product project needs. So, as I mentioned, topic page that I was doing in interacted with, Reaxys. So for this, linking between, two products or projects or use cases, we had to expand the taxonomy in the chemistry domain. So based on the, the global, products that we're serving, policy or roadmaps, let's say, we expand one branch or another. But for all of this, what we really want to do is to deliver data that is high quality data. So how do we make sure that we do have this? Because we are using linked data standards, we can rely on standard ways of QA ing. So whenever one of the twelve, taxonomy team develop a specific QA mechanism, we can apply it to other vocabularies also, And that saves us a lot of time and, and that allows us to share information and to produce standard standard reports also, not only standard QA procedure, QA pipelines, but but reports because we build it on the same tools and on the same models. So the taxonomy management system is based on a triple store and, so we're using RDF, SCOSXL, for representation. We're using Shackle's, expressiveness. For example, we have, regular expression based way of checking that a CAS number attached to a concept that is, a chemical entity is a correct CAS number. So if you're doing a typo, the system will tell it to you, and it's based on standard linked data technology. We use SPARQL for producing reports, subsets, and doing release type of global QA. We're using, yeah, pandas and an extension of pandas to doing batch processes because it's all tables and RDF triple based. We try to reuse as much as possible because we can do faster life cycles now that our taxonomies are not dedicated to a given product, but dedicated to some branches with a lot of metadata that allow, modularization and rebuild. And the publication is done also through a triple store where the, different customers can pick up the latest release. And, because we are publishing as a trig, so, all the sparkle and shackle, QA validation rules are part of the data model that is sent to the publication triple stored together with the data in a trig file, then everybody down the line has a self sustainable, unit of information that gives the semantics of the data together with the data. The publishing triple store has dereferencing and browsing UI for manual use, but also, of course, a lot of APIs and a sparkle endpoint for automatic interaction. So that is a glimpse of what we're doing with linked data at Elsevier, and I thank you for your attention. I'll be in the room for more questions. Thank you very much.",
    "transcript_length": 18594,
    "speaker": "Veronique Moore",
    "tags": [
      "Clinical Key",
      "DBpedia",
      "Elsevier data model",
      "HyAxis",
      "Mendeley",
      "Omnisciences",
      "PAV",
      "PROV",
      "RDF",
      "SCOS"
    ]
  },
  {
    "title": "DBpedia Databus: A platform to evolve knowledge and AI from versioned web files",
    "description": "Most existing systems employ a software-driven workflow to achieve their development goals. They ignore data engineering completely. This is a fatal mistake, when it comes to knowledge intensive development projects such as AI.\n\nWhile a plethora of tools is available to manage software issues, around 80% of the effort in AI goes into data engineering with hardly any tool support for acquisition, data quality curation and validation or integration.\n\nIn the presentation, we elaborate on the agile data engineering methodology that is supported by the [DBpedia Databus] (http://databus.dbpedia.org).\n\nIn the theoretical part, we will introduce the notion of abstract datasets that follow the Maven model and show how rich metadata can be exploited to structure web files in an interoperable way.\n\nTowards the end, we will present practical use cases that have been realized with the DBpedia Databus and introduce some very useful tools for knowledge engineers that speed up the data massaging for AI by a factor of 50.",
    "category": "Knowledge Graphs",
    "transcript": "Is one of the biggest and more important knowledge graphs around and has been for the last decade or so, if my memory serves me right. It has been a knowledge graph, before actually knowledge graphs were were to hike or before even they were to think. So they started, scraping knowledge from, from Wikipedia. And to do that they used some structure in Wikipedia and by doing that they extracted a big amount of knowledge and they have been building on that and creating a whole ecosystem around it. And this is actually what Sebastian is here to talk about today, the latest addition to this ecosystem and how they're using it and, I guess, how they you can also use it. So that's that's it, from, from my part. And for the rest, Sebastian, the floor is yours, and take it away. Thank you for the introduction, George. So today, I'm talking about the d DBPedia data bus, in particular, a platform to evolve knowledge and AI from version web files. This is a particular particular aspect which goes into data and knowledge engineering. So it it is it is about the engineering methodology. A part of it is about the engineering methodology behind it. So you would get some some knowledge engineering details and processes here, and then about the platform which provides the tool support for this methodology. So for for all of you who do not know DBPedia so well, I've prepared a short introduction. So starting from the left, you can see that two thousand and seven was this first Wikipedia extraction thirteen years ago with the four partners, and there was a functioning sparkle endpoint, which is stably available publicly till today, and also linked data deployment. Then soon after, basically, in the same year, there was this formation of the linked data cloud around DBPedia. Trend continues, and, so so then there was a major boost in knowledge graph and linking research. We opened the editing of the DVPD ontology in two thousand ten, which was labeled a new type of psych. Two thousand eleven, there was major industry so IBM Watson was built, for example, using DVPD. Yahoo used the the software. BBC included the identifier. Park was given to Unicode. Then, two thousand twelve and two thousand sixteen, we extended this extraction framework to cover all hundred to hundred forty Wikipedia's, comments, and also wiki data. And at this time, fourteen point four billion facts were extracted. In two thousand fourteen, we founded the DBPedia Association, which I'm the CEO of. Two thousand sixteen, we had we were try starting to tackle data quality on a larger scale. That's why we developed the Shackle web standard, which allows test driven knowledge graph development. Two thousand eighteen, this, the linked data cloud has grown a lot. And, starting two thousand nineteen, we really built DBPedia into an innovation platform. So it's not not really about the dataset anymore, but about the connect connecting of the data, linked data technology, and the ecosystem. Now in the future, we we kind of like raise the the productivity. I'll talk about this. So there are twenty two billion facts per month. This is one thing, so it's monthly releases. And we have this huge link data derived open knowledge graph, so we kind of, like, increase the speed of integration very much and are able to produce this huge knowledge graph. And also, we started to do FAIR link data. So FAIR, most of you might know it, but it means findable, accessible, interoperable, and reusable. And I'll talk about these developments in twenty twenty on the next slides. So what does FAIR linked data mean? FAIR originally comes from a scientific consortium. So there are guiding principles for scientific data management and stewardship. Personally, I always wondered these these guidelines, they are very vague, high level, idealistic, also altruistic, and not at all industrial. So what we're doing with FAIR linked data now is that we provide a practical practical implementation of FAIR. So there's lowered effort because you can reuse the implementation. We have measurable automated FAIR tests, so you can be sure that your data is FAIR, and the whole thing is industry driven. Now this, there are many things in this FAIR guiding principles that are industrially relevant. For example, findable, there is also a marketing aspect, but also internal data management aspect that you want to find data in in your enterprise. Yeah. Because it's very distributed across your enterprise. Interoperability is, of course, a great issue and reusability is is a great cost saver. Accessibility, goes along with privacy concerns and who's allowed to see data. So this has a different flavor than the the original FAIR guiding principles. I would like to introduce you the DVPDIA Association members up to now. So we are very fast growing knowledge engineering and linked data lobby. Part of it is to to give the visibility that the EPR has back to the people who contributed and build it and really drive this knowledge graph adoption. Let's I I would like to answer the question whether DBPedia is academic and is industrial is often, quite a misconception. So it's it's neither and it's both. Because we focus on knowledge engineering, and engineering is always taking scientific methods to produce industrial output. So more on the left side, we put the academic and nonprofit and public members, and then on the right side, we put the industrial members. Yeah. So you can, kind of, like, see this this, knowledge engineering methodology here or process here. I'll talk about some, economical concepts just to give a background motivation why why we are doing this. Some years ago, we had an aligned project that was supposed to align software and data engineering. We also published a book, Engineering Agile Big Data Systems. And if you do a system analysis of your data intensive workflow, you have these three properties which are agility, productivity, and quality. And here I've plotted basically the release cycle of DVPDR in two thousand sixteen and two thousand twenty. So two thousand sixteen, we had kind of like this entire pattern. It is called data quality creep. So we were always focusing on data quality and totally ignoring agility and productivity, which means if you if you follow if you go into this entire pattern of data quality creep, your releases will be delayed a lot. And this means you you use agility and you use, net output. Yeah. So productivity. This is an automatic triangle here. We lost a bit of quality because, of course, we we reengineered the whole pipeline. And during this basically, during this refactoring process, I think there are minor new bugs. Yeah. So we lost a bit of quality, but we kind of like increased agility and productivity a lot. So the last release cycle was seventeen months, and now it's monthly, and you can really add fix the pipeline better. Another economic concept is the law of diminishing returns. So if you have an economic background, you already know what this means. This measures the productivity per unit. So if you add more workers to, so it's the production factor is labor here. If you add more workers, then you, at some point, you're you're getting more productive. But then if you add more workers to it, you kind of, like, have diminishing returns. That means each worker is not so effective anymore. And then if you will continue this, let's say you want more and more data quality and you add more and more people checking this data quality, you will even get into negative returns. That the main reason here is that data quality is Pareto efficient. So there's a twenty eighty rule applies. And if you increase in quantity, you lower quality as well. And also the increase in quality makes it harder to find errors here. So as a take home message, if you ever print the slides, you can cut this out with the scissors. If you want, you you should always try to tackle data quality issues with innovation, not with more community or workforce. Yeah. So because of if you double the size of your data curation community or you have more edits or more activity, it doesn't mean that you are more productive in any sense. So this is basically the take home message and we have we focus here on innovation. Now I'll come to the core. I'll introduce some data bus concepts and, let me see what's that. No. Data bus concepts and use cases. It's in the mix, so there will be high level concepts, and also use cases and practical demos, which show these. So what's the data bus? Data bus is a digital factory platform. The basic process on the DataBus is you find data, you consume data, you process data, then you republish it. This is a very minimal, minimal building block. There's this online platform, data bus dot d b p dot org, and it's literally built like a like a data bus in in your laptop, for example. So in computer architecture, bus is a communication system that transfers data between components inside a computer or between computers. And here, of course, we're talking in the in the server. It was inspired by the Maven repository, Steam, GitHub, and also the APT package manager on Linux. So these are all software deployment systems, and we focus you on on data data consume and deployment. What you do with this basic building block is that you can build networks, and not a pipe not a static pipeline. So as soon as you, build pipelines and really hard integrate, let me see. Another question. As soon as you build hard pipelines, you lose a lot of flexibility and agility, and you cannot test the individual data output. So what you see here is that the metadata, on the top the top blue line is the data bars. You query the metadata for the files, and then you download the data from from somewhere else. So it gives you kind of like the location of the files on the web, and then you can load it into your data cleaning service, into a service, into some sort of model. And you produce output data, and this output data, you can upload the metadata on the data bus again. So it's basically orchestration of input and output between software. So how can you imagine a digital black factory platform? On the right, you see a warehouse with a lot of boxes. You kind of, like, need to know what's in the boxes, where they are going, which box you need. So the boxes here are files. So it's the registry of files on the web, which means that, any files reachable via HTTP, can be described with this metadata. Doesn't matter with the whether it's RDF, XML, JSON, PDF. It's the central storage, and the central metadata index is available via Sparkler API. So there's a knowledge graph as well. Right now on the public platform, there are a hundred thirty thousand files and two point six terabyte, and you can download all of it. So there's no restriction. It's all open. It's open data. Maybe you don't want all of it, then you need to select it and drill down with the SPARQL API, and it's also a FAIR implementation. So if you publish files on the data bus, we guarantee that you, follow the FAIR principles. So what is the core of it? There's very strict metadata. So this part is very well, well checked. So provenance, we allow users, organizations, and agents to publish data. And, of course, you have, datasets, source datasets. When your dataset is derived, you can link to the source datasets. Yeah. So this provenance, machine readable licenses is required. Everything is signed with a private key and the x five zero nine certificate. And we have, dataset identity and versioning, which is a bit different from DCAD. So there's a gap in the DCAD standard. Here, you the dataset is abstract. And then each version you have versions per dataset. Yeah. So I'll show that in the next example. Those structure, like the links, are user group artifact version. If you know Maven, you already, you're familiar with this. And I'll show you one popular dataset of DBpedia. So this is just a very tiny dataset. So these are geo coordinators extracted with mappings. That means from the English Wikipedia, you have a lot of from, you know, from all Wiki from from all major Wikipedias, it extracts the geo coordinates and put them into the file. You can see if you go on the versions that so this is the abstract data set. Yeah. And then each month, we produce a new version based on the Wikipedia dump of this particular dataset. You have a a schema, basically, for the files. This is important. So you can drill down on the individual datasets. So now you just want to select the English files. And, if you want to subscribe to it, there is a query generation functionality. So each month now, if you execute this query, you will get this this particular new dataset, just the English version of it. Yeah. If you click on a version here, there's more there's more much more documentation available and also links to the codes, and you can report errors and discuss. So there's interactivity features. There's also the link to download the metadata. So this means you can automate these downloads totally. The next thing is so if this gets more complicated because we are, the the core collection, which I will show, it has several hundred files per month. So here, users data bus users can create their own data catalogs, which we call collections. So these collections are stable they have stable identifiers, so you can put them in a scientific paper, for example, and they are dynamically updated or they can be static. Yeah. So I'm going to show you the latest core release here, which is basically what we'll then load into the main sparkle endpoint, which you all know. So here's a simple batch script to fetch the data. I hope loads loads now. Yeah. But anyhow, the the collection is backed by a query, which is can get quite long. Yeah. So the sparkle endpoint endpoint is quite good to handle this. And let's see. Yeah. So here you can see all the the datasets. Basically, each month or each day, you can check whether there are updates to this collection and then kind of, like, replicate the DBPedia sparkle endpoint on your local machine. And you cannot create your own collections via the the if you are registered. So what can you do with the collections? There is this thing which we call, it's like rapid application deployment because we need to maintain the DBPEDI infrastructure, the core infrastructure. So as I showed before, these collections can be loaded programmatically. And if you pair this with with Docker, you get a low code application deployment. So we have this Docker image, which is a virtual sparkle endpoint quick start. And, here, it is a three liner. You just feed it the collection DPPR, and then you need to restart it and it checks the updates for the collections. Now let's talk about so this was the download and action part. Now I'm going to talk about this data engineering in action. So, this part, I I do not have detailed slides here, but kind of like, if you ask what is an application and, of course, I mean, data intensive applications here. So AI, search, many things that are fed fed by a lot of data, chatbot maybe or question answering systems, NLP as well. So you have an application, that you are developing, and this application here always needs software and data. So these are the two components that you that you actually need to have a working application. Now if you think about the software engineering part, there, you have very good frameworks for it. You have Maven, you have NPM, you have the DBN package manager, you have, Scala build tools, the SPT, for example. And there you can really pinpoint, the exact software dependencies that you need. They're recursively resolved and you know kind of like for each software artifact you include, you know exactly which version it is, whether there are compatibility issues, whether things changed. You have a build tool that runs integration tests normally. So there are many, many tools there. But when you when you think about data, so if you present your application to somebody and I would ask you, can you give me a list of datasets included and where you got them and, like, a record on what's what are the dependencies and everything, then normally, this is done without any tools. Yeah. So and here I see the great, great gap. So I prepared some homework for you also. If you compare your software engineering processes and tools with your data engineering processes and tools, so how you track data and develop data and everything, do you notice a difference? And you can think about it, and you will see that there's a or I assume that you will find a great gap. Yeah. Because from my experience, I was not so happy the last ten years working with data and the tools provided to work with data. Now the main thing also may the main incentive to for us to develop the data bus is that we want to do knowledge graph engineering. This means, in this case, it's a simple example. For us, it's a simple example. So it it mainly shows the dVPDR release process. And we have this fully automated, it's in the Marvin port, so there's fifty five gigabyte per release, around twenty twenty one billion triples per month and five thousand triples per second. There's also a dashboard that tracks the process. And on the bottom, you can see the the the, debugging well, release and debugging workflow. So the the on the left top, you see that we, we take the time based Wikipedia dump each month, and then there's a time schedule extraction. This is uploaded on the data bus. And then there is the state data cleansing step, which is another component. And in the end, there are this downloads it and republishes it as quality control DBPR release groups. What we do then is, there is this large scale validation. So we run yet other tests on the final thing, and this produces reports. And here, this is the kind of like the point of truth is this community reviewing. Yeah. Because we just produce the data, but then it gets downloaded six hundred thousand times per year, and the errors are only shown in the final applications. Yeah. So if you load it in the sparkle endpoint, if people parse it, whatever. Yeah. So and then the the problem here is always to backtrack, the errors and the issues to the specific component. And this is something which is possible in the data bus because we reach this unity between data package and also piece of code and software to which will which is responsible for this particular data part. And you can break down the the tests these data tests. You can put them into the software integration tests. So here, we have kind of, like, achieved this alignment between software and data engineering and the backtracking of the of the data issues, in the opposite direction of the data. Now here, I'm go just going to show this real short. So here's a more complex, knowledge graph engineering workflow. It is more complex because it's iterative. Because here, it's not so straightforward. So it's not like it's not pipeline like a pipeline. So the release workflow is more like a pipeline and then you backtrack for the errors. Here, the individual components use datasets of other, components, but then also improve them, and, you have to reload the other components. So it's like an iterative approach. So the the borders between consumption and publishing are are kind of, like, not so not so linear. Yeah. It's more network like, and this is only internally. What we are what we're doing with this is, we're deriving huge knowledge graph from linked data. So here you can see an entity size comparison of the knowledge graphs we are building. So, the DBPedia you you know and you are used to is on the left side. Yeah. This is the English tiny diamond. Diamond because it's compressed data from several sources. We recently built a Dutch national knowledge graph from Dutch linked data sources. Then you have, as a comparison, you have items in Wikidata. And right next to it, there's this DBPedia small diamond, which compresses all language versions of Wikipedia plus Wikidata in in one, yeah, like a diamond knowledge graph. We'll build more national knowledge graphs in the coming years, like the German one or maybe the US one. If you're interested, please mail me. And, on the then the there comes the largest diamond, which is kind of like all datasets that we integrated up to now, compressed into one huge knowledge graph that you can download. So here, we increased the speed a lot. So we we need to right now, we need around four hours per linked data cloud per linked data bubble, and we are going to crowdsource this so everybody can integrate their data into the PDF. Then we have these two commercial graphs, the Google Knowledge Graph for comparison. So this is log scale and the DeepPort knowledge graph. And then we have, of course, the linked open data cloud, which is the largest knowledge graph on Earth, I think, compared to anything else. There's there's more data available there, but not findable. That is a problem. Yeah. So we need to book up the infrastructure that can really find, the particular data that you need from the LOB cloud in life and also manage the changing of the data. So now I I want to show you a data bus application. So previously, we focused on the consumer, the download, and also developing methodology with the data bus. But you can also build applications which are powered directly by the data bus. And I would say this is a unique selling point because this is not possible with other data repositories. So other data repositories, you can kind of, like, keep the or organize the files a bit, but it's very hard to build real applications on the repository itself as a platform. So this is DBP we call it DBPedia archival. So it's an ontology archive. I will show it real quick. So what we do is we crawl eight hundred. So at the moment, we discovered and crawled eight hundred ninety ontologies, and then we do a star rating. So here you can have a list of ontologies and filter them. Maybe some audience want to see their ontology. Maybe it's we don't have time to wait till somebody suggest one. But, yeah, you can try it out later how this, the star rating is done. So so what it does is, basically, this this archival tool discovers and tracks all ontologies and every eight hours, it downloads a version of the ontology onto the data bus, bus, which is persistently saved. So this means you can now use the data bus to download and, the ontologies and the individual versions. So if you click on let's wait. Let's go for the latest time stamp to see which ontologies have been edited recently. So here, this one, you can see that here every time a change is detected, the ontology is downloaded and persisted, and then we check for parsability here. For example, if you only have one star, then most likely the license is missing. Yeah. And then a consistency check. So these are the the four stars. So these are these are basically stars which are needed. So, you know, your ontology is is fair and available. It doesn't say anything about the quality, just that you can find and access the ontology and that it has a license attached. So there are, in the back end, you have also programmatic access ontology versions. And this is kind of like an application build on the on the data bus itself. Where two things where I have no time, which I just want to mention shortly. So there's also a data bus client, which is quite practically because it converts format and compression on download. So it's a bit different than content negotiation. Content negotiation does the conversion server side. And here, you can the the client kind of, like, simplifies g zed g zed g zed to b zed to conversion or things like that. So it's quite practical to homogenize the collections. And there's also data bus mods, which automate the enrichment of the metadata. So this is like so users don't have to give something like a MIME type because this can be automatically detected or file size or, what many other things, void generation, is done automatically. About the data bus, the usage is free if you register, and we are preparing on-site deploy on-site deployments for the next year beginning of two thousand twenty one, so you can have a more closed architecture. Yeah. Thank you for your attention. Are there any questions? Oliver is asking if there's a user review process. And, for example, what happens if you detect a bug in the Wikipedia data? Okay. So this is about the, the data quality. Yes? I think so. Yep. So, how does it work? So, normally, you would submit an issue in the in the data in the in the GitHub. Yeah? So this issue is kind of like not semantic yet, or actually where the users kind of like, they know already how the data bus works and how to pinpoint the error. But, so let's say you you have you are not familiar with anything. You say, okay. I found this is wrong, and then you post the issue. And what you do what we do then is we tag it with the particular artifact on the data bus. So we say, in this version, we confirm it in this version of this particular artifact. The error occurs. From there, we device tests. So there's a mini dump where we can implement software tests, And these are in the dev branch. So on the dev branch, the tests always fail. And then as soon as one of the tests is fixed, it moves to the master branch, and then it's available next month so it gets better. So this is the overall process. Okay. There's one that I know you're going to love because it's one of your favorite topics. So, someone's asking, like, okay. You know, this is such a great achievement, but, how do you actually manage it? Do you have less funding, or is it because the core team is so dedicated? No. No. It is really, so we have a so we I have to say we have a lot of we have we have a very large community, I have to say. So whenever so it it is it is still, or it comes definitely from a community based models models. So, for example, he, manages the TB, the library now, and gave us three servers. There's some hosting Manheim, open link host domain endpoint. So everybody is kind of like contributing a bit. And we are moving and and you have to see that on the board of DBpedia, there are six of the top ten knowledge engineers according to AI Miner. So we all won a prize, basically, that we are the most influential scholars in knowledge engineering, and also our our members are, very much experts in the field. So you have to there's definitely technology leadership, and this innovation helps to to keep this running. And, we focus the last three or four years to really make it because before that, two people were doing the release, and they needed twelve months or seventeen months in some cases. And this was very expensive, and now we automated it. So it just basically can be done by a very skilled bachelor or master student. Yeah. And this is kind of like a high cost saver because per se, the association is unfunded. So each of the members has a very huge budget. Yeah. Like the libraries, the companies, the researchers, and in this funding, there's maybe always a work package included, or for businesses. They, of course, they want to, show their tools with with the PDF. So there's some interest with the researchers. There's always a work package maybe that is particular to the PDF. Okay. Thanks. There's actually more questions, but since we have to to wrap up, I'll just do a quick one and give you a quick reply. Amir is asking how does the star rating, ontologies work? Is it based on, FAIR principles? No. It cannot be based on FAIR the original FAIR principles because they are too vague. So they are kind of like saying that you should have this and should have this, but they're not specifying it too much. But for ontologies, we're talking about all ontologies, which is a very small and specialized area. And there, you can have clearly measurable star ratings. For the details, you have to go to archival. Tvpr dot org. We spent almost two weeks describing these stars, so there's a lot of information there. Okay. Okay. Great. Thanks thanks once again for, the, for the talk, Sebastian. Thank you very much. Bye. Bye.",
    "transcript_length": 28292,
    "speaker": "Sebastian Hellmann",
    "tags": [
      "apt",
      "dbpedia",
      "docker",
      "fair",
      "github",
      "maven",
      "npm",
      "scala",
      "shackle",
      "sparkle"
    ]
  },
  {
    "title": "Data Observability: How to Eliminate Data Downtime and Start Trusting Your Data",
    "description": "Broken data is costly, time-consuming, and nowadays, an all-too-common reality for even the most advanced data teams.\n\nIn this talk, I\u2019ll introduce this problem, called \u201cdata downtime\u201d \u2014 periods of time when data is partial, erroneous, missing or otherwise inaccurate \u2014 and discuss how to eliminate it in your data ecosystem with end-to-end data observability.\n\nDrawing corollaries to application observability in software engineering, data observability is a critical component of the modern DataOps workflow and the key to ensuring data trust at scale.\n\nI\u2019ll share a quick start approach to achieving data observability at scale and highlight the important role of graph analytics and metadata in driving this process.",
    "category": "Semantic Technology",
    "transcript": "Hi, everyone. Great to be here. My name is Bar Moses. I'm the CEO and cofounder of Monte Carlo, the data reliability platform. And today, I am stoked to have a chat with you all about how to eliminate data downtime and start trusting your data. Hopefully, you all can see my screen, and we're gonna get started here. So just a little bit of background about myself. As I mentioned, CEO of Monte Carlo. Prior to Monte Carlo, I worked at a company called Gainsight, worked mostly with companies to help them become data driven, to actually make use of their data, for various, purposes. I, am also in the process of writing the very first book on data quality in collaboration with O'Reilly. Super excited about that. We just prereleased, so the very first chapter. So check that out if you'd like. And then another fun fact about me is that I'm a very big, fan of Bruce Willis movies. So you'll you'll find me, watching those in my spare time, when I'm not thinking about data downtime. So that's just introduction about myself. Again, stoked to be here and to speak with you all about data downtime and data observability. So let's get started. What is the problem that we're all here today to discuss? The problem is what we call the good pipeline's bad data problem. And what does this actually mean? Everyone on this call probably has some experience working with, data lakes, data warehouses, ETL, reverse ETL, BI models, ML models. Whatever you'd like, we have all the acronyms. In short, everyone, typically most companies, have invested a lot in setting up world class, best in class data infrastructure and data systems. And so we're all sort of ingesting, processing, transforming, modeling, making great use of the data. But here's the problem. We've invested so much in having amazing pipelines and amazing data infrastructure. However, so often the data that's actually powering those systems is often wrong. Not sure if anyone here, today, you know, the example here is familiar. You know, you kinda wake up Monday morning, you come into the office or maybe you log into your office, and you suddenly get sort of slammed with this, you know, this, like, meteor, shower of pings from Slack asking, what's wrong with my report? Or, hey. It looks like the data here doesn't quite look right. What am I missing? Or, you know, get pinged. What what type of, what data what, table should I be using? What what is right for this question that I'm trying to answer? And it's always at the very last minute. It's always, like, four minutes before a meeting. Or, you you know, if you get lucky, it might be on a Friday night, as well. In any case, this sort of situation is really widespread across the industry. There isn't anyone that isn't hit by this. For me in particular, I experienced this when I was managing a data team, at Gainsight, and we were, you know, sort of hit by lots of questions. Know, I felt like, oh, man. We had, like, one job to do right. It was get the data right. And we, you know, we weren't able to do that. It was really, really hard to do. And I think there's many reasons for that. Right? There's, you know, way way more data that we're managing today. There's way more stakeholders, and and data is, a lot more distributed, in different systems. And so let's talk a little bit about this problem and how it sort of emerged emerges over time. We sort of call this the data data downtime. So data downtime refers to periods of time when your data is wrong, inaccurate, otherwise erroneous. And the interesting thing about data downtime, again, something that everyone here is likely experienced, is that the impact of data downtime actually is worse as you kind of progress across different stakeholders in the organization . So if you think about a couple of years ago, maybe three, five, ten years ago, there weren't that many people who were actually working with data in the organization. And today, there's data engineers, data analytics, data science. There might be ML engineers, software engineers, definitely you know, teams that are working in in business functions. There's way, way, way more people that are actually making use of the data. And so if you think about sort of the impact of data being wrong, at the starting point, maybe when the data engineering team actually catches that issue, that is what you'd call sort of under control. However, there might be a couple weeks later that an issue goes unnoticed, and then that is discovered by the data analytics or the data science team, which might be frustrating that we as data engineers didn't catch that issue first. Maybe, you know, worse over time, that issue lingers or actually starts to have an impact across the business. Those can actually have material impact on companies, on organizations, on people. Right? And so thinking about companies in media or in retail or ecommerce companies who really sort of, are at material risk in terms of compliance if the data is wrong, if you, you know, mistakenly get credit card information wrong. You might be overcharging customers or undercharging customers. You might be powering the wrong marketing campaign with incorrect data. The list is long. Right? But the bottom line is that data data downtime impacts all of us. Its importance is rising in organizations because the impact of data downtime, is is severe over time. And so, you know, I come to you today with a lot of bad news. I realize this is a little bit of a depressing start, but I have some good news for you all. And the good news is, actually, in order to think about how do we build reliable data products, it is helpful to look at, a corollary or look at our friend software engineers who who have been building reliable products, for decades now. Right? So how do engineers actually approach building reliable products? You know, what you can see here is sort of, various categories and various tools that have built around each of the categories that ultimately help data help engineering teams build products that are reliable, secure, and scalable. Now in data so far, it's kind of been the Wild West. Right? Like, you get up, spin snow spin up, Snowflake, spin up Looker, and you get started. Everyone has access to everything. There's no test. There's no monitors. You put things in the wild, and you hope that they're okay. Right? In a lot of those instances, data downtime rises and starts to become more prominent in organizations. So what can we actually learn from our friends in software engineering? At a very high level, there's sort of three core categories here that, that emerge from this that we would like to actually implement for data and to take as best practices as well. And these three categories at a very high level, the categories of discovering problems as they arise, resolving problems as fast as possible, and then preventing them to begin with. So, again, these are all concepts that are taken from software engineering, but what do they look like when we apply them to data? What does it mean to actually discover, when an issue is detected when an issue, sort of arises in data? You know, I'll say most often, many data teams that we find that we work with, we find that they are the last to learn about issues arising in data. So, you know, they might sometimes hear about it from their internal customers, say, the marketing team or the product team, or support or customer success team. Oftentimes, they also learn about this from real customers, external customers, if you will, not real external customers. And in those cases, they're caught blinded, if you will, learning about an issue from others. So how do we actually turn this around and develop the tools so that data teams are the first to know and the first to discover about data issues. A lot of that goes into actually setting up monitoring, alerts, strong anomaly detection, and different things that that we'll touch on. Once you've actually identified that there is a problem, how quickly does it take you to resolve it? Many data teams that we speak with, sometimes it takes them weeks or even months to resolve an issue. What if you had an approach to actually resolving data downtime issues, in a way that will allow you to do it in minutes or in hours instead of weeks or months? And what does that look like? What are some of the root cause analysis that you need to do in order to identify, what exactly is the issue? And then finally, prevention. Right? If we're doing this right, we can actually reduce the number of incidents, the number of data downtime incidents to begin with. If we're actually getting smarter over time, if we're learning, what are the, you what are the main sources or the main pipelines that tend to break? What are the main issues that are causing data downtime that are sort of draining a lot of our time? If we continue to collect insight and information about this, we should ultimately be able to prevent these issues to begin with. So I very much believe in actually adopting the sort of three three step framework, that we that, you know, is tried, tested, and proven in software engineering and now applying it to data engineering. So today, we don't have time to double click into each in each of these, but I will double click into discover and talk about some of the methods that we've used, that we've identified as sort of best in class and that some of the best organizations, some of the strongest data teams are actually adopting in order to improve the ability to discover. Now, you know, one of the questions is, how do we actually measure ourselves on these three? How do we know that we've gotten better? And so we're seeing more and more data teams develop bet better methods of actually measuring themselves on these. To be honest, most data teams don't measure themselves today, but, actually, just starting to measure is a great step forward. So on Discover, folks actually look at, time to resolution. Sorry. It's time to detection. So how quickly I'm thinking ahead here. So how quickly, is there actually what is the time lapse between when the incident actually happened when we and when we detected it? That's sort of the first metric. The second metric is around resolution. And so once we have detected an issue, how quickly until we resolved it? And obviously, you know, you sort of expect that, you know, incidents will happen, but that over time, we are improving in both of those metrics. And then the third metric is prevention. Are we able to reduce the number of incidents overall? So actually starting to measure how many p one, p two, p three incidents do we have? What is, what is the the time to detect and time to resolution per issue, and are we able to improve over time? So that's just to give you a little bit of sense of how you can measure yourself as an organization to improve this. Now let's double click into the discover part here, the first portion, and talk a little bit about what are the methods that teams are using to, really operationalize that as part of improving, their data operations and minimizing data downtime. So continuing to borrow a little bit some concepts from software engineering, there's a concept called observability in software engineering, which is yet again a very well understood concept, which, you know, if you check out this quote, also has a lot to do with, with monitoring as well. And there's a lot that's been written about the difference between observability and monitoring. But in software engineering, it's sort of like a no brainer to have something like this. Right? And so the most common things, you know, talked about, like metrics and traces and logs. And there's sort of a lot of kind of solutions that enable software engineers to do their job in making sure that applications are, reliable. So solutions like PagerDuty and Splunk and New Relic and Datadog are all solutions that we are very accustomed to work with. And in software engineering, they make a lot of sense. And so the question is, in the data space, why are we flying blind? Why don't we have something like this that will allow us data engineers to actually do our job properly? And so imagine that you're actually taking the concept of observability and now applying that to data. What does that look like? So let's actually define the term data observability here so everyone is on the same page. Data observability is an organization's ability to understand the state and health of the data in their system with the objective of mitigating data downtime. Again, the the measure of success here is if we are able to prevent data downtime to begin with. We probably won't reach a hundred percent. We will not be perfect. But if we can actually minimize it and reduce this, we will all be able to spend time elsewhere on other revenue generating projects, or other things that, you know, we are curious and excited about. And so data observability, barring again from the concept of observability from software engineering, actually allows us to understand, to discover data downtime issues, ultimately increasing trust in the data. So continuing down this path of data observability, what does data observability actually mean? So in software engineering, it's super well understood and used. In in data observability, it's a new term. Right? And so what we did at Monte Carlo is actually we spoke to hundreds of data organizations ranging from small startups to large organizations like Netflix and Facebook and Uber and, actually, sort of created a sort of or collated really big dataset of what are all the reasons for why data downtime happens to to organizations, what are all the symptoms that, folks see when data downtime happens, and then all the reasons that folks or all the ways in which folks actually resolve those issues. And so we actually codified all of that good stuff into what we call the five pillars of data observability. And in our experience, we see that when you actually sort of put these to work, when you, instrument, monitor, analyze these, data teams are actually able, to to start this sort of journey towards operationalizing trust in data. And and having this combination of these five actually provides a very strong view into the health of your data. And for the first time, data organizations are actually able to see, to understand the health of their data and understand where are the sort of vulnerability spots and what are the areas where some corrections need to be implemented. So let's double click into each of these briefly to give you a little bit of an example of what these data observability pillars mean. So starting with freshness, which is the first one, which, freshness means a lot of different things. This is a particular example of it. But really sort of it comes down to the timeliness of the data. Right? So in this example, what you see here is a specific table that, you know, the the lines here indicate the table getting updated, over sort of this, axis of time here from from November tenth onwards. And you can see that the table is getting periodically updated once or twice a day, a couple of times a day. And then suddenly, there's a period of three or four days with no updates at all. In this instance, this might indicate that there is a potential issue with the data. And so actually, by using machine learning to observe the data over time, we're able to identify these in an automatic way. And now, oftentimes, data teams find themselves manually specifying a lot of these thresholds. I'm happy to say with some advances around automation machine learning, it is actually possible to do this, not only for a particular table, but rather for hundreds of thousands of tables at scale. And so you can automatically sort of turn on a slew of metrics, around sort of the freshness of data, this being one example of them. The second important pillar is the concept of volume. So pretty straightforward, as you can see here, an example where you see a number of rows. And in this particular incident, there's an unusual number of rows that were removed. You can see there's sort of a nice trend. So slowly increasing the number of rows and row count. And then suddenly, on September thirtieth, there's a very high number of rows that have been removed. So this is an example of one way to track volume. And, again, another sort of reason for why or another indication for data might be, that might be wrong. The third sort of pillar of data observability is what we call schema, and schema is often the culprit of data, going wrong or going down. Oftentimes, schema or actually or schema changes, they happen quite a lot, but they're not communicated about. So one team might make a change, but then the other team is unaware of that. And oftentimes, in that breaking point in communication is where sort of lies the data downtime problem. And so what we see data teams do is actually have an automated way of tracking schema changes. And so in this example, you see there's a bunch of fields that have been completely deleted and removed. There's probably someone downstream that has a report or other tables or another model that actually relies on these fields and needs to be notified of this. You might actually have also a specific field change in type, which might impact downstream. And so having an automated way to track these the changes in schema and a way to communicate about them is a very, very important part of this fourth pillar. And then the fifth pillar is is, or sorry. The fourth pillar is distribution, and distribution really has to do with metrics and health at the field level. So as an example here, you can see there's a field called count pets one five years old, and we're looking at percentage null. And there's a specific average, a seventeen day average, so a little less than three weeks average. And then today, the percentage of knowledge just spiked. It's way higher than the average in the last, two to three weeks. And so in this instance, you know, this might be a field where, it's it's it is quite bizarre, to have so many, so many null values. And so this might actually, be an incident, that you'd wanna look into. And so this is an example of a type of distribution issue, if you will. And, again, there's a whole you know, under distribution, you could look at null rates, negative rates. You could look at, average, min, max, a whole slew of different metrics under distribution, everything at the field level. And then the fifth linea the fifth, pillar here is the lineage pillar. It might be my favorite. And this pillar actually allows us to bring the whole story together. So what you can see here is, a table level lineage, where you have, tables from a data warehouse and then views downstream in Looker. And there's a particular incident that's been identified in a particular table called recent metrics. And you can see all the tables downstream and the views and explores, and dashboards downstream that have been impacted by this. Now the importance of lineage, both at the table level and field level, is that lineage by itself is quite useless, to be honest. So if you just have lineage, there's not that much that you can actually do with it, or it's mostly sort of eye candy, for for most teams, where we do find lineage being helpful when it's applied to a specific use case. And in this particular use case, it's around data observability. And so I think when you take lineage and you combine that with the sort of other pillars that we talked about today, that's really where the magic lies. So, you know, let's say, for example, there's a particular table that has a freshness problem. So there's data that's loading once an hour at any given day and then data stopped loading that table. It hasn't been loaded for three hours. And then downstream from that, there's another table that might actually, as a result of that, have a problem has a distribution problem. And as a result of that, there might be a dashboard downstream that might have incorrect data now. And so lineage helps us piece all of that together and help us make sense of, Okay, there's a problem somewhere, but should I care about that problem? Maybe there are no dependencies. Nobody's using this table, so nobody cares about it. So maybe, you know, we don't care about that specific problem. However, maybe there is a specific incident with hundreds of tables downstream, key reports that your board and execs and customers are using, in that instance, you probably want to give that incident a higher degree of attention, a higher degree of severity. And so lineage is incredibly powerful in helping us sort of bring this together and help us understand both on the one hand impact assessment. Should I care about this? Who cares about this? And then on the other hand, it helps us understand root cause analysis. Right? So if there's a particular incident somewhere, what are the upstream sources of that we can learn from around what might have gone wrong and where? So being able to answer all of these questions is really fundamental to our ability to restore trust in data, or to have more trust in data, if you will. And so finding or sort of developing ways to automate the instrumentation and the monitoring of these and bringing them together in one view is really what helps us in building towards this new paradigm, if you will, this new data reliability stack, which includes sort of the three steps that we talked about. And so when you think about that first step, the discover step, going back to kind of the initial prompt here, most of our reality today is a reality where we created some tests to test the things that we know can go wrong with our data, and that's a very important part of our job. And then eighty percent of our life is really kind of getting angry Slacks or emails from colleagues that are really, really unhappy with some of the data going wrong or reports getting messy or, model drift. There's all these things that people can get really angry at, and then the finger pointing, game starts. You you know, who actually owns this table and whose fault it is. Right? That's the reality that we all live in. However, I think there could be a better way, and the better way has to do with observability. With observability, you know, you still have tests that we've created ahead of time. However, there's broad automated coverage that we can achieve, with observability for eighty percent of the incidents. So we start out with a good base of issues that are detected in a way that we don't need to specify manually, but rather we can have this broad coverage for. Of course, we will always need to have tests. There's always going to be things that, you know, us as sort of folks closest to the data would know best. And it's very hard for any machine to actually learn or for anyone to do for us. And those will always continue. But with observability, you can rely on sort of broad base of coverage to help make sure that you're sort of starting from a, from a good place. And then I wish I could say we could eliminate the angry text completely, the angry Slack messages, but, you know, I do think that there's probably a one percent that will always remain with us. We we can try to minimize as much as as possible and maybe even laugh about it. But that's sort of, with observability, the benefits that data teams see. And so going back to kind of the three core components, we talked a lot about Discover and sort of what that looks like. We talked a little bit about Resolve. The five pillars of data observability fit nicely into both of these and really help us have a starting point for the data that we need to do our job well and to actually sort of generate results here to reduce time to detection, time to resolution. And then ultimately, we'll talk more about this potentially in a future talk, But, there's a lot of work more for us to do on sort of prevention, as an industry overall. I think we've made tremendous progress in some of these areas, but I'm really excited for developments in other areas as well. And so with that in mind, data observability is just getting started. It's very much a new term. It's taken the world by storm. We're seeing more and more data organizations, implement this as part of their, part of their operational work. You know, team syncs to actually review some dashboards of how we've improved in time to detection, time to resolution. What are the main, areas where we where we're seeing a higher incident of data downtime issues that we want to work through? What are the areas where we're seeing an improvement? What are the different teams that are working with us that we can actually have contracts and SLAs with in order to develop a stronger organization overall, sort of going beyond the data engineering team, and including all the stakeholders that we work with. So there's a lot more work for us to to be done here. I hope this gave you a little bit of a taste of what data observability actually is. I hope you left with a better sense of some examples, you know, maybe something that you can implement, and get started on your journey. If you'd like to reach out, feel free. This is a topic that I'm most passionate about. And if you have an example or a story, or a bad data horror story to share, would love to hear from you. Thanks so much, for having me, and I'm looking forward to the q and a portion.",
    "transcript_length": 25557,
    "speaker": "Barr Moses",
    "tags": [
      "datadog",
      "gainsight",
      "looker",
      "monte carlo",
      "new relic",
      "oreilly",
      "pagerduty",
      "snowflake",
      "splunk"
    ]
  },
  {
    "title": "Deep Learning on Graphs: Past, Present, And Future",
    "description": "Graph representation learning has recently become one of the hottest topics in machine learning.\n\nOne particular instance, graph neural networks, is being used in a broad spectrum of applications ranging from 3D computer vision and graphics to high energy physics and drug design.\n\nDespite the promise and a series of success stories of graph deep learning methods, we have not witnessed so far anything close to the smashing success convolutional networks have had in computer vision.\n\nIn this talk, I will outline my views on the possible reasons and how the field could progress in the next few years.",
    "category": "Graph AI",
    "transcript": "It's, my massive pleasure to, introduce Michael Bronstein. Michael Bronstein is a professor at Imperial College London where he holds the chair in machine learning and pattern recognition, and also head of graph learning research at Twitter. He also leads, the, machine learning project, in, project SETI, a TED audacious prize winning collaboration aimed at understanding the communication of, sperm whales. So without further ado, over over to you, Michael. Welcome. Thank you, James. I hope you can see and hear me well. So thanks a lot, for the invitation to to join this presentation today. And I would like to talk about, deep learning graphs and, try to outline what it is about, what is the present state of this science and technology and what the future columns for it. So allow me to start with actually a little bit far away and taking a step back and talking about the concept of inductive bias. So this is a fundamental notion in learning and it refers to the the set of assumptions that, a machine learning system has to do about, the data in the problem at hand. And let's take a very simple machine learning system, some of the earliest neural networks called, multilayer perceptrons. We know that they can approximate any continuous function to any desired accuracy. We call this property universal approximation, and it was proven in the end of the eighties for for these architectures. And, it sounds like a good piece of news. Right? Because we can represent anything we want with multilayer perceptrons. But with the moment we try to apply these simple neural networks to real problems dealing with high dimensional data, they tend to fail miserably. And, let's look for example at the problem of digit classification, one of the simplest examples of a computer vision problem. Essentially, what we want to say here is whether what we see is the digit three. And the way that you can think of this problem when you try to apply multilayer perceptron to this digit classification, you just stack the image into a vector and pass it as the input to the, to this neural network. The problem is that what happens when we have another instance of the same image where the unit is just shifted by one pixel as you can see here. And the input to the neural network can be very different because the neural network is absolutely unaware about the structure of the image and thinks of it as a one dimensional vector. So it will take a lot of examples and very complex architecture with a lot of parameters to learn in variance to shifts from the data. And this is one of the reasons why early attempts to apply neural networks to, to image data, to computer vision problems, failed, failed miserably. Now the breaks will in applying neural networks to images has come from the right inductive bias. And these are convolutional neural networks from the seminal work of Jan de Kann, where the inductive bias is what we call a translation equivariance. Basically, it's hardwired into the neural network architecture in the form of shared local weights. And this way, we have now way less parameters and this idea that you can recycle the same weights and apply them at different positions at the image at different scales is very powerful. That was really what made these architectures so successful, and the results speak for themselves. As you know, CNNs have really revolutionized the field of computer vision in the past decade. Now let me show you a different problem. What you see here is a a molecule, and, this is a molecule of caffeine. I hope that in the break you've had enough of it. So I have a little bit in my my tea cup here. And, we can model it as a graph. The nodes here represent the atoms and the agents represent the chemical bonds. Let's say that we want to predict some chemical property of this molecule, for example, what physicists or chemists would call the atomization energy. So it's the energy that takes to break this molecule apart. And, this is really fundamental problem in, drug design and drug drug discovery to be able to do virtual screening to predict certain properties of, potential drug candidates. So how do we represent this molecule? Again, we can just take the features of the nodes and put them into a vector as we did before with an image. The problem though that we have many, many more ways to do it. Actually, any permutation of the nodes, produces a valid representation vector. And the kind of invariance we want to have here is different from the previous example. Here, we need to account for all the possible permutations. And molecules are just one example of graph structured data. In fact, we see graphs everywhere. Probably the most prominent example are social networks where the nodes are users and the edges represent their social relations and interactions. So you can think of Facebook or Twitter or any other graph that is generated by by the activity of of humans. We also encounter graphs, or networks in biological sciences where we look at the interactions between different biomolecules such as proteins and drugs and so on. In computer graphics and computer vision where we use graphs with maybe a bit more structure that are called meshes to represent three d objects and, in many, many other fields like in brain imaging where graphs can be used to represent functional networks and so on. So if you want the gist of what deep learning on graphs is, it's essentially finding the right inductive bias for graph structured data, which is sometimes also called relational inductive bias. And in two thousand sixteen, I wrote a position paper with Jan de Kann, Pierre van der Geist, Joanne Brunner, and Arthur Schlam, where we connected several attempts to deal with, irregular or non Euclidean structures in deep learning, which we named, geometric deep learning. So I would say that now I would probably write it completely different. So I think there is a much more depth to this idea of trying to geometrize machine learning problems. But somehow this term is now used synonymously with graph deep learning or graph representation learning. And, as I said, there is more to that. We can think of geometrical learning as a framework unifying grids, graphs, curves, and gauges, which is high energy physics term for manifolds. So, in fact, we like calling it the four g of deep learning. So this year, graph neural networks have officially become one of the hottest topics in machine learning. And at least judging from the submissions of the six of ICLR, one of the main conferences in ML, this has been one of the popular keywords. So let me show you some more details and let's look at classical CNNs. So if you look at classical CNNs that take as input an image, which we define as a function that I denote here, by x on a regular two dimensional grid, What convolution does is a form of weighted aggregation of the values in the pixels of a neighborhood. And we can do the same thing in the graph. The neighbors will be the nodes that are attached by edges to any node in the graph. So so far, it looks all the same. But one thing to notice is that when we move to a different location, we still have a constant number of neighbors because the grid is regular. So in this case, each pixel is connected to four neighbors. And on the graph, on the other hand, we might have a very different number of neighbors. We had six neighbors in the previous node and five neighbors, here. And if you think of social networks, these differences can be huge. So a popular user like Donald Trump has millions of followers and other users might have just a few tens or hundreds. And this is what is called the node degree in graph, in graph theory. Another thing to observe is that on agreed, we have a fixed ordering of the neighbors. We can always talk about, a node to the left or a node to the right, and this allows me to always apply the same weights to the first neighbor and other to the second and so on. And this is exactly the idea of weight sharing in conversion on your networks that that I mentioned in the beginning. So if you represent this as a matrix, we actually see that there is a special structure that is called, circulant matrix. And, because circulant matrices commute, we also have commutativity with the shift operator, which is called shift equivariance. And in fact, you can actually derive convolution from first principles of symmetry, of translation symmetry. And, this is the the idea of geometric deep learning where inductive biases emerge from, first geometric principles. So on the graph, the situation is rather different. The ordering of the neighbors is completely arbitrary, so we don't have a canonical way of assigning a fixed weight to a given node. And this actually makes graph neural networks quite different from traditional CNNs in the form of the variance that that we get. So here's a blueprint for how to do conversion like operations on graphs, which have two types of operations. We can aggregate information from neighbors, and we can process it in some way and then update the features of an old. So these are the two operations, aggregate and update. And aggregate has a most general form of a function that is applied to to the neighbor node features. And importantly, this, function is, prohibition environment. And there are some important particular examples of architectures where this can be linear aggregation or an aggregation that uses some form of attention mechanism. So let's now dive more into the details of what is similar and what is different, between graph neural networks and traditional deep learning pipelines and look at CNNs. So if we look at historical developments of CNNs that that that appeared in computer vision problems. So early models such as AlexNet from two thousand twelve were relatively shallow, and they have just eight layers with relatively large filters of up to eleven by eleven pixels. And, as CNNs became more commonplace in computer vision, they also became deeper and used smaller filters. So the VGG architecture had twenty layers and three by three filters. And there are several reasons for, why this happened. First of all, obviously, smaller filters are more efficient computationally. But more importantly, it was shown that in convolutional neural networks, you can construct complex features from simple ones, the property that we call compositionality. So if you look at the features that that that are noted in different layers in the neural network, you see that the first layers have primitive geometric features such as edges or corners. And as you go deeper, you get more complex, complex features. It appears not to be the case in graph neural networks and it's really wishful thinking, composing complex structures from simple ones. For example, it was shown recently that graph neural networks of the message passing type are equivalent to what is called the Weiszler Lemann graph isomorphism test, which is a classical algorithm from graph theory that determines if two graphs are isomorphic by means of some color refinement procedure. And, this is a test that can tell you whether two graphs are possibly isomorphic, but it's necessary but insufficient condition. In this case, for example, the the the test will fail, because, it is known that it cannot count, simple substructure such as triangles. And one of these graphs has triangles, another one doesn't. So there exists higher order, more powerful versions of the the Weisler Lemann test, but they have, prohibitive computational and memory complexity. So another thing to notice that unlike in the traditional architectures, it is actually difficult to to train deep, craft neural network architectures and, they require a lot of, tricks such as regularization and, architectural, changes such as residual connections. And, the bottom line that that even with these tricks, sometimes shallow, baselines work better than than the deep super duper graph architectures. So one reason for this is what is called feature oversmooling. It means that features on the nodes tend to collapse to a single point in the feature space. But probably a more fundamental phenomenon is that, it was described in the recent paper, by Uri Alon that, there is a bottleneck. So in some graphs, where the number of neighbors tends to grow exponentially as you expand the neighborhood size, you get a lot of neighbors, whose features you need to squeeze through a single feature vector. And if you have, this exponential growth of neighbors and you also happen to depend on long range information, you run into the bottleneck phenomenon. And, in a sense, it's not clear for which cases, for which problems, and for which graphs, that helps. So in a sense, deep graph neural networks try to use many layers with small filters, just one hot filters. The alternative is to use fewer layers but make the filters bigger, and that's exactly what we try to do in the work with my collaborators at Peter. We took this idea to the extreme. We wanted to see what we can do with a convolutional layer, just single paragraph convolutional layer. And, an analogy in classical CNNs would be to have a shallow network with bigger filters. The nice thing here is that if we use linear diffusion, linear message passing, we can pre compute the diffuse features and then it boils down to just applying simple multilayer perceptrons to the predefined old features. And as a result, the neural network is extremely efficient, and it scales to to very large graphs. So the surprising finding is that such a simple architecture performs almost on par with some of the much more complex state of the art deeper models. And again, this brings the question when do you need depth for what kind of graphs and for what kind of problems. What is for sure that it's significantly faster by more than an order of magnitude in training and inference, and, it resembles the the inception convolutional neural networks that that were pioneered by Google a few years ago because, we use filters of different size, as was used in that in that architecture. So there are certain graph quantities we cannot compute by means of message passing no matter how deep we make our own neural network. And I should say that this is not fully understood and on the contrary, there are examples of properties such as graph moments, for example, that can only be computed unless the network has certain minimal depth. So it's still an open theoretical question. So what we can do is, to help graph neural networks to count substructures by providing these counts as some peak computed feature vectors. It's kind of positional or structural encoding, and we can do this by, peak counting some structures of size k. This could be, for example, triangles or clicks or cycles or paths of different lengths. And we provide this as node or edge fishes and then do standard message passing. So we call this architecture graph subtraction networks. And, the nice thing about it that it actually retains the linear complexity and the local structure of standard message passing neural networks. And, the, comp the the precomputation is the part that might be expensive. In the worst case, it is as complex as the high order of Le Mans, methods. But, in practice, it can the complexity is much lower. So what we gain in this way is that, the graph's abstraction network is strictly more powerful than the the device for a lemon or the equivalent message passing graph neural networks. And we have problem specific inductive bias, we see that, for example, by by counting certain structures such as clicks in social network graphs, we get significantly better performance. And especially interesting for molecular datasets describe chemical compounds. Cycles are important motif, and, they're abundant in organic molecules with structures such as aromatic rings. And, again, this is my favorite molecule of caffeine. It has two rings of size five and six. So if you use a graph substructure networks with these structures, we get significant gain in performance of predicting chemical properties of molecular graphs. And the experiment shown here is on the TING dataset that is often used for virtual screening of drug like compounds. And I believe that applications of graph deep learning in computational chemistry and drug design and discovery are probably the most promising. Allow me to come back to this point in a few minutes. So in the remaining time, let me share some thoughts on what I believe to be, the next steps in this field. And, I would like maybe to make here a small confession. I'm somewhat disappointed with, the the when I started working on, genetic deep learning probably around six years now, I was expecting something similar to the revolution that happened with the adoption of deep learning in computer vision and we have not seen anything similar yet. And, of course, there is a lot of progress and even some, commercial applications of, craft neural networks in the industry. Well, I could here shamefully put the success of a startup company that I founded with my students, that where we were using graph neural networks to detect misinformation on on Twitter and were acquired by Twitter last year. Yet, I think it has been more of an evolution even though a fast one. And, let me try to explain and maybe highlight some points that are important for future progress in this field that that will make maybe a broader adoption of deep learning on graphs. So there are three things really that sort of made deep learning happen, and these are data compute and and software. So in case of computer vision, data was a benchmark such as ImageNet. Compute was the computing power of graphics hardware, the GPUs, and software was open source tools such as PyTorch or TensorFlow that that have democratized deep learning. Now if you look at the situation, in graphs, well, we also see emergence of standardized benchmarks such as ImageNet for graphs, the open graph benchmark, software libraries such as DGL or PyTorch Geometric that implements some state of the art graph learning architectures. There are problems of efficiency and scalability. So this is really what has precluded so far the application of graph learning to industrial scale and industrial settings. So now we have, several methods that that can really work in production systems of large scale. If we look at, problems such as Twitter and Facebook, they deal with dynamic graphs. So the graph is not a static session, but it's living and evolving in time. Nodes are added and deleted, and it's really, better to think of this graph as a kind of a synchronous stream of events that form it, like edge and node insertions and deletions. And there are currently just few architectures that support these cases. So this is one of the topics that, I'm working, on a Twitter work with or we've developed recently what we call temporal graph networks. It's an architecture that generalizes message passing neural networks to dynamic graphs. Now talking about high order structures, I already mentioned them in the context of our work on graph substructure networks. And I would like to stress again that so far, graph neural networks were primarily focused on simple, structures such as nodes and edges and message passing on these structures. But we all live in many complex networks such as biological or social networks. We have complex high order structures and motifs, and, we want to better exploit them. So I believe that there will be, in the future emergence of methods that take advantage of these more complex structures and their interesting relations to previous works that have been done on topological data analysis such as persistent homologies on graphs. Another important topic is actually the very assumption that we are given an input graph to start with. In many cases, this is not the case. We don't have the graph. We just have some cloud of points and the graph can be just a convenience that can be used to model the underlying data structure. We did first work that we called dynamic graph CNNs. It were we showed that it's possible to design graph neural networks that build the graph as part of the learning process on the fly. And, the graph, for example, can be constructed, as k nearest neighbor graph and updated between the layers, in a way that is optimal for the downstream task. And this brings an important question of whether the computational graph that is used for message passing does necessarily, be the same as the input graph. There are many good reasons why we'd like to decouple the two, one of which I already mentioned, the the bottleneck phenomenon. So you might rewire your graph to make it more convenient. And another reason is that, the graph can actually be, as I as I already said, it can be done in a way that, that is optimal for some downstream task. And sometimes the graph that can be learned in this way, setting that we call latent graph learning, might be more important than the downstream task itself so it can provide some interpretation of the of the problem or the classification results. And we did with my collaborators in Munich, a recent work where we looked at health care electronic records of, different patients. And graph neural networks have been applied to these problems before, but we can craft it graphs. And here we show that learning the graph as part of the process by, by graph neural network provides better results and also better hopes to interpret these results. And, thinking maybe broader in retrospective, this kind of methods is related to what was called manifold learning or nonlinear dimensionality reduction, a class of approaches that, modeled, the data as sampled from some low dimensional manifold that lives in a high dimensional space and, tries tried them to represent this dataset in a lower dimensional space by preserving some structures such as geodesic distance and then applying, machine learning algorithms such as clustering on the slow dimensional representation. So, the problem with these approaches was that the different steps were completely disconnected from each other. And you first had to to create some handcrafted representation of the data, then build the graph that represents the structure and only then apply machine learning. And sometimes it required a lot of tuning by hand of how you represent the data and build the graph. Now with graph learning pipelines, you can, put all these stages into a single end to end differentiable pipeline, and that's why I call these methods meaningful learning two dot o. It brings all these steps into a single, architecture. And probably we'll see more interesting applications of these methods and one, maybe a little bit exotic field where these, matters are already, been shown quite, quite interesting to produce quite cool results is high energy physics where you can think of, interactions of, different particles. And, one of the key problems is to reconstruct the interaction graph. So there are many, open theoretical questions about, performance guarantees, expressive power, generalization, robustness of perturbations, and so on. And, I think what has been done so far is just the tip of the iceberg. Last but not least, and I apologize maybe running a little bit over time, I would like to finish with a few examples of what they call killer apps. So something like computer vision, was the killer app for, for traditional deep learning and conversational neural networks. For, graph neural networks, we see, we see them applied to a lot of different problems since graphs are really very abstract and universal models for systems of relations and interactions. You can find them in particle physics, in recommender systems, in, problems in social networks and computational chemistry. But, if you ask me what would be one field of which I am willing to bet where these methods would probably make a breakthrough, I would say these are problems in medicine and biology, and you can apply graphs on all scales from nano to macro, from modeling molecules and interactions between molecules to entire patient networks. And some of the results here are really extremely promising. I would even say dramatic. So one, on the nanoscale, one application is that we can model molecules as graphs and predict their properties, which is a holy grail of drug design because the space in which we operate is humongously large. We have something like ten to the sixty of possibly synthesizable, small molecules. Whereas, what we can test in clinical environment is maybe a few hundreds of compounds. So we somehow need to to bridge this gap computationally. And at the lower level, we can do, quantum mechanical models and molecular dynamics and maybe some approximations such as DFT. So, craft neural networks, already several years ago were shown by any work by by DeepMind by Justin Gilmer to be, at the level of DFT while being several orders of magnitude faster. And this is really a cheap alternative to, more complex quantum mechanical simulations for predicting properties of potential drug candidates. And one doubt that you always have when you work on this kind of biological or chemical problems that it is too simplified and kind of spherical course in a vacuum from the famous joke, but, probably not anymore and GraphML is already on the radar of pharmaceutical companies. And earlier this year, the group of colleagues at MIT showed the discovery of a new class of antibiotics where graph neural networks were used in the virtual screening pipeline. So in our application also, in drug design, drugs are typically small molecules, but their targets are usually large molecules, proteins. And, proteins are among the most important, molecules in our body. They play crucial role in almost every biological process. And, in some targets, unlike, what we see with small molecules, they're considered to be very difficult target by small molecules because they have flat interfaces. So they're, what is called undruggable. And, it is possible to develop a new class of drugs called biologics or biological drugs where the drug molecule itself is a protein. And it allows to address these kind of targets. And with my colleagues at EPFL, we've used geometric deep learning to predict protein binding properties and then construct new proteins from scratch for this called de novo protein design. And we showed, for example, that we can design proteins that disrupt the programmed death ligand complex that is used as target for, cancer immunotherapy. And, this approach potentially could pave the way to a new generation of biological anticancer therapies. So this was a paper that appeared on the cover of Nature Methods, in the February issue this year. And at the high level of abstraction, we can use graphs to model the interactions between molecules such as proteins and drugs as the the protein to protein interaction drug. And, in if you think of, the drug traditional drug therapy, in many cases, we see multiple drugs that are administered at the same time. Medics call this polypharmacy or combinatorial therapy, and it comes with the risks that some interactions of the drugs can produce bad, or even potentially dangerous effects. And, it is impossible to clinically test all the, millions of possible combinations between all the FDA approved drugs, and graph neural networks were shown to be successful in predicting drug side effects or drug interactions. So the interactions are not necessarily bad. They can actually be synergistic. And, I'm part of the the cover coalition that, tries to to develop graph models for predicting synergistic effects of, combinatorial treatments against COVID nineteen. So I will finish with the last example that takes ideas of drug requisitioning to the domain of food. And itself drugs, we can look at drug like molecules that are contained in food and you you know that many plant based foods belong to the same class of, chemicals that are used as as drugs. So it's not surprising that, for example, many, anti cancer therapies are actually, synthetic analogs of molecules that you can find in plants. And we're reading just thousands or, of such molecules, polyphenols, flavonoids, terpenoids, indoles. I'm pretty sure that most of you have never heard of them. In fact, they still remain large and unexplored by experts. They are never tracked by any, any regulatory bodies. So this is truly the dark matter of nutrition. And, with collaborations at Imperial College, we used graph based, ML techniques to discover drug like molecules in food and then, identify which foods, would work the best and hope to prevent or or treat diseases. This is really the first way this somewhat simplistic attempt to use graph neural networks for this problem of predicting health effects of biologically active molecules in full by modeling their network effect. And, in a longer perspective, our ambition is to provide quantum leap in how we, prescribe, design, and prepare our food. And as a conceptual take on this, we partnered with the molecular chef who used the ingredients we have identified to prepare simple, tasty, and, and cheap recipes that you can actually find online. So I think it's a good moment to end on this tasty note and, well, probably we need to come back and see what whatever I have prognosticated here, how much of this has materialized, in the next, few years. So thank you very much.",
    "transcript_length": 29595,
    "speaker": "Michael Bronstein",
    "tags": [
      "CNNs",
      "DGL",
      "PyTorch Geometric",
      "convolutional neural networks",
      "deep learning",
      "dynamic graph CNNs",
      "geometric deep learning",
      "graph learning",
      "graph neural networks",
      "graph substructure networks"
    ]
  },
  {
    "title": "Graph Abstractions Matter",
    "description": "While mathematicians have used graph theory since the 18th century to solve problems, the software patterns for graph data are new to most developers. To enable \"mass adoption\" of graph technology, we need to establish the right abstractions, access APIs, and data models.\n\nRDF triples, while of paramount importance in establishing RDF graph semantics, are a low-level abstraction, much like using assembly language. For practical and productive \u201cgraph programming\u201d we need something different.\n\nSimilarly, existing declarative graph query languages (such as SPARQL and Cypher) are not always the best way to access graph data, and sometimes you need a simpler interface (e.g., GraphQL), or even a different approach altogether (e.g., imperative traversals such as with Gremlin).\n\nFurthermore, the nascent graph database industry has not been able to settle on a single graph model: Developers are forced to choose between RDF, a standard that offers broad interoperability, and Labeled Property Graphs (LPG), which offer an object model-like abstraction but lack interoperability.\n\nWe present ongoing work towards \u201cOneGraph\u201d (1G), a unifying logical model for graphs. 1G will enable developers to choose their higher-level graph abstractions and query language(s) independent of a particular graph model (RDF or LPG), and thus they do not have to worry that this choice will later limit their access to other available tooling (ETL, visualization, etc.).",
    "category": "Semantic Technology",
    "transcript": "Hello. Good afternoon. I'm Ora Lassila, and I will be talking about graph abstractions. This is titled as a personal view, mostly because I'm kind of talking about things that I've been thinking about for several years now. Not only in the context of my current position as a principal technologist in the Amazon Neptune graph database team, but also throughout the work that I've done with with RDF and the Symantec Web over the past more than twenty years. So here is a rough game plan today. I'll talk a little bit about the history of graphs first, and then I'll get into graph abstractions and and really how to access graph data from the software standpoint. From that, I'll go to treating an entire graph as an abstraction and talk a little bit about what we can do to unify the currently somewhat fragmented world of graphs. And, I'll conclude with, some thoughts about how to how to move forward from the current state. So this is my take on, on on the history of graphs and ontologies. Oftentimes, these things are presented as something new and the reality is quite the opposite. So if we look at the graphs, graph theory as a branch of mathematics dates back to the early seventeen hundreds. And ever since the, the birth of computer science as a as a field of study, we've understood that graphs are really the essential underpinning for most of computer science. Social networks showed up sometime in the 1960s. Things like the small world experiment, six degrees of separation, and and, something we call the Erdos number. And then in throughout the sixties and seventies, there was work on on network databases or navigational databases and then semantic networks, which is really where we're starting to spill over to the ontology side here. If we look at ontologies, really, we can go back to the third century BC, and Aristotle's work on categories and logic. Of course, the whole idea of taxonomical classification of things, got more concrete in the early seventeen hundreds when Linnaeus classified plants and animals. In late eighteen hundreds, we saw work on library classification in the form of the Dewey classification system. And then around nineteen hundred, philosopher Edmund Husserl really worked on sort of understanding how ontology can be the foundation for representing information and the meaning of information. Now fast forward to the 1970s and onwards. Predicate logic became sort of the foundation of knowledge representation in the field of AI. And then in the late 90s is where these things come together in the inception of the semantic web and the related standards such as RDF and OWL and whatnot. And from that, we basically get the modern knowledge graphs and graph databases and all that. So long history. But despite this history, programming with graphs and ontologies is by and large still rather cumbersome. And this is something, I'd like to address. So, when we're talking about graphs today, we first of all have this a little bit of fragmentation of the space. And and and this may be all familiar to all of you, but let me just kind of summarize, anyway. So we basically have two kinds of graphs, RDF graphs and labeled property graphs. On the RDF side, graphs are such that vertices and edge types are really just identifiers. Graphs decompose into something we call triples. So basically, these are the edges and the two endpoints of an edge. Scalar data types in RDF are pulled from XSD, XML schema. And there are no composite data types per se, and composite objects are constructed using the graph structure itself. Now, on the property graph side, vertices and edges themselves are structured objects. There is no common identifier scheme like there is on the RDF side. And the scalar and composite data types, they are typically borrowed from the whatever is the implementation language underneath. We also have fragmentation when it comes to query languages. So on the RDF side, we have SPARQL. And in the context of this talk, on the property graph side, we have Gremlin and Cypher. There are others too, because the field is more fragmented. I'll be talking about RDF mostly. Towards the end, I'll come back to property graphs. If we look at RDF and the abstractions available for us in terms of programming with graphs, The triples, which RDF graphs decomposed to, are very low level abstractions. It's like programming with assembly language. So we need something different. And in the past, we've sort of tried to map graphs to trees, but we have a meta model mismatch there. Graphs and trees, there there are differences which can cause all kinds of confusion. And of course, examples of this are things like RDF XML, You're trying to use XML as the representation for RDF and JSON LD, which is basically does the same with JSON. I think of those as warning examples. Now, of course, it's also important to remember that really when it comes to graph, the syntax does not matter and it should not matter. You solve the syntax problem once, and then you forget about it. But what about the graph abstractions themselves? So could we do something that people have done on the relational side? So something that could be like an object graph mapping? Take data from a graph and map that to some kind of objects. There are it can be done, of course, but there are some interesting issues here. So first of all, what constitutes an object? So how much of the graph do you have to pull to build an object? And then, of course, there are these kind of shared references. And in RDF particularly, there's something called blank nodes. These are nodes that cannot be addressed from outside the graph and those can be difficult to handle if you do this mapping to objects. Graph queries, or as I'd like to sort of think about, this, how do I get there from here? So graph queries, mostly, they're about traversal in the sense that traversal is an excellent way to understand what you do with a graph and how you kind of find things in a graph. So questions like where can I go from here given this particular path pattern? Are these two nodes in the graph connected? And if they are, how are they connected? Using path patterns, for traversing a graph is a little bit like pattern matching the graph itself. And of course, SPARQL is a very powerful query language, but it's a very big hammer, very heavy tool for most graph access problems. We need something simpler. Something like a simple search, maybe faceted search or filtering of data from the graph. And once you've done the filtering, then of course the question is, give me everything you know. to the particular node in the graph. This is sort of related to the object graph mapping. And then, of course, queries that really are about traversal, specifically. So specifically. Now, graphs are the graph data, graphs they're modeled somehow. And here again, we have some problems. So first of all, the property graphs generally do not have schema languages that would let you do the modeling. So the model is implicit or it's buried in the code that you write. RDF, on the other hand, in some ways has too many schema languages. So, we have RDFS, we have OWL as a sort of more powerful schema language, and then we have SHACL and they are all languages that were created really for specific purposes. And I think there, what we would need is some kind of a unifying view of these because telling people that, hey, you can model your graph, but here are three schema languages that you have to use. It's just confusing. And then the third problem, and then maybe the most important problem is that how do you then take models of your graph data and map those to whatever it is that you have to do when you're writing code? There's also this question of shared ontologies or shared models. So let's say you have decided to use some shared ontology and you map your data to this particular ontology. Now what? What does that actually buy you? How do you take advantage of the fact that your data now conforms to a particular model? So here I'm sort of pondering, can we support ontologies with some kind of predefined software libraries that basically encapsulates some of the knowledge about this ontology. And I like to call these ontology engines. This is ongoing work on my part. And then finally, since we're talking about ontologies, there's the question of reasoning, which is really something we can do on the RDF side, particularly. First of all, we have symbolic reasoning. So this is really reasoning using some type of logical rules. Interesting things to to to note here is that generative reasoning or reasoning that kind of adds more edges to the graph, if you will, is something that can easily be hidden from the application itself. The application actually does not have to know that there's a reasoner running underneath. Queries and accesses to the graph are really not to the asserted graph itself, but they are to the graph plus all the things that we have managed to infer from the graph. Then, of course, we have non symbolic reasoning. So machine learning techniques can actually be used much like symbolic reasoning. You can do things like node classification or link prediction. And I think what we really need is sort of an expanded view of reasoning where we will kind of stop this sort of rift between symbolic reasoning and non symbolic reasoning. But I think more importantly, what we really need to do is we need to rethink what an application is. I think that logic should be associated with the data and the models and not with an application. Traditional view of an application is that it's sort of the gatekeeper to particular kind of data. And I think a better view of of an application is that it reflects the user's intent to, accomplish something. But in this view, you really have to kind of separate the application from the semantics of the data. I think graphs really could be sort of an overarching logical abstraction for all the data that we manipulate. Modern enterprise data practices, to be, to put this kindly, is messy and data silos, they're commonplace. And data integration often happens through ad hoc solutions or just a custom integration scripts or what have you. And this thing will never end unless we find some kind of a unifying logical not only manipulating the data from the standpoint of our software, not only manipulating the data from the standpoint of our of our software applications, but also from the standpoint of data integration. My view of this is that until something better shows up, RDF and OWL actually can provide this view. I chose this picture here specifically. This is the Tower of Babel. And I think that that's where we are when it comes to data practice today. Lots of languages, everybody's talking, nobody's understanding. And then, of course, we still have this question of RDF versus the labeled property graph, so let me talk a little bit about that. First of all, when users are given the choice between RDF and property graphs, it tends to confuse users. They don't know what to choose and once they make that choice, it tends to be hard to walk back that choice later on if you find out you made the wrong choice. This really should not matter. I think the real issue here deep down maybe that there are these two kind of ways to think about graphs. Graph as a logical representation of your data versus graph as a physical data structure. I think RDF tends to represent the former, property graphs often the latter. But regardless of this, the practical problem we face is that we still have two separate ecosystems for tooling for these two different kinds of graphs. So let's talk a little bit about these graphs, RDF and property graphs. What are the pros and what are the cons? So RDF is a W3C standard. It's well established, originally created over twenty years ago. RDF makes it easy to use external data sources. RDF comes with schema languages And, particularly when it comes to external data sources, RDF has formal semantics, and it particularly has formal semantics for graph merging. And then finally, RDF supports reasoning. So you can incorporate symbolic reasoning in your application. Property graphs on the other side are very intuitive for software more so than RDF. And they tend to integrate better with programming languages, particularly when it comes to the query language Grambling, which in a way sort of is a programming language in its own right. Now on the negative side, for RDF, it's often considered kind of too academic. I don't necessarily share this view myself, but this is what I keep hearing. And related to that is that there's no kind of easy on ramp to learn and adopt RDF. Now, on the property graphs, on the negative side is that there is no standard. There's many divergent implementations that are proprietary. There's no schema language and there is no formal semantics. So this makes it hard to do a lot of things and I'll come back to that. So this is the reality, but what we really want is we want both. We want the good things from RDF and we want the good things from property graphs. And we don't want to be confused with the choice between the two. So this sort of brings me back to the idea that the graph itself is sort of an abstraction. What we really want is a unifying model that covers both RDF and the property graphs. Why? Because this would help adoption of graph technology and I think this is important to the graph industry, if you will. It's still sort of a nascent industry and to really see mainstream adoption, we need to make things easier and less confusing. It also gives users more choices if we were to have this. So for example, I personally would very much like to run Gremlin over RDF, but today I can't do that. So in the Neptune team, at AWS, we're working on this, in a project code named OneGraph. And turns out that this is actually not so easy. There are a number of obstacles. We think we can solve them, but let me just kind of give you the highlights of what things are difficult. So first of all, there's the question of the formal semantics of RDF and also this emerging extension of RDF called RDF star that tries to bring RDF closer to how property graphs work. In RDF semantics, triples are unique, which means that that you can't have two triples that are the same but would be separate so that you could assign different properties to them. Whereas this is a very common pattern in property graphs. And of course, those of you who know RDF, there's the old style verification mechanism, for this, and it does not have this problem, but verification tends to be cumbersome and is often misunderstood. I think it's the most misunderstood part of the original RDF specification. There's, of course, this question of lack of formal semantics for property graphs. It's kind of difficult to determine what the correct semantics are. And so building this unifying and unifying model, it's now difficult because we don't actually know how to reconcile RDF semantics and property graph semantics. On the property graph side, there's also kind of a diversity of scalar, scalar data types, which would have to be reconciled against the set of scalar data types on the RDS. There's question of graph partitioning. So RDF supports a mechanism called named graphs that lets you partition your graph. This mechanism does not exist on the property graph side. Now, to make things even more complicated is that in RDF, vertices are just identifiers. There's no structure. So when you're partitioning your graph, you're really kind of what you're doing is that you're you're asking which particular partition does any particular edge belong to. But on the property graph side, both vertices and edges are structured objects. So what do you do about the partitioning when it comes to vertices? And then finally, there's this question of update semantics. So to come back to the first problem, so let's say we have these non unique triples. So kind of multiple instances of the same edge, if you will. When you delete one of them, what happens to the others? Are those deleted as well? We have to figure out what the right semantics for that is. And then, of course, if you have properties on edges, so you've associated some properties with the edges themselves, What happens when you delete such an edge? Are those properties deleted as well? So would we need some kind of a cascading delete for edge properties? All these are problems that we have to solve before we can say that we can have this unifying model. But anyway, we're working on this, we're getting there. So, the way forward, and this I really would like to present in terms of what are the real pain points when it comes to building knowledge graph software. So first of all is the basic question, how do I write software that leverages knowledge graph and knowledge graph data? We have to kind of bridge the gap between the graph and the code. Your graph has a model, your code needs to understand what that model is, how do you do that? And as I said earlier, we have these very low level interfaces to graphs, and that basically means that you end up writing a lot of boilerplate code. So I think the answer is that we really need to think carefully what the proper interfaces and abstractions are for graphs. So the second question is, okay, so I map my data to an ontology. Now what? So, what does it mean to have some kind of support for ontologies? What is the role of reasoning? As I said, hiding the reasoning works well and and and there are some things that you can pick from RDF and AL that are particularly useful. But how do you do this? At this point, I'm thinking libraries built to support a particular ontology might be the answer. Like I said earlier, I call these ontology engines so that you basically have some software support if you decide to adopt a particular ontology that you use for modeling your data. So then there's the question of property graphs versus RDF. What do we do about those? Choosing between the two is hard. It leads to confusion. We have this effort, the RDF star effort that is going to offer some alleviation to this, this problem, but may not get us quite quite there. For that, the OneGraph Unified Meta Model should bring a relief. And, we recently published a paper on this and, I'll have the link to that at the end of the presentation. And then finally, there's this question of, can we get out of this practice of never ending integrations between data that sits in silos. Modern tools we have do not actually offer a single unified view of data. And I do believe that graphs, particularly built on built on RDF and OWL modeling, can offer this unifying logical view. And that doesn't mean there has to be a physical view of the data. But we need some common language to talk about data so that we can talk about integrations and we can kind of separate the applications from the physical data. So those are my thoughts today. I'm happy to take questions and here are some links, some material about how to build knowledge graphs on using Neptune and then this paper of ours that talks about the unifying model, between RDF and Propertures. Thank you.",
    "transcript_length": 19267,
    "speaker": "Ora Lassila",
    "tags": [
      "amazon neptune",
      "cypher",
      "erdos number",
      "gremlin",
      "json ld",
      "knowledge graphs",
      "object graph mapping",
      "ontology engines",
      "owl",
      "rdf"
    ]
  },
  {
    "title": "Graph Algorithms & Graph Machine Learning: Making Sense of Today's Choices",
    "description": "Graph machine learning is coming of age, with rapid research development and growing interest. Users now have several alternatives to choose from: graph pattern matching, graph algorithms, graph embeddings, and graph neutral networks.\n\nDoes graph machine learning replace graph algorithms? How does graph machine learning different from other machine learning? What sort of system do I need to run these analytical techniques?\n\nThis talk provides a description of these four categories of graph analytics, what sort of problems they tackle, their benefits, and their requirements.\n\nThe explanations will be illustrated with examples for fraud detection, recommendation, supply chain management, and other real-world use cases.",
    "category": "Graph AI",
    "transcript": "Hello. I'm Victor Lee, vice president of machine learning and AI at TigerGraph, and I'm here today to talk to you about graph algorithms and graph machine learning, making sense of today's choices. So what is graph machine learning? You may have heard several things about it. Of course, we start with graph algorithms, knowledge graphs, which have both been around for a long time. Now we hear about graph data science, graph embeddings, graph neural networks. Maybe you know what these team terms mean. Maybe they're less familiar to you. But even if you know them, how do they all fit together? What are the use cases? How should you orchestrate these into some real world use cases? So we're gonna be going through some questions in this talk. Does graph machine learning replace graph algorithms? What is a graph embedding? What's the difference between graph neural networks and conventional neural networks? What are the use cases for these methods? And lastly, what setup do I need to run graph machine learning? These, we hope, are maybe some of the questions that you have, and we'll be addressing them in this talk. So the first thing to remember is that there's more than one type of machine learning. So first of all, we have unsupervised machine learning, which is very data driven. You you just look at the data you have, and you run some automated, analytics to try to uncover interesting things, relevant things, characteristic of that data. You're not you don't know what you're looking for. You just let the data itself tell you. And graph algorithms are an excellent example of unsupervised learning. So in that way, graph algorithms apply to machine learning. Then there's supervised. When people talk about ML, they're actually usually talking about supervised learning, particularly deep learning and neural networks. And so there's usually some, particular aim, like you're trying to make predictions based on some existing data. It's tied to classify objects. If I if I don't know what type of thing this is, can I use previous examples to develop a a model? Think of decision trees. Decision trees are an example of of a model which classifies something. You answer some questions, yes or no, and it eventually gives you an answer and says, oh, you are this type of thing or you should do this. It's it's putting you into a class and learning what well, what are the questions you we should ask in the decision tree. Sometimes you use machine learning for that. And there's reinforcement learning. That's when you just try a task and you're gonna make mistakes, but you learn from those mistakes, and eventually, you learn how to do it really well. So graph can apply both to unsupervised and supervised and and reinforcement. We're gonna focus primarily on supervised learning in this talk. It's also good to look at a typical pipeline or workflow for doing machine learning because this will tell us at what stages and in what ways graph can help. So first of all, at the beginning, when you're just gathering your data and assembling it, you made a a decision, presumably, to model it as a graph. Since we're talking about graph machine learning, we've gotta start off with a graph. So if you've made the decision that the relationships, the connections that you can express in a graph are that added, density of information, I call it. So more understanding how things are connected to each other is often what you're trying that's the nature of understanding data. And so, that's the first way it contributes. The second is kinda cleaning up your data, making sure that things are represented in a standard way, representing things that, might appear as different objects in the original data are in fact the same real world object. That's entity resolution. Trying to find those, making those matches of of different identities that are in fact the same. You can use graph based techniques for that. And when here in the middle, this is the core section where, traditionally, graph has played a role in machine learning. Using graph based features either through either through standard algorithms or writing specific pattern matching queries that that you designed for for your application. And this is all in the data preparation phase. And then we get to the model training. And and this is model training is when people talk about machine learning. That's, again, usually what they're thinking about. And now we have graph neural networks where if you have graph data, you can train directly on that using a graph neural network. So we'll talk about that. And then we have graph embedding. And and, again, that's one of the terms we're gonna define, and I've intentionally put it in between feature extraction and model training because it has aspects of both. So we have answered the first of our questions. Does graph machine learning replace graph algorithms? No. It doesn't because they contribute in different ways. Graph algorithms are great during the the feature extraction and feature engineering phase. They can also be used during the the data cleansing phase. But then you use graph machine learning is really the whole scope. But if you wanted to think of it as just the graph neural networks, then that is a a later stage that comes after you probably use the graph algorithms. So then looking at that pipeline, we saw about four stages where graph can can contribute. So we're gonna talk about each of those phases. The first being graph assisted data cleansing and entity resolution. So one thing that graphs are really good at is is identifying how things can be similar. And in this sense, let's talk about this. In this example, we have persons represented by these larger orange vertices, person a, person b. And then features or characteristics of them are represented by these blue vertices. And think about, say, x might represent a school, a particular school, And this relationship means person a went to school x, and this means person b also went to school x. So they have something in common. We y might be a city they grew up in or a place where they work. Other things they have in common. And so there's a reason why these are because, a city or a school are entities in themselves. That's why they're represented as vertices in the graph. And so there are a variety of similarity algorithms that can be used to compute a score based on this graph structural similarity of a and b. One example is cosine similarity. It takes into account the weights of these relationships and which ones match. And so the score is gonna depend on the weights of these x and y that match, but, divided by the existence of all these other factors whether they match or not. That's one way to come up with a score. And once you have these scores, you can represent them on the graph. And this means that, this group, they all have similarity scores in the nineties. This group, they also have similarity scores in the nineties, but c to e is only sixty five percent, d to f is only sixty percent. So those are lower. We can then do some scores together. If these each represent persons, that may mean of these seven original digital identities, that may mean of these seven original digital identities, we've reduced them or resolved them to two real world identities, Barry Markham and Beryl Markham. So that is, one way that we can use graph for machine learning. Another way is to help develop graph based features using either algorithms or pattern matching. And there are lots and lots of graph algorithms. This is TigerGraph's current graph algorithm library with about fifty algorithms, but they're even more. So we're always developing more. And they each have their different uses. The first thing you wanna do is understand the general type. Is it the similarity algorithm? Is it a centrality algorithm? Is it a community algorithm? And then there are different varieties among those. And then they those all lead to different uses. So here's just one example. And say you're trying to do some, data preprocessing to help predict, fraud. And so in this data, we have different application programs and then which have personal, identifying information. So this is your private information, which you don't want unauthorized persons to see. And by informing this graph, we can see that there's a cluster here, and there's a cluster here. And so and then we we apply a a community detection algorithm like Louvain. And then we move on to the next step when we apply other graph algorithms to each of these communities to score them, so to speak. So we have page rank. We have, LCC, another community detection algorithm, and diameter estimation, which has to do with, like, what are the shortest paths to get from anywhere to anywhere. So the diameter is the so called worst of the shortest paths toward the longest of the shortest paths. So we come up with an average page rank score, an average LCC score, and the diameter of each of these components. And you see, like excuse me. These scores, you can think of, they're numeric, and there's a set of them, and you can put them in a particular order. And, hey, that's a feature vector. So you now have a feature vector to describe this community. You can then put that into, machine learning algorithm to train it to detect which communities represent ones with fraud and which ones not. Because fraud tends to happen fraud is an interaction activity. You don't do fraud in isolation. It's always somebody interacting with somebody else. That's just one example. This was taken from a presentation that one of my colleagues presented. So if you wanna find out more, you can find other use cases from this presentation. It's not always standard algorithms, but sometimes it's it's designated pattern matching queries that you wrote for a specific application. In this application, we have phones and phone calls between the phones to form a graph. And the the phone service provider, the telephone company, is trying to do some automated real time detection to see if the phone call is is from an an annoying unwanted caller, a spammer, somebody who is, you know or maybe running some kind of scam. And so we extract we design several. In this case, several means a hundred and eighteen different pattern queries such as, is there a pattern that's a a four hop loop? Is there a pattern of calls to people who were called frequently over a certain time window? Is there a pattern of people who were called and then call each other forming a little subcommunity within a certain time window. And each of these features produces a score. Maybe it's yes, no. Maybe it's it's how often that pattern occurs. And so you get a feature vector. And then using that feature vector, we can, again, train it to detect in which cases, are these is this caller, the hub caller, caller number one, the source caller, somebody who might be a spammer or a scammer. So that's the second way. Again, graph can help us with data cleansing. Graph can help us with the very important phase of feature engineering. And now we're starting to get into, getting into machine learning traditionally thought of, and into the the model building. So one of the challenges with graph machine learning is that the data is so rich. It expresses such a wealth of information that sometimes doing this analytics can be expensive. And, also, conventional machine learning techniques are based on matrices, not on graphs. So there's a difference in the data representation, and there may be too much information to conveniently handle. So enter embeddings as a possible solution. What is an embedding? That was one of our questions. An embedding transforms high dimensional data into a lower dimension. An everyday example is how do we represent our spherical Earth, our planet on a map? We all read two dimensional maps, but the world is three-dimensional. A local map seems not a problem because the the Earth is relatively flat in that small space. But when you look at the whole globe, we have squashed the globe onto a flat projection. And that projection is an embedding that is a transform from three d to two d. And so you preserve the important details. You may have, you know, have some error in some of the less important details. And you can see here examples of three different ways to map the the world onto a flat surface. So in the case of a graph, what we wanna do is we start with the original graph. We take into account all the edges, the structural relationship of the graph, but we wanna reduce that down till we just have individual vertex vectors. So if these are the, the seven vertices plus their edges, we now just have these seven rows, and each row produces a a vector of latent features. They're not features that you can really describe, but a key is that this is relatively compact. And so now we have tabular data, which is relatively compact, makes it amenable to traditional machine learning. And the the characteristic of these embeddings is that if two vertices have are structurally similar because they connect to similar things in the in the same quantity, with the same similar types of paths, the same type of things, then they will have similar vectors. You can use simple algorithms that compute the similarity between two vectors, like cosine similarity, and you can make recommendations. You can group them together if they're similar and do classification. So you can do fraud detection. Again, all the use cases that we had before, you can still apply. But we've transformed the data. We've made it more scalable. We've made it more compact, and we've made it tabular, which means that you can now apply traditional machine learning techniques. So, again, why you do it? So to get a dense tabular format, and to gather graph features without manual extraction. Oops. And is it suitable? It's it's better if your data is relatively stable because if the data changes, you will have to retrain it. So this answers our second question, what is a graph embedding? It's something that transforms this graph structure into a compressed tabular format. And we're now on to the the fourth way that graph can help with with machine learning, and that is graph neural networks themselves. And start with ordinary neural networks, a powerful machine learning technique to predict and classify. That's why they're so well known, so well used. And then we have graph, which is a great way to get insight through connected data. You can somehow combine these. You can get what we call a graph convolutional neural network. So you take the traditional, neural network data flow, and you insert an additional step called the graph convolution. And what the graph convolution is doing is if each of these nodes represents a vertex in the graph, and so we're trying to learn about each each vertex in the graph. We have a convolution step where we take its features, this vertices features, and combine it with the features of its neighbors. And maybe maybe it's an averaging. Maybe it's something a little more complicated than averaging. But in some way, you combine and or convolve, as they say, the features of this with its neighbors. And that's where the graph information is coming in, looking at neighbors. So combines the added insight from connected data with the modeling power of neural networks. And so you're actually using the graph structure doing during the training. It's not just a preprocessing step, but it's you actually use the graph during the training cycle. The basic one we talked about is for a so called homogeneous graph when there's only one type of node, one type of edge. If you're familiar with other types of graph neural network other types of neural networks like attention networks or recurrent networks, There are graph versions of those. There are also algorithms for heterogeneous graphs where there's more than one type of node and more than one type of edge. And what's better, there are now, three major open source GNN libraries. All, PyTorch has one. TensorFlow just introduced one. And there's one DGL deep graph learning, which is just for graph. And so you have several choices for open source libraries, all in all in Python, of course. And so why would you use a graph neural network? If you have a graph where graph structure is important and you have a task where a neural network makes sense, then you probably should consider using graph or neural network. So you don't need to explicitly extract graph features. The training will do that. As I've said, they're they're a well established software framework, because you're you're just using a neural network. It's in Python. You can use a lot of the the same infrastructure. And so that answers one more question, and we'll we'll sort of say what's the difference between a graph neural network and a conventional neural network. It's that convolution phase when you're taking the local graph structure into account during the training. And I said, we're just gonna say, we've talked about some use cases. There are lots of use cases. Don't have time to get into all of them. But, again, if if you have graph data, you can apply these. You're gonna get some benefit. The last question we're gonna look at is what setup do you need for graph machine learning? Learning. And here it's useful to again look at our pipeline. So first off, you're gonna want a graph database and, hopefully, a scalable one to to make sure it's large enough and performant enough for the task you need to do. Because you're gonna be doing some computationally complex or intense stuff. To do the data cleansing, you're gonna have to be able to do some queries and updates. So you wanna be able to, you know, update your data in the graph. And you're gonna need a graph algorithm library as well as a graph query language. You know, all databases come with a graph query language, but you want them which is flexible enough to express some of those patterns such as the one in that, unwanted phone call application we looked at. And lastly, you need the capability to run graph neural networks. So where are you gonna get these capabilities? Oh, one more. Graph embedding also. So all of these are or should be available within a database. Obviously, this is a database, and you wanna be updating the data in the database. So, ideally, you can, run queries, update, read, read, write, read, modify, write queries here. And in this phase, you want a graph algorithm library incorporated in your database as well as, you know, a graph query language can also extract custom features. For this last phase of model building, you know, traditionally, you would export your data to machine learning server, but it's becoming more common that customers are looking for in database machine learning. And why would they do that? It's because it simplifies and shortens the pipeline. You bring the data in. You do your feature engineering and data cleansing in database. You train in in in database. You deploy in database. So there's, you don't have the the time and expense of pushing data from one place to around to another, it becomes a much more data centered operation. You you bring the analytics to the database rather than moving the data moving the data in a physical pipeline. And so last, I think that that covers all of our points. So let's let's review. So graph machine learning and graph algorithms contribute in different and complementary ways to analytics and machine learning. So it's not an either or choice. What is a graph embedding? It's something that transforms the graph structure to a a more compressed and tabular format for compatibility, with more traditional machine learning. Or you can do graph neural networks, and you actually can use both, but probably you would either use a graph embedding with a traditional neural network or a graph neural network. And a graph neural network takes local graph structure into account during the training. And lastly, if you wanna take advantage of all of this, you're gonna need a scalable graph database with flexible graph querying and doing those read write updates. You're gonna need a graph algorithm library and either an internal or external graph machine learning library with the advantages of of in database machine learning where you can do the training in the database, are starting to become apparent. And, again, we we hit on a few use cases. There's so many more. So I'll just say they abound. And so thank you very much for your your attention, and I, hope you got something out of this talk. Thank you very much.",
    "transcript_length": 20675,
    "speaker": "Victor Lee",
    "tags": [
      "DGL deep graph learning",
      "LCC",
      "Louvain",
      "PyTorch",
      "TensorFlow",
      "attention networks",
      "community detection algorithms",
      "cosine similarity",
      "decision trees",
      "deep learning"
    ]
  },
  {
    "title": "Graph Analytics vs Graph Machine Learning",
    "description": "Graph Analytics has long demonstrated that it solves real-world problems including Fraud, Ranking, Recommendation, text summarization and other NLP tasks.\n\nMore recently, Graph Machine Learning applied directly on graphs using graph algorithms and machine learning, has been demonstrating significant advantages in solving the same problems as graph analytics as well as problems that are impractical to solve using graph analytics.  Graph Machine Learning does this by training statistical models on the graph  resulting in Graph Embeddings and Graph Neural Networks that are used to complex problems in a different way.\n\nIn this talk, we will compare and contrast these two approaches (spoiler: often complexity vs precision) in real-world scenarios. What factors should you consider when choosing one over the other and when do you even have a choice?\nJoin this talk to learn about exciting new developments in Graph ML, as the  graph techniques on which they are based.",
    "category": "Graph AI",
    "transcript": "",
    "transcript_length": 0,
    "speaker": "J\u00f6rg Schad",
    "tags": [
      "Data AI Summit",
      "DeepMind",
      "Gartner trends",
      "GraphML",
      "GraphSAGE",
      "RankoDB",
      "Stanford",
      "Uber",
      "betweenness centrality",
      "breadth first search"
    ]
  },
  {
    "title": "Graph Thinking",
    "description": "This talk explores Graph Thinking as a means for conceptualizing problems which can be solved using graph technologies.\n\nParallels can be found in learning theory, for example how people organize knowledge into graph-like cognitive structures as they progress from novice to practitioner to expert levels in a given field.\n\nThis talk introduces a set of intuitive examples which convey the power of graph technologies \u2013 plus their trade-offs \u2013 to domain experts, to be used as a starting point for new graph projects. \nThis approach has been refined through business use cases in industrial AI for firms in EU, and attempts to overcome some of the cognitive hurdles that organizations face during large graph initiatives.\n\nTo put this into context, we'll review a set of common use cases in industry and how graph data science practices can be built using Python open source, along with a survey of available libraries to leverage for different aspects of graph technologies.\n\nThen we'll show the 'kglab' abstraction layer whichintegrates these various libraries into the PyData stack",
    "category": "Knowledge Graphs",
    "transcript": "This talk is about graph thinking, and the slides are online. There's a URL shown here. Actually, there's a lot of background material, a lot of links. Let's, let's consider a scenario. There's a village somewhere in the woods. In our village, there's someone named Pat who runs the pub, local pub. Pat has a couple of friends, Hannah and Thomas. Now Hana works the fields, grows the grain. Hana has a friend named Aiden. Thomas works poultry, raises hens and produces eggs. And Thomas has a friend named Brenda. And then Aiden is the miller, Hannah's friend. Hannah sells grain to Aiden. Aiden, in turn, has a friend named Chris. And Brenda, Thomas's friend, works the brewery. Brenda buys grain from Hannah and she makes beer, which she sells to Pat, who runs the pub. Brenda also has a friend named Kim. And then Chris, Iden's friend, buys eggs from Thomas and buys flour from Iden's mill and produces bread. Chris works the bakery. And of course, his bread is sold to Pat, who runs the pub. And then there's Kim, Brenda's friend. Kim works the recycler, and Kim buys organic waste from Thomas and from Brenda and Chris and then mix fertilizer to sell back to Han. So we had a graph that we described there. Now if we're gonna put this into a normalized form for a relational database, you see the schema here on the left. You would see maybe six tables if you had a fully normalized form. And the thing is, the information about the relationships in that village are completely dismantled and destroyed and atomized here by having a relational view. How can you look at this and tell what those relationships are? On the other hand, if you've got a graph, you're presented with exactly that context: the relationships of who knows who in this village, who produces what, who sells to whom, and just the general flow of commerce that's going on around inside of this little Black Forest medieval village. Now the thing is that graphs bring a kind of network view. They bring the data closer to people who can make sense of it. This is acknowledging the the complexity of the context. This is about identifying emergent patterns. This is about being able to make informed decisions based on the relationships that exist. That is the essence of graph thinking. Really it boils down to thinking in patterns. So let's get a couple of examples here. Hannah's relatively new in the village. She'd like to expand her business. She's noticed that one of her customers, Brenda, happens to buy a lot of grain. Now who are the other villagers who are similar to Brenda? Perhaps she could upsell. Well, doing a little bit of work with the graph here, you can see there's another person named Chris who also sells product to Pat, also sells waste to Canon. You know, maybe Chris's bakery would be a direct customer of Hana. I don't know. Selling grain directly to a bakery. Maybe they could sprout or make malt or something like that. Hannah is also interested in sponsoring a co marketing campaign here in her medieval village. And so she'd like to try to help drive the demand for more grain. So we can do some analysis here. Who are the customers of Hannah's grain customers? And so for this, we do some graph reversals. We're looking a couple of hops out. We noticed that Chris, Pat, and Kim are each a minimum of two hops away. And it turns out we heard about Kim a couple of times here. And just to make it more interesting, a tech billionaire uses time travel to relocate back to this medieval village in the Black Forest. Which businesses are the most influential as potential acquisition targets? Well, it turns out if you do some graph algorithms based on this data right here, there's a family called centrality. There There's a variant of this called between the centrality and it's really looking and seeing which nodes are between all the other nodes. If you calculate that out, you find that in fact Hana has the highest ranking there for between and centrality. Chris has the second highest ranking there. So, you know, these are the businesses that probably would be your acquisition targets for Musk or, you know, whomever. And it's interesting to me that when I first showed this graph to someone who has absolutely no background on graph algorithms, my my friend, she was able to immediately look at the graph, look at the relationships, and say, Hana's really hanging out there. I mean, the person making the grain is raking in the money. And indeed, that's what graph algorithms show. This is about the complexity of relationships. Now there's a lot of background when we study complexity of relationships. I wanna cover these in a couple of ways. If you go back to nineteen ninety nine, Dave Snowden was a consultant at IBM, and he developed a framework called Kinev, which tries to assess a kind of context in which business leaders are confronting problems and need to be able to apply decision making. You may have heard this referenced. It it was where, sort of the origin of unknown unknowns, which of course was referenced after nine eleven. But, in the Cynefin framework, you proceed from, say, simple context where there are established facts. You just need to go in and categorize the needs and apply the best practices, the rules. Or you can progress in more complicated kinds of situations where some kind of expertise is needed, some kind of analysis. So analysts go in, they assess the facts, they provide their analysis, and then leaders can respond based off that analysis, sort of, you know, warehouses and business intelligence. But then you can get into more complex business environments where you really must probe the situation. There's no clear answer. And the way through it is to be able to sense emergent patterns and make informed decisions based off of understanding those patterns. And that is the complex side of business decision making. And increasingly in our world, we're seeing more and more kinds of complex challenges, whether we're talking about tangled supply chains or pandemics or on and on, climate change, etcetera. The world of business has become increasingly complex as we've globalized, and these are the kind of things here that must be applied. This type of confronting complexity as opposed to sweeping it under the rug and just trying to apply best practices. Now there's a corollary also if you look at pedagogy, if you look at learning theory. Susan Ambrose, has a book at a two thousand ten called How Learning Works, and part of this describes sort of the journey from being a novice, a complete beginner in a particular subject, advancing into someone who's more advanced, becoming more become competent practitioner, and then eventually becoming an expert. And what we find as we're teaching, as as people are learning about a new subject, when they're novices, they start out with memorizing some facts, mostly disconnected facts. As people become more advanced, they gain more understanding. They start to string together the facts. You get this kind of linear thinking. Amongst competent practitioners though, we see really what what could be called decision trees cognitive structures that are much more tree like in terms of decision making. But when people move into expertise in a field, what we see are essentially learning how to break the rules, not just following those decision trees blindly, but understanding where they apply and where they they don't necessarily apply. And that creates cognitive structures, which are graphs. We should learn something about this that essentially when we're talking about working in a complex context, when we're talking about learning perceived emergent patterns in a in a complex challenging situation and being able to gain expertise on how to behave in that kind of environment, we need to go move toward more graph like cognitive structures. And this should be definitely a clue for the way forward with AI. Relational data management and reporting, that all arose from simple business contexts. And that led to things like data warehouses and data lakes, practices like business intelligence. But the the complexities, the uncertainties of the twenty first century, this really forces more where leaders must increasingly rely on sense making by leveraging the use of graph patterns. Now in contrast, there is something called ambiguity aversion. If you look this up, it's it comes from cognitive psychology but also behavioral economics. And it has to do with how when faced with uncertainty, many people will do exactly the wrong thing. They'll make exactly the wrong choice. This is highly important for AI applications in terms of helping organizations and leaders be able to augment their decision making processes to understand that, in fact, some people just try to sweep it under the rug. But really, we must be embracing the complexity, working with emergent patterns, working with graph thinking. I'd like to acknowledge a good friend colleague, Juergen Buehler from, BSF in Germany. Juergen and I put together this scenario of the village to explore graph thinking and help to illustrate this concept. We have an article also that goes in a bit more detail that that's on Medium. So talking about graphs. You know, the the thing is that in business graphs, connected data is everywhere. Now when you talk with people, when you teach people about doing data science, when you talk with people about data, they typically respond by describing a table, rows and columns, rectangles, matrices, spreadsheets, reporting tables, these kinds of things. People are trained to think in terms of square patterns when they hear the word data. And, okay, that works except when when it's not the case. Spreadsheets, SQL reporting, all these rely on graphs. Within every Excel spreadsheet, there's a dependency graph. That's the key to calculating it. Within every SQL query, there is a query plan which is directed acyclic graph. There are complex ERDs for being able to represent the scheme and resolve that. The fact is that these kinds of computational techniques rely on graphs. And in fact the metadata and the business rules which go part and parcel with these internal graphs, they get obscured by the kind of tabular format. It becomes difficult to troubleshoot and test and reuse an audit. It creates technical debt. And if you don't believe me, understand that ninety five percent of the global two thousand companies in the world, when they do their final tax reporting, the final stages of ninety five percent of the firms is done in spreadsheets. And those spreadsheets are not consistent from one quarter to the next. That is tech debt, and it's a problem. Gartner had been somewhat iffy with regards to graph technologies. However, in early twenty twenty one in February, Gartner did an about a face. They're saying that, graph technologies will bump up to eighty percent of data analytics, up from ten percent usage in twenty twenty one. Those are very rapid change. And what they're pointing toward is this. By exposing metadata and business rules, the very thing that gets obscured by spreadsheets and relational databases, which leads to tech debt, This is what graphs surface and allow domain experts to be able to manipulate. And so we're seeing a lot of rise of of graph technologies, this kind of usage throughout industry. It's appalling to me when I talk to people about graphs, they immediately say, oh, well, that's that's just for Google or Facebook, which is which is utterly ridiculous. When you look at it in the industrial applications, I I know use cases, for instance, in manufacturing where one single instance within a manufacturing firm of their graph applications, one use case alone is larger than the entirety of Google's knowledge graph. And this particular company has dozens and dozens of different use cases. So I I I think that this is a conceit to say that the the Silicon Valley tech firms have a lead in graph. In fact, they're laggards. The real problem with graphs, this is happening out in finance, happening in pharma, in manufacturing, and a lot of areas of regular industry, not tech companies. The common themes that we see have to do with data integration across business silos, having to do essentially motif mining, understanding data objects as shapes or topologies, geometries, grappling with complexity and uncertainty, working on disambiguation problems, working on, you know, eliminating cycles and data, and also being able to drill down the details, which are which are critical, more important than just receiving a bunch of aggregate accounts that you would typically get out of a lot of BI tools. You know, I'll point out a few here. Fudome, with Albert Lezlo, Barbasi, really, brilliant work is they've invented the term of network medicine. And there's a nature article that describes what they're doing. If you haven't seen this so far, it's one of the most sophisticated uses of CRAP technologies and probably some of the most major impact of what we'll see in terms of of human outcomes. Also, you know, I'll point to drug discovery from companies like Novartis and AstraZeneca, and we can talk about Roche and many others. I'm pointing here especially to, Stephen Reilings talks out of Novartis using graphs for, drug discovery, Connor Hill at AstraZeneca, in manufacturing. I mentioned about my colleague, Jurgen Buehler at BASF, also, Stefan Lempater and Thomas Hjubauer at Siemens. You know, definitely in the tech firms, you do see things like Amazon's product graph, and definitely Luna Dom is doing some amazing work there. Mark Grover was formerly product manager of data at Lyft. He's done some amazing work, talking about their use cases for metadata management using graphs. Refinitiv and Bloomberg and others, of course, in in the tech space, but, you know, Tom Baker and others talking about that with fintech data. And, and certainly, if you get a chance, check out some of the talks from Charles Hay, Sheng Ma Hay at, Ant Financial on what they're doing. And and if I were trying to to sort of paraphrase a lot of these industry presentations about levering craft technologies, one of the models that I use to try to understand these these case studies is essentially this triangle of know your business, know your customer, and know your data. So this is a a bit of rounding the edges, but it's it's sort of a a middle model for me to try to understand how are these use cases related to each other. Okay. Let me shift gears a bit. Let me talk a little bit about the graph theory because I I think it's very important for understanding how do we leverage what's happening in hardware. I'll just say that. So, we've had really good math to handle graphs for a long time. And a typical kind of thing is it's inside of algebraic graph theory. The idea is you have a complex graph and you can take parts of it and push it into a vector. You can vectorize it. And and we do this when we're training neural networks a lot. Alternatively, you could take, and represent the edges and relationships in a graph by using something, a matrix representation. So non negative matrix factorization, using things like an adjacency matrix. Or you can get a little bit complex and use something called a tensor, which is essentially like a three an n dimensional matrix, if you will. So there's ways to go between what we call algebraic objects, vectors, matrices, tensors, and graphs. You can transform back and forth. And for a long time, the trick was to use the matrix approach. There's something called non negative matrix factorization, which says, for instance, we'll run page rank. We'll take a graph. We'll make a big matrix out of it. We'll apply a lot of linear algebra and do a lot of transforms and compute up the results, the rankings that we need. And there's some really fascinating work on this. I definitely point you toward Tim Davis at Texas a and m. He runs the, Sparse Matrix Museum. You see pictured there, also David Glack at Purdue and others. You know, some of these folks, Tim Davis, of course, have been working on graphBLAST. Really fascinating work in terms of factorization, but that's kind of what we've seen in the past. There's been a lot of that. Sometimes you can't just get away with this trick of taking a complex graph and a lot of symbolic relationships and take that information and making it all numeric and then number crunching. Sometimes you have to work with symbolic. So certainly when you're working on graph visualizations, when you're calculating deep learning models, training them, when you're recalculating graph algorithms, you need to take your graph and put in some sort of numeric representation usually to be efficient. But when you're working with natural language, when you're understanding regulatory compliance audits, when you have human loop, like, some type of active learning situations, when you're trying to represent rules from domain expertise or explainable AI, these are cases where you need to work more in the symbolic representation. So, what I recommend is this thing with all apologies to Daniel Kahneman, what I call thinking sparse and dense. And this is the idea that when you're in a data workflow describing a pipeline and all the transformations across the data workflow, there are stages that are more sparse and stages that are more dense. Typically, when you're doing your data preparation, it's usually more sparse. You pull things together and oftentimes these are relatively bandwidth limited. But then when you go and train a model, for instance, it's probably more compute limited. And that's not always the case when you're doing a deep learning model. You know, parts of a deep learning model, of course, the convolution layers, those will be compute limited. A lot of vector processing there. But when you have to calculate a loss function in deep learning, this is bandwidth limited. So thinking sparse and dense across a problem, you know, typically once you've calculated and trained your model, then you have to apply it with real world data. So you sort of shift from sparse to dense back to sparse. We go through this in a lot more detail. Dean Wampler and I did a a report, working with the open source machine learning, technology leads at NVIDIA, earlier this year, and it's a free download if you wanna check. It's called hardware greater than software, greater than process. And at the core of this is this mathematical idea that we can go from a relatively complex graph and then project it into some other sort of space, reshape it as a matrix, reshape it as a tensor, do our calculation, transform it back, and then populate the answers that that we've gained to the calculation back into the graph. So a lot of transforms and inverse transforms. And this is really crucial for being able to leverage contemporary hardware. And there's a whole bunch of evolution going on right now in the hardware side for accelerators, for GPUs and FPGAs and others, etcetera. What this points to is a general kind of narrative in data science about, you know, starting with unstructured data and progressing to more and more structure and then leveraging that structure. And that is about dimensionality and how to really manage dimensionality. It's a really essential component of this trade up between numeric and symbolic representation being able to shift back and forth to those transforms. So it's it's a lot of how we're projecting this notion of graph data science, how to apply graph technologies in a very formal way at different points in a workflow and how to really manage and get the best leverage out of it. There's a GitHub repo and a community of developers around it. Really, we we wanna tackle a few things, but, you know, some of the main parts about this were that there's a lot of great tools, and they don't necessarily play well with each other. There's a lot of camps that are working graph technologies, different parts, but they weren't really talking with each other. And we wanted something that would would work well in open source with, you know, a typical data science, py data type technology stack. But at the same time, still be very suitable for for parallelization with things like Dask and Ray and RAPIDS and Spark and that. So we've been working on a lot of integration. We pull together a lot of packages into this kind of abstraction layer. And I I want to show a few of these here. You know, one is just when you're building the graph, if you want to work with w three c standards, RDF, composing, graphs out of triples, basically, being able to use a number of different controlled vocabularies. So we support this and make it a lot simpler than the low level tools like you would see in already ebblib. So with KGLab, you can define your namespaces, instantiate a graph, and then just start adding different nodes to it, leveraging those namespaces. If you're familiar with w three c, there's, like, a dozen different standards for how to serialize. And, of course, we pick up support for those along with making use of things like Pathlib and FS spec codecs and all that. Of course, this has gone into RDF lib six, but we definitely have support there. You know, along with this, a bunch of other formats too if you wanna work with dot or or, d max or in, you know, some of these as well as CSVs. But the interesting thing that we found is that Apache Arrow and Apache Parquet are orders of magnitude more efficient, actually more than two orders of magnitude more efficient than, say, using something like CSVs or or some of the forms of, like, w three c standards for serialization. So, we highly recommend using these, and, and it really fits with large distributed graphs, by the way. There's a number of excellent tools in the visualization space. These don't necessarily blend all that well with, say, RDF lib. So we built out paths to be able to do the transforms back and forth. Here, I'm showing how to do a transform for something called pyvis. We have integrations with Cairo and Matplotlib, of course, but also with our forensic Graphistry, which I highly recommend. It's a GPU accelerated. As far as querying, you know, you can do sparkle queries, but then you get back a named tuple iterator or a pandas data frame. Again, something that's just much easier to use with the PyData tech stack. Shackle is, I think, one of the one of my favorite more recent additions to w three c stack and certainly being able to do shape constraints for validation and prescription. Really a lot of great use cases like unit tests. Of course, there's a there's a wide range of different types of graph algorithms, but we provided pathways for integration of network x, iGraph, and, of course, cuGraph for, out of RAPIDS for GPU use. And and we've been looking at graph tools as well to integrate that further. There's been so much going on in this field of graph neural networks and PyTorch geometric etcetera. And so we we do support pathways for integration with PyTorch geometric and there's also some inference that's been integrated in terms of using statistical relational learning that is to say probabilistic graphs. You can provide rules that are probabilistic predicates and then run probabilistic soft logic on them, for instance. And I find this is really, really useful because it's a way of representing uncertainty per node, per edge, but also for parts of the graph overall. And, of course, there's a lot of other types of inference when we talk about graphs. We could be doing closures in OWL or RDFS. We could be doing transitive and SCAs. Of course, deep learning graph neural networks is a form of inference. There's also, you know, types of inference that can be drawn from graph algorithm use such as clustering. There's creative ways to do inference with shackle. What I want to point out here is that not all of these are quite the same. In fact, you can plot them. Some are better in terms of formalism and more analytic solutions. Others are better at working with messy noisy data and represent and being able to represent uncertainty. The point that we wanna make is mix and match. For a given use case. Find out how can you blend these together. We see how these types of operations fit in at each point. Thank you very much. If you wanna get a hold of me, here are some links. Look forward to talking to you on Twitter and throughout the rest of the conference",
    "transcript_length": 24519,
    "speaker": "Paco Nathan",
    "tags": [
      "Apache Arrow",
      "Apache Parquet",
      "Cairo",
      "Cynefin framework",
      "Graphistry",
      "KGLab",
      "Matplotlib",
      "OWL",
      "PyTorch geometric",
      "RDFS"
    ]
  },
  {
    "title": "GraphEDM: A Unified Framework for Machine Learning on Graphs",
    "description": "There has been a surge of recent interest in learning representations for graph-structured data.\n\nGraph representation learning methods have generally fallen into three main categories, based on the availability of labeled data.\n\nThe first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure.\n\nThe second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning.\n\nThe third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure.\n\nIn this talk, we present our recent GraphEDM unified framework which bridges the gap between these disparate bodies.\n\nWe use GraphEDM to describe many popular graph representation learning methods such as DeepWalk and Graph Convolutional Neural Networks.",
    "category": "Graph AI",
    "transcript": "Hello, everyone, and welcome to our third, session in the innovators track for knowledge connections twenty twenty. It's been really interesting so far, and we're pretty sure, you'll be, satisfied with, this talk, as well from, Ines Sami, who's a PhD researcher in Stanford University. Ines will be sharing with us today, Graph EDM, which is a framework that, they have developed with her colleagues. And it's about graph machine learning and neural networks and different classifications. Hello, everyone. My name is Ines Chami. I'm a PhD student at Stanford, and today I will present a recent framework for buffer presentation learning. This is a joint work with my collaborators from Google and Stanford. So this talk will be split into three sections. I'll first start by reviewing the problem setup for graph representation learning and discuss different variations. I'll then introduce our graph EDM framework and use it to describe some graph representation learning methods, both supervised and unsupervised. So let's first start by reviewing the graph representation learning problem setup. As you probably know, graphs are a universal data structure used to store relational data. In this representation, node represents objects and edges represent relationships between them. These graphs are prevalent in many real world applications. So for instance, knowledge bases such as Wikidata, which contains, the Wikipedia, data store knowledge about the world in the form of the knowledge graph. Form nodes are world entities and edges represent the relationships between them. Molecules can also be represented as graphs where nodes represent atoms and edges chemical interactions between them. And another important application is in the domain of biology where the study of evolutionary relatedness between species is done using phylogenetic trees. Note that there are many other applications not listed in this slide, such as web and Internet graphs, recommendation systems, economic and transportation networks. And so the summary here is that these graphs can be used as a shared vocabulary between fields. And if we can develop methods that operate on graph structured data, then we can have potentially impact on many important applications. To apply machine learning methods and neural networks on graph structured data, we need representations that a machine learning model can operate on, and these are usually feature vectors. This is the goal of GRL, which aims at learning representations for each vertex in a graph while preserving the graph information. So for instance, if two users have many friends in common in a social network, then good representations should preserve this information by making the known representations similar in the embedding space. And these representations can then be used as input features in downstream machine learning applications. However, graphs are sparse, discrete, and high dimensional objects, and so it's not clear how to get a mapping from a graph to a low dimensional, dense, and continuous feature representation while preserving all the important graph information. And so we'll see next how to design models that solve these challenges. So there are two main learning scenarios for graph representation learning, supervised and unsupervised. In the unsupervised case, all we have is the input graph structured data, and we wanna get representations that preserve the graph information. Examples of unsupervised tasks include link prediction, where we wanna predict whether two nodes are likely to become connected in the future. For instance, in social network, we may wanna predict future friendship relations. Other examples include graph visualization and graph clustering, where we wanna put vertices in a graph into different clusters, for instance, to detect communities in a graph. In the supervised setting, the goal is slightly different. And while we do wanna get some vertex representations, the goal here is to get representations that are predictive of some graph properties. So for instance, in node classification task, we wanna predict the attributes about nodes in the graph. For instance, we could try to predict fake accounts in a social network. And note that in contrast with the clustering task, node classification is supervised in the sense that we do need the label classes beforehand, and we need some training data to train the model. Finally, another important supervised task is graph class classification where we may wanna predict an attribute about the entire graph. So for instance, in chemistry, we may want to predict whether a molecule is toxic or not, and this is a graph classification problem. So let's now introduce our GraphiDM framework to describe methods for unsupervised graph representation learning. The input in unsupervised URL is a graph, which is simply a collection of nodes and edges, and we store the edge information in the form of a sparse matrix with zero and ones, which is basically the edges in the matrix. We may also have node features representing the vertices in our graph, and we'll talk about these in more details in the supervised learning step. In unsupervised GRL, we set mappings from vertices in the graph to vector representation such that graph similarities are preserved in the embedding space, and we'll see next how to define these similarities. So let's now describe our GraphiDM framework. As we've mentioned before, the input to GRL models is a graph that comes in the form of an adjacency matrix decoded to construct an embedding similarity matrix. And these embeddings are then decoded to construct an embedding similarity matrix. Decoder network can be as simple as taking outer products of the embeddings or be a more complex neural network. The parameters of the encoder and decoder are then learned via gradient descent using an unsupervised objective function, which essentially tries to make the embedding similarity matrix as close as possible to the graph similarity matrix. The graph similarity matrix can be defined in many different ways, which gives rise to different embedding methods. So for instance, some methods may wanna preserve the first order graph proximity, namely the AGSNC structure, while other methods want to preserve higher order graph similarities. After optimization, we can extract the word embeddings and use them as features for downstream machine learning tasks. Let's now go over a simple example of encoders, namely the shallow encoder, which is a simple embedding lookup. We can get the embedding of a specific node by doing a matrix multiplication of the embedding matrix by the one hot vector of the corresponding node. The parameters in shallow encoders are the embeddings themselves, and there are no functions computed on top of these embeddings. In other words, a shallow encoder is a simple vocabulary lookup which stores a single vector per node in the graph. Note that there are many other possible encoders such as graph neural networks, which we will describe later in this talk. So let's now look at an example. We focus on the graph factorization model, which is a popular graph embedding method that preserves first order proximity in the graph by learning essentially a lower rank factorization of the adjacency matrix. In this model, the encoder is a simple embedding lookup as we've seen in the previous slide, and the decoder takes the outer product of the embedding matrix. That is, the embedding similarity between two nodes is simply their inner product. The embeddings are then optimized by minimizing an unsupervised loss function, which encourages adjacent nodes to have a high embedding similarity and non adjacent nodes to have a lower similarity score. Note that taking the embedding's outer product defines a symmetric similarity function, which may have, which may be a limiting assumption when working with directed graphs as some nodes can be strongly connected in one direction and disconnected in the other direction. There have been follow-up works for directed graphs which basically define two embeddings per node, a source and target embedding, depending on the direction of the relation it appears in. Another line of work seeks to preserve higher order similarity in the embedding space using random walks. In skip gram GRL, the decoder function is also an outer product but the graph regularization term is computed over random walks in the graph. So for instance, DeepWalk draws analogies between graphs and language by generating sequences of nodes visited during random walks and treating them as words in the sentence to get representations. All these extensions are described in detail using GraphiDM in our paper. So let's now move to supervised URL methods. In supervised URL, the input data is the same as in the previous case, but the goal is now to learn representations that are predictive of some vertex or graph properties. In this setting, we have some additional information about the graph, namely vertex or graph labels, and we want to learn mappings from vertices in the graph to some target label space. One possibility is to apply unsupervised URL methods to get representations and then use them as input in a simple neural network. However, an important limitation of this two step approach is that unsupervised representations might not preserve important information about the graph that would have been useful for the downstream supervised task. Supervised URL methods overcome this limitation by learning representations and solving the predicted task end to end. Finally, note that node features play play a critical role for predictive tasks. So for instance, in citation networks where nodes represent papers and edges represent the citations between them, we could have had feature vectors representing the paper such as what you're making for all the words in a given paper. And this information can be critical in some supervised settings. So for instance, if we wanted to predict a paper topic, then knowing that some keywords are mentioned in a paper can significantly improve the classification accuracy. For supervised methods, we need to add an additional branch in the GraphoDEM framework to account for the label supervision. Essentially, we add another decoder, which predicts labels from embeddings, and we still keep the unsupervised decoder, which can be used to regularize the embedding and account for the graph structure in the embedding space. But note that some methods discard this unsupervised decoder, and we'll see next some examples. So let's first talk about the label propagation model. Given a graph that is partially labeled, we want to infer labels for the other nodes in the graph based on the connectivity pattern. And, essentially, if two nodes are connected, then they are likely to share the same label. The labeled propagation algorithm uses the graph Laplacian to propagate labels in the graph. And to provide more intuition, the graph Laplacian can be thought of as a discrete analog of the Laplace operator, which happens to model the propagation of the heat equation in the Euclidean space. In some sense, labels can be thought of as heat sources on the graph, and applying different iterations of label propagation diffuses the heat or labels to unlabeled nodes. While label prop is an iterative algorithm, it can also be equivalently written as a an optimization based method, which allows us to describe labelprop in graph EDM terms. Here the encoder is shallow and embeddings are directly learned in the label space, so the label decoder is simply the identity function. The supervised loss term is simply the l two distance to the labels of labeled nodes, and the unsupervised decoder uses l two distances between labeled vectors to compare pairs of nodes. And the unsupervised regularization loss compares these distances to the true graph aj s and c matrix. Minimizing this graph organization term essentially will encourage adjacent nodes to have their labels closed in the embedding space. And we can show that minimizing the sum of the supervised and unsupervised losses recovers the iterative solution from the previous slide. Finally, let's look at a recent and very popular approach for graph embedding. All the methods we've covered so far used shallow encoders and were therefore inherently transductive, in the sense that they could only get representations for nodes that were present at training time. The graph convolution model generalizes convolutions to graph by learning parametric encoders using node features. The total number of parameters in this model is independent of the number of nodes in the graph, and the learned encoder networks can be used to get representations for many input graphs that have the same input feature domain. And this is basically the inductive setting where the models can generalize across different graphs. One challenge in designing convolutional neural networks for graphs is that graphs have arbitrary structures, while usual neural network architectures operate on regular data such as sequences or images. So for instance, in computer vision, CNNs are used for automatic feature extraction by applying small localized features which activate in the presence of relevant panels. And due to the regular structure of images, these filters can be shared across many different locations in the image, making CNNs computationally very efficient. With graphs, this is much more challenging because graphs don't have this regular grid structure. So for instance, the number of neighbors varies from one vertex to another. And so to solve this challenge, GCNs make many simplifying assumptions by adding some weight sharing constraints in the model and also letting nodes interact with their direct neighbors in these small filters. So this is basically the message passing framework where at each step, nodes aggregate information from the neighbors. And as these iterations progress, each node embedding contains more and more information from further nodes in the graph. This message passing allows to preserve both structural properties of the graph, since nodes communicate only with their neighbors, but also feature information since these messages are computed from input features. The GCN model can also be described using GraphiDM. In this scenario, the encoder uses both the adjacency matrix and node features, and no unsupervised decoding is needed since the structural information is already leveraged in the encoding step. The label decoder is simply the identity mapping as GCNs map input features directly to the label space. And the encoder functions can be written in matrix form using the adjacency matrix and the degree matrix. It first transforms input features to compute messages and then aggregates messages via the graph ID as in its structure. Finally, GCN stack many of these layers using nonlinearities and the model parameters are learned by minimizing the cross entropy loss over the labeled data. This is the end of this talk. If you'd like to learn more about GRL, feel free to check out our silver paper, which has many more detailed descriptions of the different methods. In particular, I mostly focused on methods that learn presentations in occlusion spaces, but note that it is also possible to embed graphs into non occlusion spaces to better match the graph geometry. And so if you're interested about this topic topic is perfect to reach out to me or check out our first. Thank you for listening. K. Great. Thanks thanks for the presentation, Ines. And, good to have you, here with us in the flesh as it were, even though, you know, it's it's the closest, it's the next next best thing, actually. So Thank you. Our pleasure. I see that people are just now starting to to type in some some questions. So, I'll give them some time. And, I have actually written down a few of my own until we have, some from the people. So one question for me. You mentioned at some point towards the end of the presentation that one issue that, you have when using graph neural networks as opposed to, the the classic ones that says, precisely the lack of structure, and that forces you to you you some some some tricks, basically. Could you expand a little bit on on that? What does the what precisely is the issue that the lack of structure causes? Because, you know, this is not in any way my my expert graph machine learning, but based on on the cube, my the little knowledge I had I had the impression one of the benefits of using that is precisely the ability, to to add more information and, represents all kinds of data structures, basically. So can you expand can you explain a little bit what we what that issue causes, basically? So just to make sure I understand correctly, because the network is is breaking a little bit, are you asking about the lack of, regular structure in graphs and, how it makes this issue? So if you think about an image, it can be part of it as a graph in some sense, which is a grid graph. So just, two directions where every node is connected to its top left and bottom right, neighbors. So this structure is very irregular in the sense that if I move and I translate, do some translation along the image, then the neighborhood structure is gonna be exactly the same wherever I go. So each node is gonna have, again, a top left, bottom right neighbor. And so because of this regular structure, convolutional neural networks on images, can do many simplifying assumptions like, sharing filter weights across different locations in the image, which makes them very computationally efficient, but also allows them to extract, local features at different locations. With graph, if we wanted to do with a general graph, not necessarily a grid, if we wanted to do a similar method, we would have to design a way to translate those little features across different graph locations. And because these graphs have an arbitrary structure, so one node may be connected to three other nodes and another node may be connected to only one other node, it's unclear how to move those filters across locations in the graph. So that's essentially the challenge in designing methods that operate on these graphs, for, feature extraction and pattern recognition. And so, essentially, the GCN makes, some simplifying assumptions. So for instance, the weights, that are gonna interact, from, different neighboring nodes are all gonna be cool, whereas in, standard CNNs, we have the flexibility to have different weights for different neighbor interactions. So we usually make those simplifying assumption to make these, CNNs work on arbitrary graph structures. So I'm not sure if that under that answers the question. Yeah. Yeah. Yeah. It does. It does. It also brings up a follow-up question, actually. So these precise, assumptions that you mentioned, how do you make them explicit in some way, or should you perhaps make make them explicit in some way, like adding some kind of metadata basically to, to to your network so that you know, like, okay. This is my assumption and this is the basis I'm operating on. The assumption about sharing the weight constraints, is that, what you're asking about? Yeah. Yeah. Yeah. Exactly. Yeah. So when we design the models, we, explicitly have those inductive biases. And so we're gonna design, convolutional layers that account for these assumptions. So, yes, when we design the models, all these assumptions are taken into account, in the implementation, so that they can work in practice on on real graphs. Okay. We have a question from the audience here from Roberta, who's asking if you can please contrast the impact between shallow and deep neural networks. In other words, what drives to increase the complexity and number of layers and convolutions of the network? The kinds of problems or the accuracy? So I think there are two threat of, like, research progress in this field. One is more driven about state of the art. So we have some datasets, and we wanna improve the, as as Roberto said, the accuracy on these datasets. And so in this dataset, it's really driven by, architecture choices, and we're gonna make small tweaks to see whether, architectural change can make improvement. And then I think there's the practice where, maybe those architectural changes are gonna give some small improvements. But then in complexity, they're adding a significant overhead. And so we're gonna go for simpler methods that, can work in practice and, work efficiently. So there are those two threads. And depending on whether we're trying to get a paper and, like, make some research progress or whether we're trying to use these GCNs in applications, to do some prediction about real data, then we're gonna be, choosing one method or the other. Okay. Okay. That's that's actually something I also wanted to to ask about. If there are if you you, specifically, personally are working on applications like, industrial or real world applications of, of this framework. So we've been working, with knowledge graphs, a lot. So knowledge graphs are, specific type of graphs where the relationships have, edge types. And so the application we're considering is link prediction in knowledge graphs. So how can we use the, how can we use the knowledge graph structure to predict missing links, about, entities? And so for instance, in Wikipedia, I guess people are familiar, in the context of this conference. But so, like, trying to predict missing facts in, in knowledge basis, to complete, the the knowledge graph structure, and we're also using them for natural language processing applications. So because these knowledge graphs have, a lot of knowledge about real world entities, we can get representations that capture this knowledge, and they can be used, as input to, neural networks that do any language task like question answering. So we're working in this. Like, for instance, name entity disambiguation. We have, we may have, like, many entities in the sentence that, conflict with each other, and we wanna know which entity sentence is referring to. So having these knowledge graph, embeddings that give us, some more additional context about entities can be, useful in in those natural language task. Okay. That's that's interesting. Actually, I just recently, so discussion, which seems very relevant to to what you're saying. And it's been actually an ongoing, a long standing, let's say, issue in the in the knowledge graph board, precisely what you described. So being able to infer additional, additional links, basically, between these these different entities. And so far, the the general assumption has been that doing that is is a good thing. But, recently, I saw a discussion with people some people, including actually, one of my mentors, Frank van Harveen, like, one of the leading scholars in his domain, was was kind of questioning this notion. Like, the the idea was and some people were agreeing. Actually, the idea was that, well, it it all depends on on, you know, how how accurate you can be in this, link, prediction. If if it's not accurate enough or even if it's not very, very accurate, you may be doing more harm than good with with this section. Yeah. Yeah. I guess this is the precision recall trade off. So the model is gonna return many answers, and we have to choose a cutoff, for when we consider the answers are, not confident enough. So, yeah, this is probably a challenge with these link prediction, models. Like, we don't know when, when to stop adding links in the in the network. Is there are you aware of any, any methods that you can you can set this this this boundary to to be just about right? I'm not sure because in my work, I haven't worked with, like, explicitly the completion. So we use completion to get representations, but then, like, during test time, we don't actually, complete the knowledge graph, but we just get the, representations that we've learned for other machine learning tasks that require the knowledge. So we don't have to make those explicit choices for when, to stop adding links in the knowledge graph. Okay. But that's the interesting point. Okay. Yeah. Fair enough. We have another question from Matt who's asking how are different meta features on Edge data managed in the adjacency matrix? For example, has a or belongs to or is a in a knowledge graph. Quite relevant question. In the talk that I presented here, I only considered graphs that don't have edge types. These were only whether two nodes are connected or not. In the general knowledge, graph application that I mentioned, we've worked with graphs that have edge types. So, usually, the edge types are just, categories. So we know that, has or belongs to our different categories. We haven't tried, adding edge features in the model, but I know this is something that, people have worked on. So, like, maybe some relationships are very similar to each other, and so having some representations for the relations rather than treating them as completely different categories, can improve the model's quality. But, I haven't worked with edge features, in in my work. Okay. Would that be perhaps one, future, future direction for this research? Yeah. Definitely. And even in the just in the graph, neural network area, I think there's been a lot of work in trying to incorporate those edge features. So, essentially, the GCN update, aggregates messages from the neighbors. And, in the work in the talk I presented, the aggregation is just an average, of messages from the neighbors, but we can also extend that to incorporate edge features. So when we aggregate information, we also have some transformation that accounts for the type of relation that the message is coming from. So these are definitely, ongoing research directions, especially for, knowledge graphs that have, different, edge relations. Okay. Other future directions for for your work? So my personal research is mostly focused on, nonaclegian representation. So I've, briefly mentioned that at the end of the, the presentation. Essentially, all the all the methods I've, talked about in the in the talk were, occlusion in the sense that the representation slide in occlusion vector spaces where we can take inner product l two distances. And my research is focused on nonacledian representation learning. So, essentially, here the representations lie in, curved spaces. And so these spaces don't have a vector space structure, so we can't just take two points and add them. So for instance, if you think about a sphere, it's also it's a space of positive curvature. And if you have two points on the sphere and you do the Euclidean addition, then the points are gonna lie within the sphere, which is not valid, does not respect the geometry of the space, which is this curved surface. And so the challenge is in, with this, with this, spaces is to design, operations that can, that are valid in these, in these curved spaces to, do machine learning in, so for instance, hyperbolic spherical spaces. So this is the focus on my research. And the high level motivation is because, tree like data and hierarchies in general can be represented much better in hyperbolic spaces than in occlusion spaces because the hyperbolic space is like some sort of continuous version of trees, and it has a polynomial volume growth. And so because it has this growing structure, it's more suitable to represent, your archical data. And so that's why, we've been working with those, alternate geometries to represent your archival data. Okay. Thank you very much. It seems, we don't really have many more questions. Thank you so much for having me. Thank you very much. Bye bye. Bye.",
    "transcript_length": 27803,
    "speaker": "Ines Chami",
    "tags": [
      "buffer presentation learning",
      "deepwalk",
      "graph class classification",
      "graph clustering",
      "graph convolution model",
      "graph edm",
      "graph factorization",
      "graph machine learning",
      "graph neural networks",
      "graph visualization"
    ]
  },
  {
    "title": "How a Knowledge Graph Can Support Advanced Price Analytics in Supply Chain Management",
    "description": "In a company with high purchasing volume, it is crucial to design cost-efficient parts and procure these parts at competitive pricing level.\n\nData analytics can support on both aspects. Unfortunately, supply chain transactional data but also product life cycle data can be messy and distributed among many applications and data silos.\n\nFor the task to determine if a particular offer or paid price for a part is reasonable, the following two problems need to be solved:\n\nDefinition of scope: What do I get for the price? Are there any options included? Is packaging and transportation part of the offer?\n\nDefinition of requirements/features: What is the underlying product specification of this offer? What is the material? What special requirements does it fulfill?\n\nTherefore, pricing information needs to be connected to design information and a clear designation system for scope and requirements must be developed in a way that it can be formally represented by a machine.\n\nThis presentation will provide insight on how a Knowledge Graph is used to deal with heterogenous data input. Additionally, an example will show how automated pricing analytics is possible using semantics and logic available in the Knowledge Graph.\n\nA Knowledge Graph can serve here as a data integration layer based on a shared vocabulary, defined relations and properties for parts and components of which the company\u2019s products are made of.\n\nA crucial role plays the use of core ontologies applicable for industrial and financial domain such as the ISO 15926-14 or the FIBO ontologies. QUDT is helpful in providing a solid basis for units and measurements.\n\nAdditional domain ontologies e.g. for purchasing or engineering help to include semantics and relations among the specific domain data elements.\n\nThe transactional purchasing data as well as product design data will be mapped to the concepts and relations defined in the core and domain ontologies.\n\nWith the help of a reasoning engine automatic identification, classification and relation mapping is done. This results in automated pricing evaluations possible for most parts of the purchasing volume.\n\nFor instance, the inlet system for a gas turbine is modeled with properties such as material or inlet mass flow. Past and current pricing of the inlet system is available from the global purchasing organization for the entire gas turbine portfolio ranging from small engines to very large engines.\n\nUsing the above inlet system data model with the defined properties a regression chart can be established showing inlet system pricing over mass flow and material categories. Both properties are known to be the major cost drivers for such systems.\n\nThe regression chart can be used to determine an average price level or for detecting price anomalies. E.g. an specific offer can be evaluated against the average or even lowest price line.\n\nThis procedure is also called (multi) linear performance pricing (see https://commons.wikimedia.org/wiki/File:Linear_Performance_Pricing.png) where sales prices are shown related to performance. Performance is an aggregated value considering the important part properties.\n\nUsing a Knowledge Graph with reasoning capabilities it is possible to apply linear performance pricing at scale for all parts which need to be purchased within a company. The relevant data can be automatically mapped from legacy data bases into the Knowledge Graph. No manual data collection, curation and update processes are necessary.",
    "category": "Knowledge Graphs",
    "transcript": "",
    "transcript_length": 0,
    "speaker": "Marcus N\u00f6lke",
    "tags": [
      "Denodo",
      "Excel",
      "Fibo ontology",
      "ISO",
      "Metafactory",
      "ODBC",
      "Oracle",
      "RDFox",
      "Redshift",
      "SAP"
    ]
  },
  {
    "title": "Humans and the Graph - Inspiring and empowering GraphQL adoption",
    "description": "Realizing the transformational benefits of GraphQL involves much more than learning and adopting a new API technology; it requires new mindsets, new collaboration patterns and new models of governance to maximize the value of this new enterprise-wide asset.\n\nIn this talk, Dan Boerner, Apollo\u2019s Graph Champion, will explore the human factors side of GraphQL and share how leading graph adopting companies are focusing on empathy, inspiration, change management and communications to deliver the change and the benefits they seek.",
    "category": "Knowledge Graphs",
    "transcript": "Hi, everyone. My name is Dan Boerner, and I champion the graph at Apollo. And in this talk, I wanted to share my thoughts about the human side of GraphQL by focusing on the people involved with adopting the technology, their motivations, objectives, and how they're managing the change from a traditional point to point API to a query based unified graph. Now everything I share today comes from what we've learned from others that are evangelizing and championing the use of GraphQL in their organizations, and you can see their logos on the screen here. And I'd like to give special thanks to the panelists shown here who joined from Netflix, Zillow, and Dava and Expedia recently to discuss this very topic at a recent GraphQL summit roundtable. And if you're interested in hearing a great interactive discussion on this topic, you can find a recording at the link below or by visiting summit dot graph q l dot com. Now anytime I see an exponential adoption curve, I think about the underlying motivations that are driving the change. In this case, the curve shows the number of downloads of Apollo's open source GraphQL client over time. And this is a helpful proxy for overall GraphQL adoption. I think the curve makes a strong argument that GraphQL is one of the biggest innovations in API technology in our generation. So it's worth first to understand what's going on. What's driving this adoption? And while I think like other innovations, cloud native computing, containerization, reactive programming, it's natural to try to explain the rapid adoption from a technical point of view by focusing on how this new innovation is better than the old in various technical aspects. Now we'll get into a few of the technical benefits of GraphQL in a moment, but I think that focusing on the technical benefits alone leaves out a big part of why any of these innovations take root and spread quickly. And I I think that's because successful innovations provide better solutions to problems that people face, to teams face, to human problems. And those solutions help them build their products and bring them to market, and that's what really drives the change. Now in my career, I've been involved with a number of large scale tech adoption efforts and some of which have been more successful than others. And I have two observations. One, widespread adoption of any new technology is very difficult and time consuming. And most adoption efforts, frankly, don't succeed. They don't reach their goals. And my hypothesis is that it's only innovation that solve both technical and human problems that justify the large amount of time and effort required to successfully see them spread across the enterprise. Without that return on effort, the initiatives fade, and they fall short of their original goals. And let's see if we can tease out why that might be so by answering a few questions. What is motivating, in this case, a fundamental change to our API stack? What resistance do we face and do we encounter when we try to make this change and why? And in face of that resistance, how can we create space and safety so that that change can happen and can be absorbed? Let's begin first with motivations and and the problems that GraphQL solves. Now from a simplistic tactical point of view, GraphQL is essentially defined by three things. It's an API query language. Much like SQL queries databases, you can use GraphQL to query across APIs. Now, secondly, it's a stable, strongly typed, declarative API contract, which is captured in a schema definition language or SDL file. And third, it's a runtime system. Right? There's a client library, gateway, and server to make and process requests, to route them to their underlying services or data layers, and then to return composed responses back to the client. And GraphQL solves a large number of technical problems, and they're well understood, and they're well explained in many talks and and resources online. So I won't dwell on them today other than to say that I think they're very compelling, and a lot of companies find them compelling. But if we want to have a widespread and successful adoption of GraphQL across the whole enterprise, let's check-in with our hypothesis. And it tells us that we need to solve both technical and human problems. So let's look at what kind of human problems exist in our typical service oriented architecture. Now I'll start with the viewpoint of an application or a front end team. And this wealth of service APIs and data reminds me of the phrase, water, water everywhere, but not a drop to drink. Because in order to build products from all those experiences, to build experiences from services, I should say, those application teams have to integrate with many different APIs. They have to deal with many API versions. They have to configure API endpoints, learn about different API idioms and syntaxes, and orchestrate multiple calls together. And as a result, a lot of logic gets built into the clients, into the front ends. And if that was done once, that might be okay, but it's not. That logic is duplicated into each application or each front end, and that duplication causes two serious problems for people. One, for internal product teams, all that duplicated logic means each new experience or product feature has to be built multiple times, slowing innovation. For customers, it leads to a fragmented customer experience when they use different clients or devices. And this fragmentation is leading to frustration and a dizzying amount of difference for differences' sake. We see this across experiences all over across the web and applications across all industries. Expedia, for example, had this problem, and they turned to the graph to unify their customer experience. And they coupled a unified graph, a design system, and a component architecture to create a unified customer experience across all their touch points. And you can see the benefits that their team called out with this approach. But it's not just travel. In retail, Walmart turned to the graph to create a seamless shopping experience across their online and in store applications, which had previously been completely separate and accessed through separate tabs in the user interface. With a unified graph, they were able to unify and create a seamless customer experience across web, iOS, and Android in record time, just in time for Black Friday. So you can enjoy this experience by visiting Walmart today. And those are just two examples of problems faced by customers and application teams. But what about service teams? Well, this might be a view from the services team perspective Managing all of those connections, all of those client dependencies is time consuming. Managing the different versions, convincing clients to upgrade to new ones, or hopefully to deprecate and move off very old ones, it's all overhead. Not to mention requests from enterprise architecture teams or IT to unify on, say, an API gateway. It can feel very overwhelming, and it leaves very little time to focus on improving their services' underlying functionality or performance, let alone the cost of the occasional need to architect completely their service. But what's worse, even if they do these things, it still pushes all that complexity down to each client where it gets duplicated. So many teams have built front end APIs in the back end for front end pattern to simplify the complexity Steam quickly Steam quickly. So more human problems. We've got version and API endpoint management, direct coordination between apps and service teams, duplication of logic, slowing experimentation, and new feature rollout. So let's briefly turn to how GraphQL addresses these very real real and very human problems. First on the left, it's an API built for building products. It's freed from managing service endpoints and orchestration. App development teams can focus on experiences, not integration. Second, it's a query language that's tailored for use. Apps pick what they need from a shared common contract, optimizing performance and removing complexity from each application. Third, in the middle, the unified representation of your services, data, and digital capabilities is created, each capability is expressed as declarative abstract contract via schema and unified together into a composed super graph schema. Now an insulating layer is the fourth step. On the service side, on the right side, decoupled from those direct app client dependencies, service teams can focus on optimizing their capabilities and architecture without fear of breaking changes, and they can even reimplement their architecture behind a safe typed contract without even letting their clients know. And that technical architecture empowers new collaboration models as well. On the left, app teams bring their understanding of the customer problem, the customer experience, and what's needed to build it. On the right, service teams bring their domain declarative, strongly strongly typed, abstract schema contract that once built allows each team to focus on what they do best. And then revising the schema contract as needed in a lightweight, backwards compatible manner, thanks to automatic schema and composition checks offered by Apollo's graph platform. Now this sounds fantastic. No? Everyone's working together to leverage a new API paradigm and technology to get more done with less hassle. But it's a big change, isn't it? Service teams and front end teams coming together design an API doesn't happen overnight. So let's let's see what changes that that provokes. And As much as we'd like to think that we're hyperrational beings who evaluate the merits of each new technology, logically, it turns out we're people. We're human. And humans have an innate tendency to minimize threats and seek safety. And, commonly, we see major tech changes like GraphQL provoke some of these triggers. Fairness. My hard fought skills, are they gonna be made obsolete? Is everything changing? I have no certainty. Am I losing control? Are others now telling me what my service what my team should be doing, what good is? That affects my sense of autonomy. Now in the recent roundtable I hosted, Steven Spalding of Netflix noted that these triggers are actually biological in nature. Nature. They're identified by our amygdala as a survival mechanism. And our lizard bean brain actually tells us to run away from some of these proposed tactical changes before a rational engineering mind has even gotten a chance to consider them, or worse, to fight back with every argument we can find. Now it's perfectly natural to think that we can just dispassionately compare the benefits of a query language or a new server architecture. But it turns out we need to identify and address the human triggers that are happening along the way. And that's not intuitive, is it? Because as an evangelist of an innovation , when I trigger a fight or flight response in someone I'm trying to convince, what what's my response? Well, it's only to push harder, to fight back with fresh arguments or to give chase, essentially attacking them with believe me, believe me. And that just makes it worse. That just creates more triggers. So if change necessarily triggers base emotions in those we seek to convince, what can we do? Is there a way to avoid this conflict and avoid these triggers? Well, I wish I could say I believe there was, but we discussed this topic in the roundtable, and we all came to the same conclusion. Friction or conflict is just another word for interaction. All big leaps come from a controlled amount of struggle and friction. And second, diversity of thought is the source of insight. It's not to be avoided. Knowing where your blind spots are, knowing what you don't even know you don't know is crucial to making discoveries. And third, avoiding conflict via path of least resistance isn't healthy in relationships, and it's not healthy in system design. It leads to extra layers and complexity being built to avoid the need to come together and reconcile those differences. So, again, great. What do we do then? If friction and conflict are inevitable and they're not to be avoided, what can or what should we do to manage that change and the reactions that come about? Well, let's talk about a few tools that you can use to promote that positive friction that we talk about. Broadly speaking, there's four that I'll share with you today. The first is empathy. Rather than forcing a solution on others because it's technically better, try adopting a product mindset and work to figure out your customer's problems, the problem that the adopting teams have. By thinking and talking in terms of other people's interests, what they care about, we gain empathy, and that leads to trust. And trust is what we need to encourage change and work through fear and doubt. Second, shift attention to common outcomes. The best way to find common ground is not to technically crush any doubt or push back with the awesome force of your will and persuasion. That energy only increases resistance to new ideas. But rather, redirect to discussion of common outcomes that you all seek. It's very difficult to align on approach if you don't have alignment on your goals, and the more agreement on outcomes, the easier it is to explore solutions that meet those needs. Third, demonstrate versus scold. When we want new behavior, we can demand it, we can criticize noncompliance or resistance, or we can demonstrate and show its value. And often, this means actually rolling up your sleeves and working with those adopting teams. Putting skin in the game not only shows your commitment to outcomes, but helps you understand where changes are needed to your current approach. So adopting a show and tell approach with the emphasis on show is a practical tool you can use. And finally, and perhaps most importantly, embrace new mindsets. At Apollo, we've seen many companies adopt GraphQL at scale across our organization, and we've observed a very natural instinct to see the new in terms of the familiar. I mean, we've been doing APIs as a community, right, for a long time. So adding a query language across multiple APIs, that's great, but the shape and the purpose of every API, that doesn't need to change, does it? Well, our experience has been that when you take that approach, when you try to fit the new into the old, you often miss out on much of the value of the change. And in the case of APIs, there's a strong consensus now about the power and importance of designing with an API first mindset. Right? But if we look a little deeper, if those APIs are designed for each service by each service team independently and they reflect only the view point of view of the service owner, then they're in a sense thrown over the wall to the front end. And while consuming multiple of them in a single query is mechanically better, it doesn't really address the complexity problem. Whereas, if we adopt a true customer mindset for APIs, we see we need a connected set of APIs, a graph shaped for use by its clients, not its underlying services. And designing such an interconnected graph requires a much deeper level of collaboration, doesn't it? And that pulls our teams together to build something that truly captures, something long lasting, something durable. And that's an evolving API contract that offers a unified and interconnected set of data and capabilities built upon but abstracted away from the underlying service and data laters. And that's just more valuable than enabling queries. So let's talk about a few takeaways to summarize the topics that we've covered today. First, tech adoption is a human endeavor. If your organization is adopting GraphQL, your people are dealing with change. And along with the benefits, the value of new technologies such as GraphQL, change often triggers resistance. Constructive friction is needed to advance, to make the leaps. To avoid change is a siren's call to the rocks. It sounds attractive to avoid it, but, ultimately, it's destructive. Rather, by practicing empathy, shifting focus, demonstrating success, and adopting new mindsets, you can create a healthy space for change and healthy conflict. And, ultimately, and this is borne out over across a hundred or so companies that we've talked to about GraphQL adoption. Human factors affect the success of tech adoption more than tactical factors do. And that's the end of our talk today. I wanna thank Connected Data World for this opportunity to reach you today and to you for watching the presentation. For more content on GraphQL, the unified graph, Apollo's graph platform, and more about how companies can get the most value out of adopting GraphQL, please visit our site, our YouTube channel. And if you wanna continue the conversation directly with me, please reach out via Twitter or the email addresses here. And thank you.",
    "transcript_length": 16977,
    "speaker": "Dan Boerner",
    "tags": [
      "SDL",
      "api first mindset",
      "api gateway",
      "apollo",
      "back end for front end",
      "component architecture",
      "containerization",
      "design system",
      "graphql",
      "reactive programming"
    ]
  },
  {
    "title": "Hybridization of Machine Learning and Operational Research is the future of AI",
    "description": "Recently, Graph Neural Network (GNN) gained a lot of attention, but did you know that Graph theory is actually considered by some as a subfield of Operations Research?\n\nEven better, did you know that Machine Learning (ML) was considered as a subfield of Operations Research (OR) a few years ago without (much) controversy?\n\nWhat is OR you might ask? Not an easy question but think about optimization. Every time you try to optimize something, you probably are in the realm of OR somehow.\n\nML as a subfield of OR? Basically, in ML you try to ... optimize your predictions!  Beyond naming disputes, there are two fields, ML and OR, that are remarkably complementary and when you combine them together you can go (much) further than just using ML or just using OR.\n\nThere are at least 4 ways to combine ML and OR. You can use ML and OR as two black boxes without any interaction between the two.\n\nMost of the time, you use ML to get accurate predictions in a first step and then in a second step, you optimize your problem with OR using those predictions as an input.\n\nThen there are two technical ways to use them: use one to improve the other. It works in both directions and it really improves algorithms in both fields. Finally, there is a fourth way to combine both where new algorithms are created.\n\nThe first combination is becoming more and more popular and used since about 2019 to solve industrial problems.\n\nThe second and third combinations are essentially the subject of academic research by a few teams around the world.\n\nThe fourth combination is quite unknown but is probably the most interesting one as - in my view - it could lead to real intelligence.\n\nIn this talk, I'll briefly present what OR is and how it is related to ML. Then I'll present real and successful industrial cases for the first three combinations. I will conclude by briefly presenting what I believe can be done with the fourth combination.",
    "category": "Graph AI",
    "transcript": "",
    "transcript_length": 0,
    "speaker": "Nikolaj van Omme",
    "tags": [
      "big data",
      "data science",
      "descriptive analysis",
      "diagnostic analysis",
      "graph neural networks",
      "graph theory",
      "machine learning",
      "ml",
      "operations research",
      "or"
    ]
  },
  {
    "title": "Implementing Informed Consent with Knowledge Graphs",
    "description": "The GDPR legislation has brought to light one\u2019s rights and has highlighted the importance of consent, which has caused a major shift in how data processing and sharing are handled.\n\nData sharing has been a popular research topic for many years, however, a unified solution for the transparent implementation of consent, in compliance with GDPR that could be used as a standard, has not been presented yet.\n\nIn this talk I will present my research, which proposes a solution for implementing informed consent for sensor data sharing in compliance with GDPR with semantic technology, namely knowledge graphs.\n\nThe main objectives are to model the life cycle of informed consent (i.e. the request, comprehension, decision and use of consent) with knowledge graphs so that it is easily interpretable by machines, and to graphically visualise it to individuals in order to raise legal awareness of what it means to consent and the implications that follow.\n\nThe talk is suitable for both academia researchers and industry professionals.",
    "category": "Knowledge Graphs",
    "transcript": "Hello, everyone. Welcome to this talk on implementing informed consent with knowledge graphs. My name is Anelioa Kurteva, and I'm a PhD researcher at the Semantic Technology Institute at the University of Innsbruck. Currently, I am interested in my PhD in knowledge graphs, the semantic web privacy, GDPR, and human computer interaction, and how they can work together to solve issues such as, consent in, different domains. So let's begin. A bit of background information on GDPR first. So GDPR became applicable in May of twenty eighteen in all member states to harmonize data privacy laws across, Europe. GDPR's main aim is to give control to individuals over their personal data, and it introduced the notion of informed consent. And, also, GDPR views consent as one of its legal basis for data processing. But, of course, there are hefty fines for not being compliant with GDPR. Up to ten million or two percent, of the firm's worldwide annual revenue. So what is consent according to GDPR? So consent according to article four of GDPR is any freely given, specific, informed, and unambiguous indication, of the data, subject agreement to the processing of personal data related to him or her. And to be more specific, what is exactly informed consent? So before the data subjects grant consent, they should be married, aware of information such as who who is the data controller, what kind of data will be used, for what purposes is the data required, how the data will be used, how the data will be processed, where the data will be stored, with whom will be shared. And, of course, much more information that can be provided here to individuals so they're more informed. So here come two projects that I'm currently working on and that also consider, GDPR and consent. We have the company, project, whose goal, is to, develop an open platform on which private and public institutions can create different campaigns. And a campaigning company is a specific request for a data from individuals, and, that data will be analyzed. The main focus for now is vehicle sensor data, and, actually, company runs in Hanover, Germany. And the second project is the Smashfit project whose goal is to establish a framework, for secure and trusted sharing of, personal data and industrial data streams across different silos and, smart cities, for example, which I'll talk a bit a bit, further in the presentation. And, yes, consent in smart cities. So in a smart city, consent is, consent and also the data of the individuals can be spread across multiple different zeroes, which are databases, locations, people, and can be used for different purposes by different entities simultaneously. Here on the side, you can see, smart city domains. We have smart retail, smart mobility, smart health, smart government, education, smart homes. And, the other goal is to implement the consent, in a way that does not distribute the data flow between those silos while still being compliant with GDPR. And consent in smart cities is actually one of, the use cases that inspired the smart chip project, which you can also have a look at. So the main challenges that we've identified are, responsibilities. Who is responsible for what? Who records the consent? Storage. Where is the consent stored? What privacy and security mechanisms are in place? Who has access to the consent and also to the data? And can the consent decision of the individual be changed without the, his knowledge? Also, what are the implications of revoking consent? How can a user revoke their consent? And what happens to the data and the processes that, use the consent after the consent is revoked? And finally, awareness, awareness of what it means to give consent and the implications that follow. Some knowledge graphs, which I'm sure most of you are aware of, they can be used as a consent solution, and they have many benefits that can be related to consent, but, of course, are not limited. For example, knowledge graph provide transparency because data is bought in human readable and machine readable formats. They provide traceability, which when it comes to consent, is really, crucial because, it provides provenance, information. We can use different, ontologies for provenance events and time stamp, data. So we preserve the records of consent, when the consent decision was made, when it was changed, and more. We have also knowledge discovery. We can discover different patterns and consistencies, security and privacy issues. We can perform reasoning on knowledge graphs, which is especially useful for GDPR compliance checking. We can understand connections between data. For example, how a third party gain access to specific user data. And, of course, knowledge graphs provide a common understanding across all entities in the smart cities of the notion of consent and many more concepts that exist in the domain. So we performed a survey, which you can check to have, input to the reference on semantic models for consent. And, of course, work has been done through the years, and several ontologies exist as you can see. However, we've identified common limitations. There these ontologies are either too general because, they're part of project, and these projects have their own use cases. And this the ontologies also focus only on these, use cases. The ontologies, yes, they represent only consent sometimes, which, means that no other information about the data itself, or the processing that's applied to the data is modeled. And, some of the ontologies, do not model consent state state changes such as revoked, withdrawn, etcetera. And, most of the ontologies are not open access, which can complicate their reuse. Current goals of, the projects that I presented, Kompano and, SmashIt, is to implement the life cycle of informed consent with knowledge graphs. And when we say life cycle, we view, here on the side the different steps that can be undertaken when dealing with consent. We have the request of consent, the comprehension of consent, the decision about consent, and the use of consent. And all of these, stages also consists, of, different sub steps that can be taken, different, tools and approaches that can be adopted when it comes to the management of consent at each stage. And the next goal is to raise the awareness about what it means to give consent and the implications that follow. So the first step that we took is to model a sample, a campaign in the company or project. The campaign is called, and it's actually a real campaign that was run by the Tyrolian government here in Austria, a couple of years ago. The main date of interest in that campaign was to, GPS data from car drivers in the area of Innsbruck up to fifteen kilometers away, and the data was stored anonymously in a central database in Kuchstein or by owned by the government. Upon the user, consent, the data was, or will be also, up to today, shared with, STI Innsbruck, which is my research group, and we run different processing connect. So based on that simple campaign, we created our, main ontology and then, we added data and created the the knowledge graph, which you can see here. It's quite simple and represents currently only one campaign. Rosa Finn's book has presented earlier, and we have the different data associated with it when the consent was asked. So we have the status of consent, which is, given. We have the duration of campaign, twelve days. We have the main purpose, which is for fraud detection. We have also who requested the campaign, which is the Innsbruck government. We have the type of data, GPS data, and, of course, the person who gave their consent. And while this data is really, easy to understand by machines and experts both in human readable, machine readable format. How can individuals with no knowledge of linked data understand this, model? So we decided to visualize the consent first, and, we began by analyzing existing consent request user interfaces. Some of the issues that, we came across are information overload, privacy and policy privacy policies written in technical and legal jargon. As you can see here on the side, we have long documents and legalese that people rarely understand and read. We have consent requests which are which are not really compliant with GDPR because no information about, the consent is presented. For example, why is the data required? Why is the data, what is the purpose of the data, how data will be processed, where where the data will be stored, and more. And, also, these user interfaces, do not clearly indicate that the individuals have the right to, revoke their consent, which is also known as the right to be forgotten. So we, created, a simple user interface for the company or project, which displays the different campaigns, which you can see on the side. Or it you can see campaigns from different companies. The UI was developed with, Google's new UI toolkit called Flutter, and it follows the, consent knowledge graph which we, shown before. And the UI is designed to be used in vehicles, for example, on tablet or a phone by the individual. Here, you can see more details about the actual campaign the details when the user selects the campaign by by, Volkswagen. You can see the, company name, the campaign, roads of Innsbruck, what data will be collected, when is the start date, the end date of the campaign. And further, when the user selects to agree to participate in that campaign, we present them with, preference, lists. So the user can select really if they want to share all of their data that's asked for or only GPS, tire pressure, speed, and then finalize their consent. But then, we were, thinking of how to even further, help raise the awareness of individuals. So we decided to focus on visualizing of, what happens after the consent is given. And for that, we use, again, the knowledge graph which we have with the different campaigns as the main source of information and the d three, visualization library. And, we actually published a paper in June this year. You can, have a look if you're more interested, which focuses exactly on this topic. So we presented individuals, with, this, user interface. In the middle, you can see this is the vehicle from which, the data is transmitted or the individual itself inside of the vehicle, and we have the different campaigns that these individuals participates in. For example, green future, drive smart, highway zero, and each campaign is associated with, different, types of data. And we use color blocking to identify that. For example, speed is in yellow, GPS is in purple, and we have also a team change from light to dark. You can actually test the user interface, here and play with it for yourself, see if you like it, and maybe you can even, provide some feedback about, future improvements that can be done to it. So when the individual selects a campaign, for example, green future, they're presented with a bit more detailed, information about this campaign. Specifically, what type of data was, retrieved at what time and by which company. We performed some user evaluation. We had twenty five participants, female and, male, age eighteen to, fifty five, with different university degrees, and eighty percent of them had a driving license. Evaluation details you can find in the paper. So, we asked the same questions about consent before presenting the individuals with the UI and after. So, before, we asked them if they're willing to share their GPS location and other types of data with companies, and most of them said that probably, not and never they would share such data. And after we presented them with a visualization of what happens to their, data after consent is given, most of them actually changed their opinion and said that possibly, they would if they had this, tool. Next, we asked them to, describe the interface with some keywords. They said it's organized, effective, innovative. We also evaluated the comprehension of individuals. We asked them if they can easily see what's happening to their data at specific times, and most of the individuals strongly agreed and agreed. And, when asked if the graph helped them understand what happens to their data, most individuals, again, agreed and strongly agreed. So the main takeaways from our evaluation are that visualizations help increase the comprehension of what it means to give consent and, trust of the users. And that, the application that we created creates more transparency, the data sharing process. Here are just some examples of, future things that, I think are also important to want to consider. For example, consent standard standardization. I think that the W3C consent community group is currently working towards different, and better solutions for consent so you can have a look there. Also, what happens, beyond GDPR because GDPR, yes, it's a a really huge law, but, in some places, it does not apply. In California, for example, we have the CCPA, and other countries have their own laws about consent. And when in some cases, consent might not be enough, and what happens then? Maybe contracts and licenses can come into play. So these are just some things that we can all think about. And if you have any suggestions or you want to discuss, feel free to reach out and send me an email or even here in the chat. So So thank you very much for your attention, and I'll take any questions now if you have.",
    "transcript_length": 13460,
    "speaker": "Anelia Kurteva",
    "tags": [
      "anelioa kurteva",
      "rosa finn"
    ]
  },
  {
    "title": "Introducing D3FEND: A Knowledge Graph of Cybersecurity Countermeasures",
    "description": "Cybersecurity architects are using D3FEND to describe specific technical functions within cyber technologies in a common language of countermeasure techniques. A research project funded by the National Security Agency, D3FEND provides a large collection of digital artifacts to model cyber systems and related countermeasures.\n\nThis creates a foundation for automated reasoning about the complex interplay between computer network architectures, threats, and cyber countermeasures. Our is goal to make it easier for architects to understand how countermeasures work, so that they can more effectively design, deploy, and ultimately better defend networked systems.\n\nD3FEND is a framework which provides a countermeasure knowledge base, but more specifically, a knowledge graph. The graph contains semantically rigorous types and relations that define both the key concepts in the cybersecurity countermeasure domain and the relations necessary to link those concepts to each other.\n\nWe ground each of the concepts and relations to particular references in the cybersecurity literature. Numerous sources of research and development literature were analyzed, including a targeted sample of over 500 countermeasure patents drawn from the U.S. Patent Office corpus. The graph supports queries that can inferentially map architectural elements to both cybersecurity countermeasures and offensive TTPs.",
    "category": "Knowledge Graphs",
    "transcript": "Hello. Connected Data World twenty twenty one. My name is Peter Kaloroumakis, and I'm happy to get to present the defend project to you today. A little background on me. I work for, MITRE Corporation. We're based in the United States, and we focus on, research and development problems for our sponsors, primarily, the government. Before I came to MITRE, I worked in commercial product development, and I, designed and built, malware detection products, for network security purposes. So today, I'm gonna give you, an overview of sort of the motivation behind this project, what problems we were trying to solve. I'll talk about, what we built, a little bit on how we build it, and then also talk about how some people are using it to help better organize their cybersecurity defenses. So let's get started. So first, I wanna get into the motivation for this project. And before I start talking about defend, one of the things I have to orient you to is another MITRE project. It's called MITRE ATT and CK. MITRE ATT and CK was a is is a very popular framework for characterizing, what specific things, cybersecurity adversaries are doing when they break into a network. This model was built by MITRE over a number of years and publicly released, and it's really taken the industry by storm. And the result effect is that when when cybersecurity products are able to, detect something bad, they characterize it using terminology from this MITRE ATT and CK project. So, once that was, deployed, sort of it got out there, it became very successful, a sort of natural question arose, which is sort of, could you describe what cybersecurity defenders do and as much fidelity as we, describe the cyber the attackers? So we basically proposed this project to help organize the defensive space the same way that, MITRE had organized, the offensive space. So this defensive space is huge. And if you think about it in two main buckets, first, sort of technical policies. There are thousands of these technical policies which describe how cybersecurity protections can be applied to IT infrastructures. And, you know, there's a lot of material there. The top image at the right is sort of documents that describe how to do this grouped by the general, sort of domain. And in each of those documents, there might be hundreds of recommendations or requirements that people have to implement. The other big component of this space, the other major bucket is, cybersecurity vendors. So the cybersecurity marketplace is pretty large. It's a pretty big market. There are thousands of these vendors. Some of these vendors have multiple products, and some of those products, performed dozens of discrete functions. So this really quickly gets into, a high dimensional space that's difficult to manage if you're, you know, organizing your your defenses and you have to keep track of all of this information. So in order to address all of this complexity, we wanted to build a conceptual model to help organize this space. There's a lot of challenges with trying to do this, particularly because it's such a complex and nuanced space. One of the challenges specifically with cybersecurity is the amount of complexity and nuance in all the terminology that's used. And one of the things we asked ourselves was, how do we know if we were successful in building a useful model? We felt like if we could use the same model for multiple target audiences, that would give us some indication that we hit that abstraction or semantic sweet spot, if you will. So imagine three sort of target audiences. First, cybersecurity architects who are managing these complex portfolios. They really have to understand what specific functions all of these, products and offerings they're purchasing, do in great detail. Another target audience is what we call adversary emulation, where they're doing a pen test on a system, and they have to really understand how these things work in order to have a successful , useful pen test. Imagine a third audience for this. Let's say a cybersecurity investor who's trying to decide whether or not they should invest in a product. They have to understand what problem is being solved, has that been solved before, and is this being done in a new and novel way. Right? So we felt like if we could address these multiple audiences, that would give us some indication we were on the right track. So we set about building this, and three years, in the in the making with a very small team, we put together what you see today. We call it the defend knowledge graph. You can go to defend dot mitre dot org to browse the graph, and it renders portions of the, graph in the underlying model for people to start to use it. It's very early stage, and there's, some rough edges, but we felt like it was at the point where it made sense to open it up for collaboration. We built the graph by, analyzing lots of open source materials that explain how cybersecurity technologies work in detail. This information is very difficult to get, but we found a dataset that was sort of hiding in plain sight. And it didn't look like anyone else had actually systematically analyzed this corpus before. And these were, patents that were filed by these cybersecurity vendors that explain how their product works or what specific problem it's going to solve. There's a lot of diversity in the patent corpus. And for us, the challenge was how do you take a twenty or thirty page document and turn it into two, three, or four words that really crisply explain what it's doing. So we developed a methodology which lets us systematically do that, and that's part of how we built the the overall model. The picture you see at the top left is the most prominent taxonomy inside of defend. Those are the defensive countermeasures, of course, arranged by, type hierarchy. And what we did was we render it as sort of a tabular view in order to get folks interested in the content that's embedded in the graph. We also want people to use this graph to do more sophisticated modeling, and we built it using standard ontology tools in order to support those use cases. One obvious first use case is, okay. Now that I've got these defensive countermeasures identified, what offensive techniques are are those going to address? So you see this picture of defend connected to attack, with these red lines. And then you see this call out in the middle here with this idea that we call digital artifacts. So instead of just directly relating the defensive techniques to the offensive techniques, we do it through inference and reasoning through this intermediate model. And I'm gonna talk about that in much more detail on the next slide here. So what you're gonna see is this sort of cartoon view blown up with specific examples of the types of information, we're tracking. So, the same view, you've got attack on the left, defend on the right. And in the middle, you see what we call the digital artifact ontology. So this is describing all the sort of basic computer science concepts, that someone taking computer science one zero one might be learning about. And, surprisingly, we didn't find a model that existed, that met our needs to for cybersecurity purposes. So we ended up having to take a lot of basic computer science terminology and make it a bit more specific, because at the end of the day, a lot of times, the attackers are abusing functionality in the computer, and we end up needing to be more precise about what specific functionality they're manipulating. So you see on the left, a few of these offensive techniques, exploitation techniques, and then another exploitation technique called, process injection. And and then on the right, you've got this defensive technique process code segment verification. And both of these are mapped to this intermediate concept called a process code segment. And the relationship on the offensive side is that these techniques are modifying this concept, and then on the defensive side, you're verifying this concept. You can see that a process code segment is a type of code segment, which is a digital artifact. And then you see some object property relationships. For example, a process running on your computer contains process code segments. The process originated from an executable binary file, that contains code segments. And when the process is running on your system, it loads those executable, parts of the file into memory and then executes those. And then sometimes those are the parts that attackers will overwrite with code that you don't want. So if you wanna check the integrity of your processes in their memory space, you might try to verify process code segments. So once we have those definitions defined, we can then infer that this one defensive technique might detect this particular offensive technique, without needing to directly declare that. And because of the amount of information we're dealing with, this ends up being a much more scalable, manageable approach that lets you reason about, the the the whole graph, rather than trying to manually go through and map everything, accordingly. So this is sort of, an advancement in the state of the art for us, which is, to to do these sorts of things by inference instead of manually mapping things one to another. So next, I'm gonna show you what this information looks like in the actual tool. So if you go to defend at mitre dot org, you can, load up, the home page, and you'll see this countermeasure technique taxonomy that that you see here. Across the top, we have what we call defensive tactics. Those are sort of the maneuvers that a defender can perform in response to adversary activity. And then beneath that are what we call defensive techniques or countermeasure techniques. These are, the the highest order ones at the at the second level and then, more specific types of of the technique as you go down the column. So user behavior analysis has more specific types of user behavior analysis, and process analysis has more specific types of process analysis. One of which is is the example I gave in the slide, process code segment verification. Right? And this number we're showing, we found six public references that indicate there are technologies that do this. So we believe it's worth including into, the knowledge base. So if I click this, you get, what we call the knowledge base article. This is all encoded, in the ontology. And there you have a crisp definition of of this concept. We wanna be able to ask a vendor, do you do this? Yes or no? You get an explanation of how this works that's developed by reading all of the references at the bottom of this page, and and then our subject matter experts basically summarize those. And they also add a a section on consideration. So, in this case, with this technique, they're talking about potential false positives and false negatives. Then, once we specify what this technique is doing to what specific digital artifact, we can then infer what are the related offensive techniques that we might care about. Now this, this table down here is sort of a a miniature, attack table. So if you go to attack dot mitre dot org, you'll see the full table, which has, around five hundred of these techniques. In this case, we're selecting only the relevant ones for the user to, review and determine, okay. Will this defensive technique, protect me against, these offensive techniques? And then the semantic relationships go both ways. So some of how we're doing the inference, takes into account, hierarchy and, the type hierarchy within the the artifact definition. So if a higher order concept is interacting with a digital artifact, then that flows down to the lower order concepts as well. So in this case, we're showing if if you hover here, you can see the relationships on the offensive side. Process hollowing is modifying a process code segment, but in this case, credential API hooking might modify or may modify a process code segment. So a lot of times in cybersecurity, you're dealing with maybes, and, we've at least come up with a systematic way to describe those sorts of scenarios. And then down here is the references we used to develop this article, and you can go click those and see the in this case, this is a particular patent that it links to, and you can see, what we read to basically develop this article. Right? There's a number of other things you can do with the user interface. You can look up topics by offensive technique or defensive technique with these two search boxes. So if I wanna look up a defensive technique like dynamic analysis, I can do that here, and that'll take me to that page. Or I can look up the offensive technique. Let's say I wanna look up rootkit. What do what do attackers do when they install a rootkit? Well, if you look at this graph, we sort of codify that, and the offensive technique is in the middle. Some inferred defensive techniques are on the left, and then the relevant digital artifacts are on the right here. And we're showing that this offensive technique might modify kernel modules, shared library files, the kernel itself, or firmware on the system. So, then we infer because these defensive artifacts are interacting with those that they might detect it or you could deceive this offensive technique or you might harden against this offensive technique. Right? And then you can click on any of these boxes and get definitions. So what is a what is firmware in defend? Well, if you click that, it takes you to this page, which has a definition and additional relationships. In this case, it's showing you firmware verification, might, verify firmware. And then there's, some specific offensive techniques that attackers do to modify firmware. And in this case, we're showing different types. So you see, there's firmware, but then there's specifically system firmware as well. So some offensive techniques modify your system firmware, and some of them might modify, peripheral firmware, for example. So that's a quick spin through the user interface. So I wanna talk a little bit about how we name these things. Basically, the methodology I talked about earlier was is pretty straightforward, but it, kind of embarrassingly took us a long time to figure this out. We simply describe it along two dimensions of specificity, the noun and the the verb. So we we say the the digital artifact, the noun, or defensive action, the verb. So, there's three examples here. The least specific example would be segment analysis. And then as you move up to the right, we get, the most specific process code segment verification, the example I gave earlier. Verification in this case here is we're saying is a more specific type of analysis. And the reason we took this approach was we wanted multiple teams within an organization to be able to use the same, framework. If you have someone working in acquisitions, they may not care too much about the specific the most detailed example of, let's say, file analysis. They just wanna track whether or not they have a file analysis capability or a process analysis capability. But if you have a practitioner, they might want to know about all the different ways you could do file analysis or process analysis. And by having that taxonomical approach, it lets us address both technical and nontechnical users and then capture metrics accordingly with the same language framework. Right? So I'll talk a little bit here about how people are using defend to answer questions about their cybersecurity posture. So in this example, we took a specific threat. It was, identified in this threat report. You can see the link there. And then they wanted to know, would we have had any protections against this threat given these three specific products that we purchased? So what we did was we took the threat report, and it was mapped to MITRE ATT and CK in terms of the offensive techniques that the adversary was using. So you can see in this attack matrix view on the left, for initial access, they executed a supply chain compromise. For persistence, they installed a scheduled task job and a boot auto start execution script. And then what we did was we took the defensive products and broke them down using defend and identified what they were doing. So of these three products, they were doing some hardening techniques, some detection techniques, and some isolation techniques. And then But they were not part of these three products' capabilities, so that's marked as red and purple. And then we step through the rest of the attack, and we had some capability in this area. So that was marked amber and then red in this other area. So the result was that you get these sort of higher level insights about, where you might wanna make your investments. You might wanna, in this case, make your investments more in application hardening, platform hardening, and operating system monitoring because you don't have anything there. Now they did have some capability here, but here's a case where, everybody's sort of different. If you're in a well resourced environment and you wanna add diversity and depth to your defenses, you might actually make additional investments in this area, to add diversity to your defenses, or double up on certain types of analytics, for example. But if you're in a resource constrained environment, you might, take some of the investment you put in this area and put it into the areas where you had no capability. So the idea is that depending on what question you're trying to answer, you can use the same framework to tell the story about, what direction you think your organization should go. So finally, I'll leave you with this. This is a project that we open source to collaborate with the industry on. So I'll close out with, how to get involved with the project if you're interested in it. You can ask vendors, what defend techniques they support when you're making an acquisition in this space. And by thinking in terms of digital artifacts and defensive verbs, you can start to answer this question. With this specific defend technique, we can detect these attack techniques. And then that tells the whole story about what specific function in your enterprise is is is dealing with a particular threat. Right? The knowledge base, again, is available at defend dot mitre dot org, and we've published the ontology on that page as well. And it's, it's an OWL two format, and people can download it and extend it as they see fit. And we're looking for contributions. So if folks wanna add to it, they can send us an email at defend at mitre dot org, and we'll be very happy to engage with you. So thank you for the opportunity to speak here to you all, and, I look forward to your questions.",
    "transcript_length": 18799,
    "speaker": "Peter Kaloroumakis",
    "tags": [
      "adaptive analytics",
      "cybersecurity",
      "defend",
      "defensive countermeasures",
      "digital artifact ontology",
      "knowledge graph",
      "mitre att and ck",
      "mitre corporation",
      "offensive techniques",
      "ontology"
    ]
  },
  {
    "title": "Knowledge Graphs as Hub for Data, Metadata and Content",
    "description": "There is no mature market for Knowledge Graphs! Not yet. But they disrupt data and content management by offering flexible integration, unified access and a platform for analytics.\n\nWe will mindmap the markets and the applications where knowledge graphs add value:\n\n- Decision making: derive profound insights from richer and better interconnected data.\n- Search and discovery: get more relevant results and connect the dots in less time.\n- Publishing: repackage and better monetize content and data.\n- Data governance: automation of updates and data quality management.\n- Operations: infrastructure monitoring and configuration management.\nWe will present how Ontotext puts together the technology stack and the partner ecosystem needed to deliver all these applications.\n\nFinally, we will show how the JDBC driver and the data virtualization, recently added to GraphDB, make it easier to transform your data into a graph, mapp them to a big ontology (e.g. FIBO) and access them from a BI tool.",
    "category": "Knowledge Graphs",
    "transcript": "It's my great pleasure to introduce, Naso Kiriakov and Vasil Momchev from, our good friends, OntoText. OntoText have been in this space a long time, and really looking forward to finding out the latest kind of direction of the, the the product that, they've they've developed. And I'll be moderating this session. Please, say hello, introduce yourselves, tell us where you're from, and also don't hesitate to, answer any questions, and we'll, we'll cover those at the end. Great. Welcome, Naso and, Vasil, and and over to you. Thank you. Thank you very much, James. It's indeed a great pleasure to, yeah, to be to be again part of this conference. We have fresh memories from one year ago in London. So, it will be myself presenting and and OntoTech's, CTO, Vasil Mamcheff. So I I I will I will entertain you with a bit of introduction of, who we are, and then I'll share our vision on on the market for knowledge gap technology and different applications and how, semantic metadata are the key to put, to for quite a number of different applications and how how they, can improve both data management and content management, and and actually put data and content together. Then, I'll hand over to Vasil to share with you, yeah, few updates about, Onstext platform and and and our database engine, Graph DB, and to to to basically walk you through, the different aspects of what it takes to to get data in a graph database, to to transform them, to make a proper graph out of it, and then to consume them either from whatever tool all the way from BI tool, that access data to a JDBC driver to another application that use use it through, say, GraphQL or other APIs. And then we'll finish off with an overview of our partner ecosystem because, with the those wide range of applications of these technologies, there is no single vendor, at least, no no no none of these, specialized technology providers, who who can deliver all the applications to you. So we built a partner ecosystem, to to to to be able to, put the right technology stack in place and to be able to to to also deliver it at at scale. So let's get started with the introduction . So on to take this, yeah, we started with, what was back then semantic weapon, semantic technology in year two thousand. And we had the chance to to to be to be part of the avant garde in this technology and went to the all the evolution to link to open data, nowadays, knowledge gaps. And, well, nowadays, we are, like, most of our clients, are are the total leaders in different industries and markets, like multinational companies. Still, if we have to to to come to where where their headquarters are, it's quite an even split between UK where where we got our first clients, big clients in stories ten years ago, and, then US has a growing area, Asia also, and main, mainland Europe. Up until a year ago, it was easy to say Europe. Now you gotta say Europe and UK. But, anyway so, you see some selection of our clients. I'll talk a bit more about the verticals , But, yeah, you see that most of them are precisely this type of really big enterprises. We we are not allowed to mention some of them, but that's that's the way it is. We being being part of this industry and part of the this community that is, developing the trends and setting the standards, we we do this through through a number of, yeah, organizations, worldwide web that we would see, being the main one, which not only standardizes World Wide Web, HTML, HTP, and all these standards on which the World Wide Web is built, but they are also the ones that standardized RDS, Fargo, OWL, and the stack of, the stack of standards that we we build our technology on. And we have we had the chance, and we we are still very happy to, to keep pushing our research to a collaborative project. So we are we are very, very well interconnected with those sorts of academic groups in this field in Europe. Our best, our most popular product is, Onstead's Graph DB. We're getting, I don't know, probably, thousand or a bit more downloads, of, Graph DB per month. And, yeah, we have an active user community of more than thirty thousand deployed instances. So there are plenty of statistics like that, and and we're happy that we also see recognition among, yeah, different, market analysts. So, OnSpace GovDB is seen as as one one of the one of the leading engines in this field. On top of this engine, we actually have a bigger set of, a bigger platform, knowledge graph platform, called you wouldn't guess it on the text platform, which extends what we get out of Graph DB with, analytics services and infrastructure for text analysis, for data quality management, and quite a few other things. So, this platform, yeah, helps you yeah. It makes it much easier to to to deploy, to to make knowledge center solutions. But we'll talk about the platform, later on in this presentation. So stop here. On the right hand side, you see, yeah, a business case or or or the typical solutions, that we offer and that we can customize across across different industries. Like, historically, we started with media and publishing, then we went to financial services. Life science and health care is a is a strong one too. So to finish off with the intro, what makes us unique is this combination of the graph graph database engine, which is suited for all all kinds of use cases for for knowledge graphs. And the combination of this database with text analysis, services and infrastructure tools, like curation tools, and manual annotation tools and so on and so forth that can use and are tuned to use big knowledge graphs for, text analysis. As I said, the entire portfolio is based on, the double two c stack of, standards. And there is also what if open source tooling as part of this this environment. So the entire front end of our database engine is open source. You can get it for kit, use it for everything you want. Then the the plug in API is open source. We are supporter of one of the leading open source projects in this space. The probably the biggest supporter historically, are the four j, also for text analysis platforms, and so on and so forth. So we had the chance to to to and I had the chance to gather a team with a very, very diverse set of, skills and and expertise across different types of technology and and science and and verticals. I'm not going to list them because that's not the purpose of this this presentation. Well, over time, we realized that, actually, the biggest the biggest asset and the most important thing that we have is this ecosystem of partners, which, two years came to trust on to text technology and work with us in in in different ways. So we have all all sorts of consultants and OEM partners, and we partner with quite a few, tool providers. And and many of these people are actually present on this conference. So Symantecweb company, MetaFacts, and and and few others. So, this was all in terms of introduction. So now let's talk about the market. If I gotta talk to to to the investors, the positive message is, well, this market is doing very well. If Gartner is saying that it will be doubling for another two years two years, That's that's just great. And if you look at these, hype cycles and curves, you see knowledge graphs are still still still going up, so on the this peak of, inflated expectations. So, luckily, we're still getting people to who who are calling us just because knowledge graphs are hot. But we also see that, and and and in this case, I can agree with with Gartner. We see that, this trend, is just about to to, yeah, go go down to the yeah. What what they call disillusionment because, yeah, what is important now is to see, really a mature ways, mature patterns, best practices, and and and tools which can be used to deliver to deliver value. So on the bigger picture, knowledge graphs are the adoption of this technology is slower than what my investors would want to see, and probably yours and and and, everybody. The fundamental reason is that, well, that this technology is quite complex. If you make it small, if you just convert a piece of data into into RDF, or you convert the taxonomy in and represent its cost, very likely, you'll you'll not you'll not be able to deliver on all these promises of all these knowledge graph magic does. So you you you have to use, and make big knowledge graphs that can really provide provide some context and allow better better, interpretation of the data and bet better, like, cap better capabilities for linking data. And once we get there, well, it's not as as easy. It takes a while, a while to do it to get the right tooling and the infrastructure to do it. That's why it's it's, our interpretation, why it's slower than expected. We we others this by, yeah, having prepackaged solutions and and partnering with, other vendors who who have solutions also. So the short story here is that, well, there is no single techno. It's not a single, single market. Knowledge graph technology is in the middle of other number of different applications. And, actually, there is no mature market for it now because there is no critical mass of such applications. We we gotta make the applications. We gotta find our way to to to to the end customers, to to really demonstrate the value behind the hype. And then five years from now, we we we we we can expect to see, a a critical mass of applications that generates a critical level of demand for the technology. So, it's clear that with all the diversity of what's possible to do with this technology, no no no single company of, our size can can deal with it. Even much bigger companies cannot deal with this this scale. So, again, our approach to this is to, have a have a ecosystem of partners, that can, that have the right vision for the specific type of applications, and and and have the engineering talent to implement them, taking into account all the specifics and the capacity to deliver. That's quite often a combination of different players. So, that's, yeah, our mind map of knowledge graph applications. So, to to us, knowledge graph is knowledge graphs are on the bigger picture part of the bigger metadata management story, and the applications go into, yeah, equally between data management and content management. There are plenty of knowledge management tools which also use this technology, and that's a big part of our best partners, like, Symantec Web Company and Synoptica. They are in in part of their offerings are in the space of enterprise taxonomies, and and and, vocabulary and this kind of technology, this time based on on on on on, yeah, knowledge gaps. Quite often, these taxonomies and vocabularies, they are actually used for better content management. This is the the the the the applications that you see on the right hand side. That's what Google calls inside engines. So so it's it's all about being able to do cool things with content, to to search, to recommend, to to to re if you're a publisher, for instance, to to repackage it and to to to better deliver it, to extract information, to extract data from it, and so on and so forth. Nowadays, probably the most, yeah, the the the most popular application, although probably not the most often delivered one, is in data management. So using using knowledge graphs for, data integration analytics. And that's what, Gartner called data fabrics. That's that's the whole story of, what was the data warehouse in the past and how you can use them to, yeah, do better better business intelligence and better decision making. Data management is not just that. It's actually much more than this. You have, a range of data governance and cataloging applications, And quite a a big number of these projects are are about, dealing with big, databases, like master and reference data management tasks, so that, yeah, you can aid improve your transactional systems either, like in our case, we we have many projects where we use big volumes of reference data, master data for better text analysis and content content analytics. So you see that, yeah, the the range of applications is quite different. Essentially, each of these, are already some mature markets. So, we we we have to find a way in which this, technology in the middle can contribute to to to, to to making more efficient specific applications in this market, and find a way to to, yeah, gently disrupt them and and and and, offer you you you value there. So that's enough about it. So to summarize the role of, how we see the role of knowledge graphs, the main thing is that they they they essentially put it in context, and and and they do it in two ways. One thing is, linking. So by interlinking descriptions of different things, you you basically help help help different pieces contribute to each other's description. And then the other the other thing is semantic metadata. So semantic metadata is is king. So you you have better formal descriptions of everything that's that's in there. What is that value? Well, someone gets better better insights, better results, better, discoveries, through, through the capability, by by by being able to have unified view, on diverse data. Diverse databases be being able to combine, structured data from different sources, being able to also put in the mix text documents and other content. And, well, quite often, what we see is that to to get the right level of analytics, enterprises should combine proprietary data with publicly available data, with with global data, to combine their wisdom and understanding of the market with background knowledge, which can serve for better interpretation. That's that's what we, deliver most of the time. So, we combine, master data, reference data from different with the different sources, what you see on the right hand side in a knowledge graph, and then we use this knowledge graph to analyze documents. Analyzing documents, we produce metadata, which becomes part of the knowledge graph. And this way, we we put, different kinds of databases and content together. Obviously, there are plenty of applications, and and benefits of doing this both on the content management side and and data management. So metadata is king. Metadata is everything. If you listen to to Gartner, metadata is, at the middle of the, yeah, next generation data integration applications, what they call data fabric. So it's about gathering different types of, metadata, putting them together, structuring them, interlinking them, activating them, and so on. The important thing to realize here is the the diversity that, you everything could be could be metadata. It could be, everything from from the schema of your your database to all sorts of catalogs or taxonomies or reference data, tagging, annotations of content, operational logs, and so on and so forth. So it's really diverse. You quite often need several, many types of metadata combined from from different systems to be able to to to make a flexible data integration platform or content management. I'm not going to read and, through all these, citations of Gartner. The baseline is that the only way to manage this metadata and to put it together is a knowledge graph. And, it is it is a semantic knowledge graph. The semantic approach having formal descriptions of, this metadata that helps you put data and content together. So the RDS tech web technologies allows you to do all this, in well. And there there are several features of the standards in this tech which contribute to this all the way from having global identifiers, which is key for interoperability. Formal semantics is really important because, it it allows sharing meaning and and, interpretation without ambiguities and and and, basically enables enables, much much better analytics and combining different analytic techniques. Standardization is, yeah, someone that everyone in in enterprise data management, enterprise content management, needs and wants unless you want to stick with a single vendor. And, yeah, at the end of the day, it's always about quality and being able to maintain quality. So, also so all means for, like, RDF shapes and other techniques for validation are very important. All these things are missing with the property graph standard. So, they they do great great job for for graph analytics, but there is essentially no standardization, and there is no notion of semantics. There is even no notion of schema. There is no data schema language. So it's obvious that RDF is much much better suited for knowledge gaps, and that's what we hear from, our clients being predominantly enterprise date data architects. We have built the methodology of everything that that it takes to, build in a and and if of knowledge graph all the way from requirement analysis, data gathering, and so on and so forth to, putting together semantic data model, integrating the data, reconciling the data, cleaning the data, and then being able to store, index them, enrich them, make them accessible, and, unable, updates and be able to maintain this knowledge graph. Once once you come with such a complex phenomenon, it's, not a piece of cake to keep it in good shape and good quality and be able to make all the updates that are necessary. So we introduced the On-tech platform, which supports quite quite a big fraction of this tech. Everything that is about, allowing data architects and everyone who who data management people, to collaborate with content management people and deliver value for the business users and the data scientists, which are quite often the consumers of this technology, as part of downstream models or people who train specific machine machine learning, models on it. Quite important to all here is dedicated to to business analysts and subject matter experts. So, part part for offering, is a set of tools that that allows, defining APIs and interfaces and specifying, views on this on this, knowledge graph in the middle by experts, by business analysts. So so you don't don't need a a big team of, programmers to to code middleware. With this, I'll finish my part of the presentation. I want to hand over to Vasil to tell you some updates about the OnText platform and and Graph DB. I'll stop sharing so that Vasil can take over. Thanks for the excellent market overview and giving us all the complexity of the market of around the knowledge graphs and how we can deliver these type of solutions. As database provider, when we meet clients, often we are asked, okay. I understand the concept of agreeing cohort shared identifiers for classes, instances, and some predicates, which identify the meaning of our organizational data. But could you tell me how should I put the data in Gravity, in your database, and how I should really develop applications on top of it? And this is probably the most frequent question we get from our clients when they expect really some sort of a mature tooling around the technology in order to quickly develop solutions. In this my presentation, I'm going to share five key highlights which has happened this year of twenty twenty, And these are really the major developments and tools we which we enable our clients to develop and partners, of course, to develop faster and better knowledge graph solutions. So the first one is really the data virtualization, which gives you a virtual sparkle endpoint access to the different databases. Then we enable the mapping with the mapping new mapping UI subject matter export with no coding to generate RDF models around some skimmers. Then I will speak about the data reconciliation, how you really need to integrate multiple datasets, which speaks about the same type of entities. Then we we go to the consumption of the data. Once we have the knowledge graph, how you can let your data scientists and business analysts to consume the data. And last but not least, I'm going to introduce the GraphQL platform, which gives you a really fantastic way how you can design APIs also for your application solution developers, and there is an automatic way to interact with the knowledge graph with really minimal calling. Okay. First thing first, data virtualization. Nearly all enterprise data is still stored in relational databases, and there is a good reason there because these are transactional systems, master reference records, where the information is primarily stored. The most logical question is how I can move the data from relational databases into a knowledge graph, basically. And we give you two type of use cases. The first type of use case is I want to make a decorative transformation of the relational database into RDF, or I want to access the database on the file. In order to enable this use case, Graphvini nine dot five integrates with the on top framework, which supports decorative, descriptors like the and the OBDA, which are part of the On top project to give you all the fly access to a wide range of relational models like Postgres, DRAMU Oracle, MySQL, Microsoft SQL Server, really a big range of technologies. In order the design decision we took was to expose every virtual every relational database as its own virtual repository in order to make the data locality very transparent for the users. This made the users fully aware that they are accessing different type systems. However, in order to optimize the performance, we implement the highly efficient internal federation, where if you federate two endpoints which are located on the same CloudDB Server, you use a highly efficient internal protocol for data exchange. The benefit is that you get to this on top integration also integrated with all the security features and the the Graph DB security model, single sign on, and all these type of functionalities. So the second feature is really mapping UI, how you can let your subject matter experts and the people knowing the data without coding to populate the knowledge graph. How many of your organizations have most of the critical data in some sort of a spreadsheet, CSV file, or something which is used as a reference model? Using the open refine tool, which is nicely integrated with open with Graph DB and Graph DB Workbench, you can transform any type of structure that you are form of JSON, XML, CSVs, g files, whatever you have, and it gives you an efficient way to clean and cluster the data. Usually, once you start with any type of CSV or Excel files, there is a really sort some type of cleaning. This is nicely integrated with CloudDB so you can create a project with the table of data. On the right side, you can see the new mapping interface, which checks what are the word ontologies in the repository, and it can guide the user what are the product predicates and the properties sorry, the types to use in order to generate a mapping, which is then transformed into RDF file. So from this table or data, you can really record this as a structure. You you basically say how you transgenerate the RDA values and how these values are connected. As a next step, it supports two modes. One is really the interactive mode where the user can, preview the information, clean it, check what we would the output. But, also, it supports the tool supports to batch everything, to record the transformations, and really to make it sound and transparent using a batch script. Also, the API supports streaming data streaming where you can batch really big files and process them on demand. The third big feature is the data reconciliation, and this is the case when you have to integrate multiple datasets sharing the same identifiers. Once again, our focus is to subject matter experts and how without code to perform the entity resolution. The data resolution is the entity resolution is actually matching entities into different type of, tables. And we say this is a mapping of string two things. By using the graph connectivity and the schema, this gives you really a power of subject matter experts in a stepwise process to say how you can reconcile the data. On your screen, you see some American US banks, and the task is really to find the right ID in dataset, like QuickiData or whatever. So why this is an efficient work process and it requires no calling? Because the subject matter experts can start reconciling the state. Like you see on the diagram, the reconciling the US states is an easy task because there is no big community. Then you can go to the cities by using the state, which should use the, in the because the, cities are connected to the states. Later on, at the final step, you need to really reconcile the organization and pick the right organization. Next, let's imagine that you have the knowledge graph and you passed all the hurdles to put the data in a knowledge graph and the RDL databases. But the next question is, hey. I have a big data scientist and knowledge engineer team, and none of them knows Parco. They're really good with business BA type of tools like Tableau, Microsoft Power BI. Could you just export me a spreadsheet and a table in where I can analyze it? No. This is really a forward approach because if you you'll be replicating data, which is wrong. Now browDB comes with the JDB access, which enables all type of tools which supports JDBC like Tableau, Microsoft BI, to directly interact with the data and to connect, investigate what are the tables. And you can directly visualize and take the data out of the database. The way how we do this is we define SQL views, which are basically SPARQL results. In these SPARQL results, there is a nice user interface where you put your SPARQL query, you say what is the SQL table name, and the interface, depending on the data distribution, guides you how to use this right SQL types in order to get the results. At the end so below you see a one technicality is that this driver will also try to push as much as possible of the complexity down to the database. So if you have work logs in the SQL query, it will be pushed down to the sparkle in order to minimize the exchange between the SQL engine and the RTL database. Last but not least, this is really a very big topic. Autotech invested in a platform based on GraphQL, how you can really expose how you can expose knowledge graphs and quickly build and generate APIs. One of the main challenges is once you design any type of a system is that you need to plan for the APIs, and these APIs needs to be synchronized with the scheme of the model. Now the Graph DB and the now the auto text platform can read the data with using this auto text platform workbench, the schema of the of the graph database, and automatically generate the Graph QL API for you. Using this GraphQL API, you can select what are only the required permitted operations in order to control the access to the data. By for bridging the GraphQL to RDF, Autotext develops some really key features. One is really a highly efficient graph to sparkle translation. This seems as an easy task, but in order to get the full complexity, we have tweak the semantic the semantics of SPARQL and having a special execution mode, which makes this really efficient. Also, this is a bidirectional mapping where we're using mutations, which can be also automatically generated. You can interact with the data and modify it, directly from the GraphQL. This is really very good if you have UI applications who need some which needs some sort of data validation or access control, which is also supported over the schema. What's point here is, this I'm not sure how many of you are familiar with the GraphQL ecosystem, but there is another excellent feature, which is the GraphQL federation, which allows the integration of third party services. This is a service level integration of the acquired data visualization, but for other type of services. Since this is a very broad topic and I don't have time to go in details, I really invite you to join the later session, which will be led by Jim Rayfield, our chief solution, Arhite, who can give you in the deep technicalities of GraphQL federation and knowledge graphs and how you can control your knowledge graph via the GraphQL APIs and how you open the spectral chaos. Okay. Thank you. I give it now back to Naso to continue with the partner ecosystem and how all these tooling and key features we develop as a successful business model using our partners. Thank you, Vasil. Although, yeah, we we we have a very capable database engine, and we have, a platform on top of it that that provides a lot of infrastructure and technology, we still cannot cover everything that you may want to do with the knowledge graph, and, before start speaking of applications. So, to address this, we have, a range of portfolio partners, which are companies like Symantec Web Company, Synaptica, MetaFacts, the sensor, Omni, Memotix, Workbench, quite a number of partners who provide tools and platforms which integrate Graph DB. And and and, this way, we complement each other's portfolio, and we can much better add the the full stack of needs for this technology. Beyond that, we we we work quite a lot with, different kind of consulting partners. So we work with, some of the biggest, IT service, providers and system integrators on Earth. On the list there, you have, like, Infosys, WIPO, Auto SentiData, Fujitsu. These are five of the top ten biggest IT services companies, and, they they are familiar with the technology and and and delivery to to to big clients all the way from government to to pharma and back. We also have, much, yes, more specialized, let's call them boutique consulting and delivery partners, like, data language in in UK and enterprise knowledge in in US, Southworks in Korea, and and and many others. So, this is an ecosystem that currently consists of more than fifty partners. So, we will not try to list all of them, and we took care to to to put it right on for all kinds of, agreements to to make to make this ecosystem operate, properly. So, yeah, we're looking forward to join other partners to complement the even further, the ecosystem with either consulting delivery skills, either a a a, yeah, tools. And this this good point, to to invite you to the webinar. Vasu, if you can go to the next slide. The the the the webinar that, will take place in twenty minutes. That's part of the promotion to party eight of our partner, Semantic Web Company. You see that Vasil is having a very intensive day today. So he'll finish here, and he'll be part of, the webinar that starts in twenty minute with minutes with Pu party eight. So, it will be it will essentially show how, yeah, yeah, the integration between pool party eight and and and and and Gravity makes pool party much better performant and, improves on on the scalability side. And we also demonstrate, Vasil, I think you gotta press next because there is some animation. Okay. Part of this demo will be, well, we have a we we we have a, new stacking service scope now, and it works with giant technology graph derived from Wikidata, geo names, Wikipedia, whatever. And we use it to analyze news articles. So part of this service will be a demonstration of how you can, tune the performance, the the the the behavior of this, very sophisticated natural language processing text analysis pipeline by, doing small corrections to this knowledge graph with pool party eight and immediately see improvement of results. In this way, we we show how voice management and tailor made vocabularies, and and, expert work can can can combine and can deliver value in combination with very big knowledge graphs. So it's it's a work with them. We've been and we're I've been working half half life to make this possible, so you you still have a chance to join in twenty minutes. And that's just one of our partners. We are very happy with our partnership with Symantec Web company. So join the webinar if you wanna pursue, and, the the people from Semantic Web Company tell you more about it. That's everything from us. Thank you very much for your time and attention. Massive, thanks to Naso and, Vasil. It's really interesting to see.",
    "transcript_length": 32147,
    "speaker": "Atanas Kiryakov",
    "tags": [
      "james",
      "naso kiriakov",
      "vasil mamcheff",
      "vasil momchev"
    ]
  },
  {
    "title": "Knowledge Graphs as Hub for Data, Metadata and Content",
    "description": "There is no mature market for Knowledge Graphs! Not yet. But they disrupt data and content management by offering flexible integration, unified access and a platform for analytics.\n\nWe will mindmap the markets and the applications where knowledge graphs add value:\n\n- Decision making: derive profound insights from richer and better interconnected data.\n- Search and discovery: get more relevant results and connect the dots in less time.\n- Publishing: repackage and better monetize content and data.\n- Data governance: automation of updates and data quality management.\n- Operations: infrastructure monitoring and configuration management.\nWe will present how Ontotext puts together the technology stack and the partner ecosystem needed to deliver all these applications.\n\nFinally, we will show how the JDBC driver and the data virtualization, recently added to GraphDB, make it easier to transform your data into a graph, mapp them to a big ontology (e.g. FIBO) and access them from a BI tool.",
    "category": "Knowledge Graphs",
    "transcript": "It's my great pleasure to introduce, Naso Kiriakov and Vasil Momchev from, our good friends, OntoText. OntoText have been in this space a long time, and really looking forward to finding out the latest kind of direction of the, the the product that, they've they've developed. And I'll be moderating this session. Please, say hello, introduce yourselves, tell us where you're from, and also don't hesitate to, answer any questions, and we'll, we'll cover those at the end. Great. Welcome, Naso and, Vasil, and and over to you. Thank you. Thank you very much, James. It's indeed a great pleasure to, yeah, to be to be again part of this conference. We have fresh memories from one year ago in London. So, it will be myself presenting and and OntoTech's, CTO, Vasil Mamcheff. So I I I will I will entertain you with a bit of introduction of, who we are, and then I'll share our vision on on the market for knowledge gap technology and different applications and how, semantic metadata are the key to put, to for quite a number of different applications and how how they, can improve both data management and content management, and and actually put data and content together. Then, I'll hand over to Vasil to share with you, yeah, few updates about, Onstext platform and and and our database engine, Graph DB, and to to to basically walk you through, the different aspects of what it takes to to get data in a graph database, to to transform them, to make a proper graph out of it, and then to consume them either from whatever tool all the way from BI tool, that access data to a JDBC driver to another application that use use it through, say, GraphQL or other APIs. And then we'll finish off with an overview of our partner ecosystem because, with the those wide range of applications of these technologies, there is no single vendor, at least, no no no none of these, specialized technology providers, who who can deliver all the applications to you. So we built a partner ecosystem, to to to to be able to, put the right technology stack in place and to be able to to to also deliver it at at scale. So let's get started with the introduction . So on to take this, yeah, we started with, what was back then semantic weapon, semantic technology in year two thousand. And we had the chance to to to be to be part of the avant garde in this technology and went to the all the evolution to link to open data, nowadays, knowledge gaps. And, well, nowadays, we are, like, most of our clients, are are the total leaders in different industries and markets, like multinational companies. Still, if we have to to to come to where where their headquarters are, it's quite an even split between UK where where we got our first clients, big clients in stories ten years ago, and, then US has a growing area, Asia also, and main, mainland Europe. Up until a year ago, it was easy to say Europe. Now you gotta say Europe and UK. But, anyway so, you see some selection of our clients. I'll talk a bit more about the verticals , But, yeah, you see that most of them are precisely this type of really big enterprises. We we are not allowed to mention some of them, but that's that's the way it is. We being being part of this industry and part of the this community that is, developing the trends and setting the standards, we we do this through through a number of, yeah, organizations, worldwide web that we would see, being the main one, which not only standardizes World Wide Web, HTML, HTP, and all these standards on which the World Wide Web is built, but they are also the ones that standardized RDS, Fargo, OWL, and the stack of, the stack of standards that we we build our technology on. And we have we had the chance, and we we are still very happy to, to keep pushing our research to a collaborative project. So we are we are very, very well interconnected with those sorts of academic groups in this field in Europe. Our best, our most popular product is, Onstead's Graph DB. We're getting, I don't know, probably, thousand or a bit more downloads, of, Graph DB per month. And, yeah, we have an active user community of more than thirty thousand deployed instances. So there are plenty of statistics like that, and and we're happy that we also see recognition among, yeah, different, market analysts. So, OnSpace GovDB is seen as as one one of the one of the leading engines in this field. On top of this engine, we actually have a bigger set of, a bigger platform, knowledge graph platform, called you wouldn't guess it on the text platform, which extends what we get out of Graph DB with, analytics services and infrastructure for text analysis, for data quality management, and quite a few other things. So, this platform, yeah, helps you yeah. It makes it much easier to to to deploy, to to make knowledge center solutions. But we'll talk about the platform, later on in this presentation. So stop here. On the right hand side, you see, yeah, a business case or or or the typical solutions, that we offer and that we can customize across across different industries. Like, historically, we started with media and publishing, then we went to financial services. Life science and health care is a is a strong one too. So to finish off with the intro, what makes us unique is this combination of the graph graph database engine, which is suited for all all kinds of use cases for for knowledge graphs. And the combination of this database with text analysis, services and infrastructure tools, like curation tools, and manual annotation tools and so on and so forth that can use and are tuned to use big knowledge graphs for, text analysis. As I said, the entire portfolio is based on, the double two c stack of, standards. And there is also what if open source tooling as part of this this environment. So the entire front end of our database engine is open source. You can get it for kit, use it for everything you want. Then the the plug in API is open source. We are supporter of one of the leading open source projects in this space. The probably the biggest supporter historically, are the four j, also for text analysis platforms, and so on and so forth. So we had the chance to to to and I had the chance to gather a team with a very, very diverse set of, skills and and expertise across different types of technology and and science and and verticals. I'm not going to list them because that's not the purpose of this this presentation. Well, over time, we realized that, actually, the biggest the biggest asset and the most important thing that we have is this ecosystem of partners, which, two years came to trust on to text technology and work with us in in in different ways. So we have all all sorts of consultants and OEM partners, and we partner with quite a few, tool providers. And and many of these people are actually present on this conference. So Symantecweb company, MetaFacts, and and and few others. So, this was all in terms of introduction. So now let's talk about the market. If I gotta talk to to to the investors, the positive message is, well, this market is doing very well. If Gartner is saying that it will be doubling for another two years two years, That's that's just great. And if you look at these, hype cycles and curves, you see knowledge graphs are still still still going up, so on the this peak of, inflated expectations. So, luckily, we're still getting people to who who are calling us just because knowledge graphs are hot. But we also see that, and and and in this case, I can agree with with Gartner. We see that, this trend, is just about to to, yeah, go go down to the yeah. What what they call disillusionment because, yeah, what is important now is to see, really a mature ways, mature patterns, best practices, and and and tools which can be used to deliver to deliver value. So on the bigger picture, knowledge graphs are the adoption of this technology is slower than what my investors would want to see, and probably yours and and and, everybody. The fundamental reason is that, well, that this technology is quite complex. If you make it small, if you just convert a piece of data into into RDF, or you convert the taxonomy in and represent its cost, very likely, you'll you'll not you'll not be able to deliver on all these promises of all these knowledge graph magic does. So you you you have to use, and make big knowledge graphs that can really provide provide some context and allow better better, interpretation of the data and bet better, like, cap better capabilities for linking data. And once we get there, well, it's not as as easy. It takes a while, a while to do it to get the right tooling and the infrastructure to do it. That's why it's it's, our interpretation, why it's slower than expected. We we others this by, yeah, having prepackaged solutions and and partnering with, other vendors who who have solutions also. So the short story here is that, well, there is no single techno. It's not a single, single market. Knowledge graph technology is in the middle of other number of different applications. And, actually, there is no mature market for it now because there is no critical mass of such applications. We we gotta make the applications. We gotta find our way to to to to the end customers, to to really demonstrate the value behind the hype. And then five years from now, we we we we we can expect to see, a a critical mass of applications that generates a critical level of demand for the technology. So, it's clear that with all the diversity of what's possible to do with this technology, no no no single company of, our size can can deal with it. Even much bigger companies cannot deal with this this scale. So, again, our approach to this is to, have a have a ecosystem of partners, that can, that have the right vision for the specific type of applications, and and and have the engineering talent to implement them, taking into account all the specifics and the capacity to deliver. That's quite often a combination of different players. So, that's, yeah, our mind map of knowledge graph applications. So, to to us, knowledge graph is knowledge graphs are on the bigger picture part of the bigger metadata management story, and the applications go into, yeah, equally between data management and content management. There are plenty of knowledge management tools which also use this technology, and that's a big part of our best partners, like, Symantec Web Company and Synoptica. They are in in part of their offerings are in the space of enterprise taxonomies, and and and, vocabulary and this kind of technology, this time based on on on on on, yeah, knowledge gaps. Quite often, these taxonomies and vocabularies, they are actually used for better content management. This is the the the the the applications that you see on the right hand side. That's what Google calls inside engines. So so it's it's all about being able to do cool things with content, to to search, to recommend, to to to re if you're a publisher, for instance, to to repackage it and to to to better deliver it, to extract information, to extract data from it, and so on and so forth. Nowadays, probably the most, yeah, the the the most popular application, although probably not the most often delivered one, is in data management. So using using knowledge graphs for, data integration analytics. And that's what, Gartner called data fabrics. That's that's the whole story of, what was the data warehouse in the past and how you can use them to, yeah, do better better business intelligence and better decision making. Data management is not just that. It's actually much more than this. You have, a range of data governance and cataloging applications, And quite a a big number of these projects are are about, dealing with big, databases, like master and reference data management tasks, so that, yeah, you can aid improve your transactional systems either, like in our case, we we have many projects where we use big volumes of reference data, master data for better text analysis and content content analytics. So you see that, yeah, the the range of applications is quite different. Essentially, each of these, are already some mature markets. So, we we we have to find a way in which this, technology in the middle can contribute to to to, to to making more efficient specific applications in this market, and find a way to to, yeah, gently disrupt them and and and and, offer you you you value there. So that's enough about it. So to summarize the role of, how we see the role of knowledge graphs, the main thing is that they they they essentially put it in context, and and and they do it in two ways. One thing is, linking. So by interlinking descriptions of different things, you you basically help help help different pieces contribute to each other's description. And then the other the other thing is semantic metadata. So semantic metadata is is king. So you you have better formal descriptions of everything that's that's in there. What is that value? Well, someone gets better better insights, better results, better, discoveries, through, through the capability, by by by being able to have unified view, on diverse data. Diverse databases be being able to combine, structured data from different sources, being able to also put in the mix text documents and other content. And, well, quite often, what we see is that to to get the right level of analytics, enterprises should combine proprietary data with publicly available data, with with global data, to combine their wisdom and understanding of the market with background knowledge, which can serve for better interpretation. That's that's what we, deliver most of the time. So, we combine, master data, reference data from different with the different sources, what you see on the right hand side in a knowledge graph, and then we use this knowledge graph to analyze documents. Analyzing documents, we produce metadata, which becomes part of the knowledge graph. And this way, we we put, different kinds of databases and content together. Obviously, there are plenty of applications, and and benefits of doing this both on the content management side and and data management. So metadata is king. Metadata is everything. If you listen to to Gartner, metadata is, at the middle of the, yeah, next generation data integration applications, what they call data fabric. So it's about gathering different types of, metadata, putting them together, structuring them, interlinking them, activating them, and so on. The important thing to realize here is the the diversity that, you everything could be could be metadata. It could be, everything from from the schema of your your database to all sorts of catalogs or taxonomies or reference data, tagging, annotations of content, operational logs, and so on and so forth. So it's really diverse. You quite often need several, many types of metadata combined from from different systems to be able to to to make a flexible data integration platform or content management. I'm not going to read and, through all these, citations of Gartner. The baseline is that the only way to manage this metadata and to put it together is a knowledge graph. And, it is it is a semantic knowledge graph. The semantic approach having formal descriptions of, this metadata that helps you put data and content together. So the RDS tech web technologies allows you to do all this, in well. And there there are several features of the standards in this tech which contribute to this all the way from having global identifiers, which is key for interoperability. Formal semantics is really important because, it it allows sharing meaning and and, interpretation without ambiguities and and and, basically enables enables, much much better analytics and combining different analytic techniques. Standardization is, yeah, someone that everyone in in enterprise data management, enterprise content management, needs and wants unless you want to stick with a single vendor. And, yeah, at the end of the day, it's always about quality and being able to maintain quality. So, also so all means for, like, RDF shapes and other techniques for validation are very important. All these things are missing with the property graph standard. So, they they do great great job for for graph analytics, but there is essentially no standardization, and there is no notion of semantics. There is even no notion of schema. There is no data schema language. So it's obvious that RDF is much much better suited for knowledge gaps, and that's what we hear from, our clients being predominantly enterprise date data architects. We have built the methodology of everything that that it takes to, build in a and and if of knowledge graph all the way from requirement analysis, data gathering, and so on and so forth to, putting together semantic data model, integrating the data, reconciling the data, cleaning the data, and then being able to store, index them, enrich them, make them accessible, and, unable, updates and be able to maintain this knowledge graph. Once once you come with such a complex phenomenon, it's, not a piece of cake to keep it in good shape and good quality and be able to make all the updates that are necessary. So we introduced the On-tech platform, which supports quite quite a big fraction of this tech. Everything that is about, allowing data architects and everyone who who data management people, to collaborate with content management people and deliver value for the business users and the data scientists, which are quite often the consumers of this technology, as part of downstream models or people who train specific machine machine learning, models on it. Quite important to all here is dedicated to to business analysts and subject matter experts. So, part part for offering, is a set of tools that that allows, defining APIs and interfaces and specifying, views on this on this, knowledge graph in the middle by experts, by business analysts. So so you don't don't need a a big team of, programmers to to code middleware. With this, I'll finish my part of the presentation. I want to hand over to Vasil to tell you some updates about the OnText platform and and Graph DB. I'll stop sharing so that Vasil can take over. Thanks for the excellent market overview and giving us all the complexity of the market of around the knowledge graphs and how we can deliver these type of solutions. As database provider, when we meet clients, often we are asked, okay. I understand the concept of agreeing cohort shared identifiers for classes, instances, and some predicates, which identify the meaning of our organizational data. But could you tell me how should I put the data in Gravity, in your database, and how I should really develop applications on top of it? And this is probably the most frequent question we get from our clients when they expect really some sort of a mature tooling around the technology in order to quickly develop solutions. In this my presentation, I'm going to share five key highlights which has happened this year of twenty twenty, And these are really the major developments and tools we which we enable our clients to develop and partners, of course, to develop faster and better knowledge graph solutions. So the first one is really the data virtualization, which gives you a virtual sparkle endpoint access to the different databases. Then we enable the mapping with the mapping new mapping UI subject matter export with no coding to generate RDF models around some skimmers. Then I will speak about the data reconciliation, how you really need to integrate multiple datasets, which speaks about the same type of entities. Then we we go to the consumption of the data. Once we have the knowledge graph, how you can let your data scientists and business analysts to consume the data. And last but not least, I'm going to introduce the GraphQL platform, which gives you a really fantastic way how you can design APIs also for your application solution developers, and there is an automatic way to interact with the knowledge graph with really minimal calling. Okay. First thing first, data virtualization. Nearly all enterprise data is still stored in relational databases, and there is a good reason there because these are transactional systems, master reference records, where the information is primarily stored. The most logical question is how I can move the data from relational databases into a knowledge graph, basically. And we give you two type of use cases. The first type of use case is I want to make a decorative transformation of the relational database into RDF, or I want to access the database on the file. In order to enable this use case, Graphvini nine dot five integrates with the on top framework, which supports decorative, descriptors like the and the OBDA, which are part of the On top project to give you all the fly access to a wide range of relational models like Postgres, DRAMU Oracle, MySQL, Microsoft SQL Server, really a big range of technologies. In order the design decision we took was to expose every virtual every relational database as its own virtual repository in order to make the data locality very transparent for the users. This made the users fully aware that they are accessing different type systems. However, in order to optimize the performance, we implement the highly efficient internal federation, where if you federate two endpoints which are located on the same CloudDB Server, you use a highly efficient internal protocol for data exchange. The benefit is that you get to this on top integration also integrated with all the security features and the the Graph DB security model, single sign on, and all these type of functionalities. So the second feature is really mapping UI, how you can let your subject matter experts and the people knowing the data without coding to populate the knowledge graph. How many of your organizations have most of the critical data in some sort of a spreadsheet, CSV file, or something which is used as a reference model? Using the open refine tool, which is nicely integrated with open with Graph DB and Graph DB Workbench, you can transform any type of structure that you are form of JSON, XML, CSVs, g files, whatever you have, and it gives you an efficient way to clean and cluster the data. Usually, once you start with any type of CSV or Excel files, there is a really sort some type of cleaning. This is nicely integrated with CloudDB so you can create a project with the table of data. On the right side, you can see the new mapping interface, which checks what are the word ontologies in the repository, and it can guide the user what are the product predicates and the properties sorry, the types to use in order to generate a mapping, which is then transformed into RDF file. So from this table or data, you can really record this as a structure. You you basically say how you transgenerate the RDA values and how these values are connected. As a next step, it supports two modes. One is really the interactive mode where the user can, preview the information, clean it, check what we would the output. But, also, it supports the tool supports to batch everything, to record the transformations, and really to make it sound and transparent using a batch script. Also, the API supports streaming data streaming where you can batch really big files and process them on demand. The third big feature is the data reconciliation, and this is the case when you have to integrate multiple datasets sharing the same identifiers. Once again, our focus is to subject matter experts and how without code to perform the entity resolution. The data resolution is the entity resolution is actually matching entities into different type of, tables. And we say this is a mapping of string two things. By using the graph connectivity and the schema, this gives you really a power of subject matter experts in a stepwise process to say how you can reconcile the data. On your screen, you see some American US banks, and the task is really to find the right ID in dataset, like QuickiData or whatever. So why this is an efficient work process and it requires no calling? Because the subject matter experts can start reconciling the state. Like you see on the diagram, the reconciling the US states is an easy task because there is no big community. Then you can go to the cities by using the state, which should use the, in the because the, cities are connected to the states. Later on, at the final step, you need to really reconcile the organization and pick the right organization. Next, let's imagine that you have the knowledge graph and you passed all the hurdles to put the data in a knowledge graph and the RDL databases. But the next question is, hey. I have a big data scientist and knowledge engineer team, and none of them knows Parco. They're really good with business BA type of tools like Tableau, Microsoft Power BI. Could you just export me a spreadsheet and a table in where I can analyze it? No. This is really a forward approach because if you you'll be replicating data, which is wrong. Now browDB comes with the JDB access, which enables all type of tools which supports JDBC like Tableau, Microsoft BI, to directly interact with the data and to connect, investigate what are the tables. And you can directly visualize and take the data out of the database. The way how we do this is we define SQL views, which are basically SPARQL results. In these SPARQL results, there is a nice user interface where you put your SPARQL query, you say what is the SQL table name, and the interface, depending on the data distribution, guides you how to use this right SQL types in order to get the results. At the end so below you see a one technicality is that this driver will also try to push as much as possible of the complexity down to the database. So if you have work logs in the SQL query, it will be pushed down to the sparkle in order to minimize the exchange between the SQL engine and the RTL database. Last but not least, this is really a very big topic. Autotech invested in a platform based on GraphQL, how you can really expose how you can expose knowledge graphs and quickly build and generate APIs. One of the main challenges is once you design any type of a system is that you need to plan for the APIs, and these APIs needs to be synchronized with the scheme of the model. Now the Graph DB and the now the auto text platform can read the data with using this auto text platform workbench, the schema of the of the graph database, and automatically generate the Graph QL API for you. Using this GraphQL API, you can select what are only the required permitted operations in order to control the access to the data. By for bridging the GraphQL to RDF, Autotext develops some really key features. One is really a highly efficient graph to sparkle translation. This seems as an easy task, but in order to get the full complexity, we have tweak the semantic the semantics of SPARQL and having a special execution mode, which makes this really efficient. Also, this is a bidirectional mapping where we're using mutations, which can be also automatically generated. You can interact with the data and modify it, directly from the GraphQL. This is really very good if you have UI applications who need some which needs some sort of data validation or access control, which is also supported over the schema. What's point here is, this I'm not sure how many of you are familiar with the GraphQL ecosystem, but there is another excellent feature, which is the GraphQL federation, which allows the integration of third party services. This is a service level integration of the acquired data visualization, but for other type of services. Since this is a very broad topic and I don't have time to go in details, I really invite you to join the later session, which will be led by Jim Rayfield, our chief solution, Arhite, who can give you in the deep technicalities of GraphQL federation and knowledge graphs and how you can control your knowledge graph via the GraphQL APIs and how you open the spectral chaos. Okay. Thank you. I give it now back to Naso to continue with the partner ecosystem and how all these tooling and key features we develop as a successful business model using our partners. Thank you, Vasil. Although, yeah, we we we have a very capable database engine, and we have, a platform on top of it that that provides a lot of infrastructure and technology, we still cannot cover everything that you may want to do with the knowledge graph, and, before start speaking of applications. So, to address this, we have, a range of portfolio partners, which are companies like Symantec Web Company, Synaptica, MetaFacts, the sensor, Omni, Memotix, Workbench, quite a number of partners who provide tools and platforms which integrate Graph DB. And and and, this way, we complement each other's portfolio, and we can much better add the the full stack of needs for this technology. Beyond that, we we we work quite a lot with, different kind of consulting partners. So we work with, some of the biggest, IT service, providers and system integrators on Earth. On the list there, you have, like, Infosys, WIPO, Auto SentiData, Fujitsu. These are five of the top ten biggest IT services companies, and, they they are familiar with the technology and and and delivery to to to big clients all the way from government to to pharma and back. We also have, much, yes, more specialized, let's call them boutique consulting and delivery partners, like, data language in in UK and enterprise knowledge in in US, Southworks in Korea, and and and many others. So, this is an ecosystem that currently consists of more than fifty partners. So, we will not try to list all of them, and we took care to to to put it right on for all kinds of, agreements to to make to make this ecosystem operate, properly. So, yeah, we're looking forward to join other partners to complement the even further, the ecosystem with either consulting delivery skills, either a a a, yeah, tools. And this this good point, to to invite you to the webinar. Vasu, if you can go to the next slide. The the the the webinar that, will take place in twenty minutes. That's part of the promotion to party eight of our partner, Semantic Web Company. You see that Vasil is having a very intensive day today. So he'll finish here, and he'll be part of, the webinar that starts in twenty minute with minutes with Pu party eight. So, it will be it will essentially show how, yeah, yeah, the integration between pool party eight and and and and and Gravity makes pool party much better performant and, improves on on the scalability side. And we also demonstrate, Vasil, I think you gotta press next because there is some animation. Okay. Part of this demo will be, well, we have a we we we have a, new stacking service scope now, and it works with giant technology graph derived from Wikidata, geo names, Wikipedia, whatever. And we use it to analyze news articles. So part of this service will be a demonstration of how you can, tune the performance, the the the the behavior of this, very sophisticated natural language processing text analysis pipeline by, doing small corrections to this knowledge graph with pool party eight and immediately see improvement of results. In this way, we we show how voice management and tailor made vocabularies, and and, expert work can can can combine and can deliver value in combination with very big knowledge graphs. So it's it's a work with them. We've been and we're I've been working half half life to make this possible, so you you still have a chance to join in twenty minutes. And that's just one of our partners. We are very happy with our partnership with Symantec Web company. So join the webinar if you wanna pursue, and, the the people from Semantic Web Company tell you more about it. That's everything from us. Thank you very much for your time and attention. Massive, thanks to Naso and, Vasil. It's really interesting to see.",
    "transcript_length": 32147,
    "speaker": "Vassil Momtchev",
    "tags": [
      "james",
      "naso kiriakov",
      "vasil mamcheff",
      "vasil momchev"
    ]
  },
  {
    "title": "Knowledge Graphs, Graph AI, & the Need for High performance Graph Computing",
    "description": "Learn how to extract actionable insights from massive datasets using a Graph Intelligence Platform.\n\n- The need for Graph Intelligence\n- The next era of graph computing\n- Platform overview\n- Use cases in Life Sciences, Financial Services, and Information Security\n- Benchmarks and proof points",
    "category": "Graph AI",
    "transcript": "It's a great pleasure to give this talk. Thank you so much for inviting me. My name is Keshav Pingali. I'm the CEO of Katana Graph. I'm also a chair professor in the computer science department at the University of Texas at Austin. And today, I want to talk about knowledge graphs, graph AI, and the need for high performance graph computing. Let me begin by telling you why we at Katana Graph feel there is a great need for high performance graph computing. Usually, when people talk about high performance computing, they think about computational science applications, such as the design of aircraft or the design of buildings, for which people use very large clusters, like the stampede cluster that we have at the Texas Advanced Computing Center in Austin. However, about ten years ago, a group of, researchers, including myself and and some colleagues at MIT, CMU, and other places, realized that there was a big opportunity to take all of the techniques that we had developed in the context of computational science app applications in high performance computing and to use them for doing high performance graph computing. And, basically, there are two parts to the argument for why this is a useful thing to do. One having to do with the volume of data and the other having to do with time to insight. So first, when it comes to the volume of data, we're all familiar with terms like the data tsunami, the data flood, data deluge. What all these apocalyptic terms tell us is that the world has generated an enormous amount of data and is generating even more data at an ever accelerating pace. And so if you read documents like IDC twenty, for example, you find statistics like more than half of the world's data was created in the last two years, but less than two percent has been analyzed. To be sure, some of this data that's being generated is relational data. You can represent the data very usefully as tables or relations, store them in relational databases, and then query them using SQL and other time honored techniques. But what we find is more and more of the new data that's being generated is what we call unstructured data that doesn't quite fit the relational model that well. And a lot of this unstructured data, it turns out, can be usefully represented as a graph and then processed using graph techniques. So that is the first side of the argument, the first part of the argument about why high performance graph computing is essential, the sheer volume of graph data that needs to be analyzed. The second part of the argument is time to insight. In most application areas, there's usually a window of opportunity. And if you can perform your analytics on the data within that window of opportunity and obtain some insight, then you can profit from it. Whereas if your insights come after the window of opportunity has expired, well, then the, processing that you have done, the analytics that you have done are not very useful. And so these are the two reasons why we believe high performance scale out graph computing is going to be increasingly important as we go forward. Just the sheer amount of graph data that needs to be processed and then the need to process that very quickly. One question that we're always asked is, what are some of the areas where we see the use of graphs and graph technologies? This wheel over here tells you about some of the areas where we have, seen our customers using graphs. And I'm just going to go very quickly over some of the highlights. At the top right hand side of the circle, you see intrusion detection, fraud detection, large multinational defense contractor that was interested in doing real time intrusion detection in computer networks. And the way they wanted to do it was by building what are called interaction graphs and then mining these interaction graphs to find forbidden patterns within these graphs. So finding needles within a big haystack, as it's sometimes called. So that is one application, fraud detection, anti money laundering, lots of applications there. Another area where we see graphs being used increasingly is in the identity management space. So one of our customers ultimately wants to go to a graph with a trillion edges, which is very large indeed. Even in more conventional areas that have been around for a while, like electronic design automation, so circuit design, we see increasing use of graphs and hypergraphs. So circuits can be looked at as graphs or hypergraphs. The pins of the circuit correspond to the nodes of the graph. The wires correspond to the edges. And it turns out that a lot of the things they do in chip design, like static timing analysis, logic synthesis, placement routing, All of them can be done very effectively using parallel algorithms of the kind that we do at Katana. The final area I mentioned is medical knowledge graphs. So precision medicine, hypothesis generation for drugs. So here, the problem is you have a lot of medical data, and you want to represent that medical data in one big graph. And so this is called a medical knowledge graph. And one of the customers that we're currently dealing with wants to be able to mine these medical knowledge graphs in order to generate promising treatments for diseases and equally importantly, to rule out potential treatments for diseases before having to go to the lab, do a lot of expensive tests, and then find out that that treatment actually doesn't work. So these are just some of the areas where we see graphs. There are, in fact, many more. So this brings us to a data graph. So what do we do? Well, at the heart of our system, and I'll show you a cartoon diagram of our system on the next slide, we basically have a scale out graph engine that is optimized for doing analytics, for doing AI machine learning on very large graphs. So what we are building is what we call a graph intelligence platform, and it's been architected from the get go to handle massive graphs. It's been tested with some of the largest publicly available web crawl graphs, like w d c twelve, which has a hundred and twenty eight billion edges. Our system has been designed from the get go for graph computing and not so much for, for being a graph database unlike many of the other systems in this space. And so we can do in memory analytics, AI, and machine learning very fast. And for some of the routines that we have implemented for our customers, we're, like, ten to a hundred times faster than competing solutions. Massive scalability, we've shown that our graph engine scales to up to two fifty six machines on the big stampede supercomputing cluster that we have at Texas. We also run on all the three open clouds. So AWS is your Google Cloud. We are cloud first, as they say. And then the final thing that we have in our graph engine is we have native AI and machine learning. So we support a lot of unsupervised and supervised training algorithms. And then you can have your AI and machine learning pipelines that include querying, analytics, as well as third party integrations in your complete workflow while without leaving our system. And I'll show you some examples of that as we go forward. So here is the one click down cartoon level picture of our system. So at the heart of it is in the middle, you can see the scalable distributed graph engine. So it's got three parts to it. So the first part is CUSP, which is a streaming partitioner. If you have a big graph, it's usually where it's at rest. It's in AWS s three storage, your Google Blob storage, or somewhere. So what CUSP does is if you point it to this particular graph that's sitting in secondary storage, and then you say, here are the number of machines on which we want you to partition or shard the graph. And then you could also specify the, sharding policy or the distribution policy. You can even write your own partitioning policy quite easily using CUSP. What CUSP does is it then reads in the graph from secondary storage, and then it shards or partitions the graph between as many machines as you have told it to do. And then it builds an in memory representation of each portion of the graph on each of the machines in your cluster. Then the Galois graph computing engine kicks in. So the Galois graph computing engine is a shared memory graph computing engine. It's got a bunch of data structures and a runtime system that, again, has been optimized for many years for the needs of graph computing. Now anytime you shard or partition a graph, you need to worry about what happens at the boundaries of the partitions. You're going to need some synchronization and communication in order to keep the data on the different machines in lockstep, so to speak. So we support, programming model that's called the bulk synchronous programming model. And the communication and synchronization that's required in order to keep all of the different machines working together is performed by the Gluon communication engine. This, again, is a communication runtime that has been designed by us and optimized for the needs of graph computing. So this is essentially the heart of our scalable distributed graph engine. This is our platform. And as I told you, we have shown that it scales up to two fifty six machines, and it would probably scale to even larger numbers of machines as needed. We're exposing the functionality of this graph engine through c plus plus. So if you like writing c plus plus code, we give you a small number of constructs, and then you can go write c plus plus code and orchestrate the graph engine directly. And many of our routines in our libraries are written that way. But for data scientists, we're also providing, access to the graph engine through Python. So, again, we give you a small number of constructs, some of our data structures, and then you can orchestrate the graph engine directly from Python. What we are doing on this graph engine, the libraries that we are building are in the space of graph query. So we support OpenCypher, which is a query language that Neo four j has popularized, and we like it a great deal. GQL is a new query language that's coming out soon, so we're going to support that as well. We also have a lot of graph analytics routines, so all of the routines for path finding and community detection between the centrality and so on. If you go to our website, you can see a long list of these analytics routines that we support. We have some of the world's experts on graph pattern mining working in the company, and so we also have a lot of graph mining routines such as, for example, click detection or frequent subgraph mining routines. And then, of course, increasingly important areas, so Graph AI. So we have both supervised and unsupervised learning algorithms. And as I was saying, you can build an entire AI machine learning pipeline based on our system, as I'll show you later. We run on CPUs as well as GPUs, and we could easily extend this to FPGAs as well, although we haven't found that much of traction in the customer space for FPGAs yet. So now let me double click a little bit and tell you about knowledge graphs in a little more detail in some of the verticals that we are engaged in. So we're engaged in three verticals, pharma, financial services, and information security. And what I want to show you in the next few slides is basically what these knowledge graphs look like or what these graphs look like in these areas, and then the kind of graph AI, graph analytics that's required in these domains. So first up is health and life sciences. So here, as I was telling you earlier, people want to build these big medical knowledge graphs. And the nice thing about graphs is that they can represent entities of many different types. So graphs have heterogeneous nodes, heterogeneous edges. And this sort of cartoon graph over here shows you nodes for drugs, a node for a molecule, a node for a target protein, a node for a side effect. So all of these are entities that have different types. They can have different kinds of properties, but they can all live together in the same graph, so to speak. And then, of course, the edges also have type. So you can say a drug has a certain structure, which is this molecule. A drug has a certain side effect. It's associated with certain proteins, and so on. Another nice thing about graphs is that they can represent information at many different levels of abstraction. So for example, a given chemical molecule has a given chemical structure that the chemists know, and you can represent that chemical structure itself as a graph. And so that chemical structure of this molecule can live in the same data space, so to speak, as but drug and proteins, drug side effects, and so on. So data at many different levels of abstraction can all live together in the same data space if you use graphs. So what are the use cases that we are seeing? Well, I was telling you about a medical, a pharma company that we're working with that's growing drug hypothesis and discovery. Another area that we're very actively engaged in is in precision medicine, and I'll tell you a little more detail about that in the next few slides. Another place where graphs are used extensively is in financial services or fintech. So the idea here is to take financial transaction data and then represent that as a big heterogeneous graph, and then basically do analytics, AI, mining, and so on on these graphs in order to do things like fraud detection, identity theft, customer three sixty, and so on. So if you look at this, small graph over here, it's showing you a client, and then the client has an occupation. The client has purchased some services, client owns an account, the account has been invested in. And and, again, the key thing to take away is these are big heterogeneous graphs where the nodes have different types, each type has different sets of properties, and then the edges themselves have different types as well. And that is the kind of data that can be represented very conveniently using a graph. And then what we need to do in this particular area is to feed these graphs into traditional machine learning models in order to do things like fraud detection and so on. The final kind of knowledge graphs I'll show you are from the information security area. So I was telling you earlier that, we worked with a multinational company on real time intrusion detection and networks. So here is, interaction graph that was actually built by our system for this company over a period of time. And what you notice again is that there are different kinds of nodes. So, for example, there are websites. There are nodes for processes, nodes for files, for ports, for users, and so on. And if, for example, you read a certain file or you write to a certain file or you fork a process and so on, all of that activity can be represented very nicely using a graph of this sort. And then the intrusion detection problem that we solved for them was looking for certain forbidden patterns within this graph. I don't have the time to tell you about the particular pattern that we detected, but the highlighted edges in this graph were a forbidden pattern and that was highlighted and then reported to the operator. So that is the data. All of this data in these domains are being represented using graphs or knowledge graphs. Now what we need to do in order to do the analytics is you can have supervised learning, unsupervised learning. So we support unsupervised methods like Louvain clustering, page rank, betweenness centrality, which is very important in security applications, as well as supervised learning algorithms like GNNs and GTNs. And, again, you need high performance scale out processing because the datasets in these worlds are very, very large. So you could have hundreds of billions of nodes in these graphs with hundreds of billions of edges, and then you need to do all of this processing very fast. And so I'm making the same argument that I started with, earlier in my talk, except that now I made it a little more concrete by giving you a bunch of use cases and, showing you what is done with that data. So here is a slide that goes a little more into this pharma use case that I was, showing you. And the takeaway from this particular use case is the need for an integrated system with querying, analytics, AI, all built into the same workflow. So let's take a look at what this is. In this particular company, they had a big medical knowledge graph, and then they gave us a query, which I've shown in the bottom right. The query says extract oral drugs and associated targets for heart failure treatment, return the top ten chemical compounds most similar to a given compound, such as benzene in this case, and return their targets. So what is done is to take that English language specification, and then that gets translated into a small graph that I've shown at the left in my diagram. So you have this big graph, which is the medical knowledge graph. That's called the host graph. This is called the query graph. And then one of the key things that you need to do to process this query is to find all instances of this query graph in this big medical knowledge graph. So each instance corresponds to a hit. Let's call it that. And so you basically return the list of all the hits that you got for this particular query. Then the next thing you need to do is to take all of these hits and then find their similarity. You need to give a similarity score to each one of these hits using a particular similarity metric that's called Tanimoto similarity. Now this is something that's implemented by what they call a chemical cartridge. So these are just routines, libraries that they use. We're not going to reimplement them, but basically, this shows you that when you build this kind, what you need to be able to do is to do already integration of these kinds of third party packages into your overall workflow. Now since you have a bunch of hits and you can compute their similarity scores in parallel, one of the nice things that we can do in our system is that we can compute the similarity scores themselves in parallel using our engine. And so this chart at the top shows you scaling for computing tiny motor similarity on a shared memory machine. And as you can see, you get very good scaling as you increase the number of threads. Once you've got the scores, the similarity scores for all of the hits, you sort them, and then you return the top ten. And so what you see over here is a relatively complicated workflow. We work with far more complicated workflows. But even in this example, you can see the need for querying. You see the need for integration with third party packages, such as these chemical cartridges like RDKit. And then you see the need for doing ranking and other kinds of algorithms. So let me end by telling you about a few use cases. So this is GraphAI in the real world. This is some work that we're doing with one of the biggest, fintech companies in the world. So they want to handle massive datasets with billions of nodes and edges. And they gave us a bunch of problems, and I'll show you results for two of them. One of the things they wanted was a page rank computation, and then the other is a unsupervised learning algorithm called Louvain clustering, which is basically a community detection algorithm that builds hierarchical communities, and that's very popular now in the machine learning world. And so the chart below shows you some numbers, for two different graphs, a small graph with a hundred million nodes and a bigger graph with twenty billion nodes. And, we're also showing you the number of machines we used and then the time to compute page rank and Louvain. And these are substantially faster than what they were able to do on the system that they had been using previously before they started working with us. Here's another application. Lots of interest in this area. So molecular property predictions. So here is the way to think about the problem. I have a bunch of molecules whose properties I know, And these properties could be at many different levels of attraction, so they could be quantum properties, physical, chemistry properties, or even physiology properties, like whether this particular compound is toxic to human beings or not. So whatever these properties are, the way to think about the training data in this case is that you have a collection of molecules, and each molecule is labeled with a property. And then the inference task is given a new molecule whose properties you don't quite know, well, can you predict what the properties of this molecule are given all of the training data for the molecules that you do know? So this is a very important problem. Lots of interest in this area. There's a benchmark competition that's called TDC benchmark competition that lots of people are entering now. And, recently, some of our guys came up with a innovative way of using graphs in this particular area. So given a bunch of molecules and their labels, what they actually do is to build what they call a molecular similarity graph. So in the molecular similarity graph, each node corresponds to one of the molecules, and then there is an edge between them if they are similar in their properties according to some RDKit measure. So you can think about this as representing similarity of the different molecules that you have in your training data. And the node features come from the Katana Life Science module. So what we do with this labeled graph is that we use a g n n in order to create vector space representations for each of the nodes. So we embed each of the nodes in some high dimensional space. And then given a new molecule, we basically use these learned embeddings in order to do the prediction . And we were able to improve the leaderboard by about six percent, in with just about two weeks of effort. So what you can see is what we are doing is inventing new algorithms, new ways of thinking about problems, AI machine learning problems using graphs. But then we also have this very nice platform that we can quickly implement these new algorithms on and then see how well they do. Here's another application from the precision medicine area. Lots of VC interest in this area. Lots of companies working in this area. So here is the precision medicine problem at a very simple level. Right now, if you have a disease, you go to a doctor, chances are you will get pretty much the same treatment that he would have given to some other patient who has the same disease that you have. What we would like to do, obviously, is to be more precise about the treatment that we give you by exploiting things like your medical history, your genetic profile, everything that's known about your family, your financial circumstances, your environment, and so on. And so what we need to do is essentially get a patient three sixty information, so to speak, about each patient and then use that information in order to direct the treatment that we're giving you for a particular disease. So we are working with one of the big precision medicine companies in this space, and here is how they want to do it. They want to take all of the data about a patient and represent that data as a graph. So for in your training set, you have a bunch of patients, and so you've abstracted each patient out into a graph. And then for each patient in your training set, you know whether the treatment has been successful or not. And so you can label the graphs in your training set with that particular label. And now the problem is the following. If a new patient comes in, what we do is we build a graph corresponding to the data for that person, and then we infer the label for this new graph based on everything that we know from the training set. So this is basically what's called a graph classification problem. And, again, we're using state of the art techniques in graph transformer networks and graph neural networks in order to solve this particular problem using AI and machine learning techniques. This is, some recent work on graph transformer networks. So this is something that we developed for the precision medicine use case. And, I'm not going to go through the details of the running times and accuracy, but I want to make two points using this slide. The first is that we were able to improve the running time over previous algorithms for GTNs. And the way we do it was by inventing a very new algorithm that uses sampling without losing accuracy. So developing new algorithms for these graph machine learning problems is one of the things that we do at Katana. And then again, as I've been emphasizing, we have this this platform on which we can quickly implement these algorithms in parallel and see how well they do. This is my last slide. I want to conclude by telling you about something that you all know, and that is that knowledge graphs and AI, at least in our opinion, are the next base big thing. So this whole area of analytics started out with what we can call descriptive analytics. So this was the old style analytics where basically you just said, what happened? So you said, I had, pain in my chest, and there was a shooting pain down my left arm. That's what happened. Diagnostic analytics is the next more sophisticated kind of analytics. So here, what we want to do is to also say why something happened. So it's not just an account of what happened, but also you probably had all of those problems that you described because you were having a heart attack. So that is diagnostic analytics. The next more sophisticated analytics is prescriptive analytics. Given that these things have happened and given that this is why they probably happen, here are all the things you need to do in the future in order to mitigate the bad effects of whatever happened. So here you obviously need some predictive models in order to be able to say what needs to be done in the future. And then the most sophisticated thing is what we call predictive analytics. And in predictive analytics, you're not really building models based on rules or something that people give you, but you build the models automatically using all of the big data that you have and all of these wonderful machine learning and AI techniques that are available to us. So that is where there is going to be a lot of action, predictive analytics. There are many kinds of predictive analytics on different kinds of data, but we believe graphs are going to be a very big part of predictive analytics. And that is the space that katana graph plays in. With that, I'll conclude. Thank you for your attention.",
    "transcript_length": 26749,
    "speaker": "Keshav Pingali",
    "tags": [
      "cusp",
      "galois graph computing engine",
      "gluon communication engine",
      "gql",
      "graph neural networks",
      "graph transformer networks",
      "katana graph",
      "openCypher",
      "rdkit"
    ]
  },
  {
    "title": "Knowledge Graphs: Moving Beyond RDF",
    "description": "The Semantic Web is twenty years old, and like many twenty-year-old technologies, many of the original concepts that drove the technology have become increasingly outdated even as the need for a compelling graph technology story has only become greater.\n\nSeveral technologies have arisen in the last few years, including JSON, machine learning systems, GraphQL, Lambdas, and others.\n\nWhat's more, the first generation semantic web has struggled to assert itself while dealing with problems such as the failure of adoption of complex ontologies, data science workflows that have largely bypassed RDF as a mechanism for expressing rich data structures, slow adoption of even foundational technologies such as Property Graphs, RDF-Star, SHACL, or JSON-LD.\n\nThe focus of this talk is to explore the emergence of second-generation knowledge graphs, utilizing GraphQL as a means to abstract out data models and construct both set and hierarchical structures in a manner that makes sense to the data science, machine-learning, and web communities.\n\nAdditionally, such tools make mutational processing independent of platform, and even, utilizing such abstractions, make it possible to create truly functional and dynamic properties in the space.\n\nThis author hopes to explore how GraphQL and related paradigms (DIDs, machine learning models, web component frameworks, distributed ledgers and hyperstatial systems) all will impact the future of the knowledge graph over the course of the next decade and beyond.",
    "category": "Knowledge Graphs",
    "transcript": "Welcome to Knowledge Graphs, moving beyond archeo- RDF. My name is Kurt Cagle, and I wanna thank you for joining us this afternoon. A bit of a timeline, and note that these dates are only approximate. RDF, the resource description framework, was created in two thousand by sir Tim Berners Lee and others, others, pursuing an idea that emerged as he was developing HTML. With that language, he'd observed that hyperlinks, which connected a phrase or image in one document with a document elsewhere on the web, could be generalized to the idea of one concept or resource having a relationship with another concept in a conceptual space. This web of ideas, which you would christen the Semantic Web shortly thereafter, would end up driving the graph revolution of the twenty ten CVR. Originally, he used XML as the default language for this web, But over time, he realized that XML wasn't ideal for the task, even as he and others worked at the logical formalism language called the Web Ontology Language Language or OWL. By two thousand seven, SPARQL, an RDF query language, was produced to to querying sets of assertions using a new language called the Chirce RDF language, which hurdle, formally released in two thousand fourteen. Sparkle one point one was released about the same time, and it's worth understanding that Turtle and Sparkle have only just gotten started. This is very important as it effectively marked the end of the RKO RDF era. Turtle tends to get a lot of short shrift first from ontologists who grew up with OWL, RDF XML, and three notation and protege. And from those newer graph technologies who felt RDF XML and OWL were too complex or too dissimilar from the way that they were normally working with languages like JavaScript and Python. However, turtle as a language, has a great deal to recommend it. It is a much tercer language than RDF XML. Certainly, it's that's tercer than, m three. And arguably, it's JSON when namespaces are taken into account. Please see JSON LD. While OWL was built on XML, RDF XML can be very verbose and far from intuitive. Turtle is actually a surprisingly good language for expressing triples. It makes a good language additionally for prototyping or modeling objects. What's more, Turtle and SPARQL are largely complimentary because they essentially emerged in tandem. As Sparkle continues to evolve, so too does the expressiveness of Turtle. Indeed, I contend that part of the reason the turtle isn't used more widely is because it is a very young language, younger than JSON, and developed to express a very different need. Indeed, the primary limitations of Turtle as a language are simple. Turtle isn't JSON. We'll come back to this. It's worth dividing up the evolution of RDF into three distinct areas or eras. The first, the Archaeo RDF era was largely dominated by OWL, utilized n three for ingestion, a not very common format, and was built primarily for inferencing, a glorified word that means that it is possible to express new assertions by using logical operations on the existing graph or triples, then adding these back in as virtual triples. Serialization was primarily accomplished via fairly ugly XML representation. It appealed to academics, but was not very attractive to the woke up programmers out there. The Meso RDF era, on the other hand, began with SPARQL one point o, released in two thousand and seven, but took both the emergence of Turtle around twenty twelve and the beefing up of SPARQL in twenty thirteen to really take off. This was RDF's nuclear winter, where interest in the semantic web as an idea almost died even as significant new developments, the emergence of turtles, not mammals, were taking place. SPARQL one point one brought with it some significantly needed improvements that was still not quite sufficient. The SPARQL update facility was also introduced, replacing virtualized triples with formerly persistent ones, and the notion that you could query in a federated manner was introduced, although, again, not fully fleshed out at around the same time. Finally, we began to see the first upticks in the use of JSON as representations of troubles, especially in corporate environments with JSON LD linked data. It could be argued that JSON LD is possibly, probably, in fact, an evolutionary dead end, but it is signal signaled that the two communities were talking with one another. I believe we are in the beginning of the NeoRDF era. There are a whole host of new developments that last few years are beginning to coalesce, and that will likely be formalized within the next couple of years. This is not necessarily all where RDF is going, but it is what is most visible right now. I wish to cover each of these individually. The RDF role tends to spit out big, scary words. One of these is reification. A reification is a statement that describes another statement. For instance, let's say that you have a statement that specifies a relationship, a has route to relationship, between two airports in Seattle and San Francisco respectively. This description describes a route, and that route can have multiple properties such as distance in which airlines fly that particular route. Labeled property graphs, otherwise known as our LPGs, can can describe the first relationship but sputters on the second. With RDF star, first proposed in twenty seventeen, and Sparkle Star, a more compact notation for reification was proposed for both Turtle and Sparkle. This new notation makes a great number of operations much easier and goes a long way in making RDF graphs compatible with local property graphs. A second area where some exciting development is happening is in the increasing sophistication of property path languages, something familiar to users of both OpenCypher and Gremlin. With new ways of describing property graphs as first class objects and providing ways of specifying wildcards and variable bounds. Such a language could radically empower SPARQL to combine both inferencing and graph analytics operations into a single unified language. While there are several potential candidates for such a language, Shackle property paths look to be the most promising areas yet. This will be covered more when I talk about Shackle. Shackle, the shape constraint language, like most RDF technologies, was formulated in reaction to the emergence of turtle as the new face of RDF. Going through several iterations, it was finally approved in twenty seventeen. Shackle, ironically, is changing the dialogue about data modeling with RD web RDF away from the OWL specific sets of rules and formalisms towards a format that is more familiar to data modelers, application developers, and data scientists. In many respects, it is providing key in the integration of r d RDF with GraphQL as well. There is an active community now looking towards a SPARQL two point o implementation, building off of the previous implementation's weaknesses. Already, there are limitations pushing these boundaries by incorporating more sophisticated property paths, establishing functions, arrays, sets, and dictionaries as first class objects, fixing significant problems with lists and how they are managed in SPARQL, utilization of JavaScript like capabilities such as math map reduce type operators, lambdas or arrow functions, and template literals, and utilizing shackle to make JSON construction much easier. GraphQL is a good idea. While it is not part of the W three C stack that was originally developed by Facebook, It should be because it works very well with what RDF was intended to be. GraphQL has limitations. Reification is almost impossible to manage. The JSON data model is just not as robust at handling things RDF does well, including inheritance. And its hierarchical structure is at odds with the potential secret nature of graphs. But it is useful language for making RDF stores look like JSON stores from the outside world. Shackle is playing a big part of that, and almost every GraphQL bridge currently in production uses Shackle on the back end. With some changes to sparkle to make JSON, constructible directly from graphs, this could mark the merger point of these two query language technologies. Region. The twenty thirteen specification is a good start, but as implementations have been built, holes have become obvious. This will happen soon, probably within the next two years. Finally, it's worth recognizing that machine learning and graph technologies are in a collision course, and it is likely the next innovations will be the integration of these two, for everything from discoverability to better classifiers and more robust machine learning models. Thank you for listening. Please feel free to drop me an email or contact me on LinkedIn if you have any questions. Thank you.",
    "transcript_length": 8805,
    "speaker": "Kurt Cagle",
    "tags": [
      "graphql",
      "gremlin",
      "json",
      "json ld",
      "opencypher",
      "owl",
      "rdf",
      "shackle",
      "sparql",
      "turtle"
    ]
  },
  {
    "title": "Knowledge Mesh: From Data Silos to Data Fabric at Global 2000 Enterprises",
    "description": "A common go-to data strategy adopted in large, data-intensive enterprises, is based on creation of centralized, monolithic data platforms, such as data warehouses or data lakes. Such platforms are well-known for their many limitations and lack of long-term sustainability: growing stale data silos, loss of ownership and domain context of data assets, and organizational bottlenecks around highly specialized data engineering teams. The ultimate outcome is the increase of the cost of effective data utilization within the enterprise.\n\nData fabric is a design concept combining multiple modern data management technologies aimed towards supporting \u201cfrictionless access and sharing of data in a distributed network environment\u201d (Gartner).\n\nKnowledge Mesh, a recent product delivered by BlackSwan Technologies, is an implementation of the data fabric architecture, consisting of domain-driven metadata and (meta)data management and integration tools aimed to maximize the effectiveness of data utilization in knowledge graph applications in different domains.\n\nIn this presentation we will:\n\n- Introduce BlackSwan Technologies with its core technology vision and offering\n- Describe the challenges involved in 360-degree entity view use-cases at Global 2000 enterprises\n- Explain how enterprise knowledge graphs (EKG) help addressing these challenges\n- Introduce the concept of data fabric via its implementation as Knowledge Mesh, and argue for its capabilities of scaling and automating EKG development in multiple domains",
    "category": "Knowledge Graphs",
    "transcript": "Hi, everyone, and I'm very happy to be here today. Hope you are enjoying so far from the different sessions. This session will have two parts jointly given by Szymon Klarman and myself. My name is David Amzalag, and I'm the chief product officer of Black Swan Technologies. Black Swan is a growing startup that specialize in data. We have presences in six different countries, more than two hundred and fifty employees, and different type of customers. Why different types? Because every company, enterprises, for instance, is having a very large number of data sources. Sometimes hundreds of different source data sources and sometimes thousands of different data sources that the organization is relying on and using him for all type of usages. Obviously, regardless the industry, those enterprises are belong to, health care, finance, insurances, retail, etcetera. What is common with all of them is the real and big concern they have on how to effectively manage all of those data sources, keeping them synchronized, accessing them in real time, keeping the infrastructure not so expensive, and very important, how to monetize valuable insight from all of them. Sounds like a big problem. No? Let's make this problem even more realistic. Almost all data sources are feed by different applications. Salesforce Salesforce and other CRMs, NetSuite, AWS, Facebook Ministry of Interior, etcetera. Those probably are geographically spread in addition. Now what we are doing in many of the cases is to build a data lake, one centralized data lake from all data sources that this means that the data will be stored at the same place. No doubt that this might help operationally. However, this is not so cheap solution, and data is not kept synchronized in many of the cases. Moreover, once the data is kept in a single place, different personas of the organization might have different usages of the data. Different queries might be asked, and data from different sources must be consistent and aligned. Actually, a very big problem. Black Swan created technology so no data leak is needed, and access to data source is done directly to where they are. Geography here is not so important. Interface is being done throughout the no code type of user experience, So people from all type of organizations, meaning accounting, hospital data scientists, investors in bank, etcetera, will be able to create applications and queries by themselves actually very easily and really to generate insight from the data with without being, let's say, very good software people. The insight is being generated continuously throughout a large infrastructure of knowledge graphs and its generalizations. Thank you very much for your time and handing over to Shiman to speak about our interpretation of knowledge graph. We call it the knowledge mesh. Hi. My name is Shimon Klarman, and I'm a knowledge architect at Dexcom Technologies. As David already explained, the main challenge we are trying to address is how to facilitate utilization of data within large data intensive organizations who commonly own enormous amounts of data assets, and yet they struggle to get the actual insights and knowledge out of those assets. So a prototypical, starting point for those kind of scenarios could be a use case we could call a three hundred sixty degree entity view. So something, that in practice would be known as single client view, know your customer, compliance, a scenario where we want to gather information about a certain, real world entity or collection of such entities, where this information is actually spread across multiple data management systems. It's well known that in this kind of scenarios, organization struggle with a number of, data challenges such as data discoverability issues. Sources are often based in multiple even multi cloud environments. Because of that, there's lack of uniform accessibility rules. There's a heterogeneity of database models and schemas, lack of semantic interoperability across those schemas in which data is expressed. There's a number of data quality issues such as data incompleteness, granularity, or data normalization issues. Now one, natural way of handling this kind of three hundred sixty degree entity view use cases is by employing enterprise knowledge graphs. I guess within this audience, this is, this is actually a well known solution. We know that we would like to get the clean, the duplicated, resolved, reconciled, up to date view of all the entity data. This kind of entity and relationship centric, view is arguably much closer to the real world or at least the way we think about the real world. It presents all the information integrated as it's a very powerful data integration framework under a single consistent data model. Further, this being a a graph, also a semantic graph, which we can explain a bit later, means that we can apply a range of analytical techniques on top of such structure to get insights, things like semantic inference or machine learning on graphs. So altogether, if we add it up, offers us possibility to get a reliable knowledge, something we can trust, something that gives us a lot of contextual information out of just a plain raw data source. Of course, we know where we'd like to get as the ideal, end of the journey, but how to get there is a challenge in its own right. And knowledge graph construction process, according to some textbook recipes, could be, under some simplification presented as this kind of, ETL pipeline going from the sources to the final knowledge graph by the process of extracting, transforming, and loading the data, most likely to a graph database. The extraction process would most commonly be achieved using type, sort of extractors depending on the specific data sources. So for instance, SQL extractors for structured databases, scrapers for, HTML websites, NLP algorithms for processing text documents. The data extracted from extractor using extractors would then be mapped into a target knowledge graph schema that we want to use in the application, And extraction results would be further reconciled, meaning, quite literally would basically connect the dots, resolve we would resolve entities ensuring that there are no duplicates, which in the same way would resolve relationships between those entities and normalize the attributes. At the end of the day, we would have a a nice, clean, consistently duplicated knowledge graph. This being an ETL pipeline, however, is actually where big problem is. Yes. We are getting a nice, clean knowledge graph at the end, but, actually, it's a knowledge graph that, again, is very hard to maintain and evolve over time, and it doesn't exactly solve the data siloing problem that we wanted to solve to start with. Instead, it just creates yet another data silo. Now why is that? Well, if we compare this ETL approach to some other known ETL based, centralized, data integration platforms, such as based on the concepts of data lakes and data warehouses, we can see that, there's a lot of commonalities. Okay. The final results might might look a bit different, but it's this centralization of this whole architecture And, the the the fact that the process is based on ETL, one directional ETL process, is what causes the issues. And the issues are basically, that we generate a lot of ETL and ELT code, which, actually encapsulates a lot of meaning, with that we attach to this data, a lot of semantics. And this semantic is only accessible and manageable for developers who build this code. And the result of it is that this meaning of data becomes essentially lost, and there is no real end to that end data ownership, that would, allow different, domain focus teams to be able to manage and, govern different data assets over longer periods of time. Now what's an alternative? Following years of experience with single ad challenges, we arrived in architecture, which we call the knowledge mesh, which essentially tries to challenge all the most important pain points of those, previous centralized, data integration architectures. Instead of centralization, knowledge mesh assumes a decentralized data world. Instead of moving data to a single central storage place, we only virtualize the data at the query time, at the, analytic insight time or put it generally at the right time when the data when the integrated data is needed. Instead of developing bespoke ETL pipelines, we provide generic capabilities driven by metadata, which can be managed by nontechnical subject matter experts. And instead of focusing on specific applications of particular data assets, we focus on collecting and maintaining general knowledge pertaining to broader business domains. So overall knowledge mesh is this domain driven architecture of metadata and data management and integration tools, which are aimed to maximize the effectiveness of data utilization in knowledge graph applications in different domains. Let me now walk you through very briefly through, different layers of this architecture, to highlight the most important aspects. So firstly, there's a uniform data access and virtualization layer. Essentially, data fetchers are these simple, services that allow to extract the data at query time. They are not basic they are not just data extractors as in this bay previous ETL, approach to to knowledge we have constructors. They are little database interfaces, essentially, which can take any data source and expose it using a uniform data description language, basically, a uniform schema of the source, and offer a generic query language, again, which allows you to formulate queries that that are needed by the application to satisfy certain, data requirements. We use, GraphQL as this interface, although, obviously, this could be achieved, in number of other ways. The important part is that the source, the the data fetcher that exposes the source is agnostic about the application. It's the mapping, explicit declarative mapping layer that connects the, the schema, exposed by the fetcher to the application , which provides this essential glue between the sources and the application layer. Now, the one of the most important, if not the most important, aspect of this architecture is the rich metadata layer. What is this metadata? It's basically the data about, all the data assets that are involved, in the, in the application. But even more than that, it's about all the infrastructural components that are necessary to efficiently conduct whole data integration and data interpretation process end to end. So it's the information about the data sources, about the fetchers that serve them, about the domains where those sources are assigned to, about the mappings, which map, the data as served by the fetchers to specific domain vocabularies, which are also associated with those domains. All these components form essentially a connected metadata graph, which provides a lot of powerful capabilities to the system. It allows to catalog different kind of resources to increase discoverability. It opens up a whole range of, low code, capabilities, where we can actually, instead of writing the spoke, code support certain functional functionalities, we can offer, generic solutions which are consistently driven by this metadata to achieve certain goals and support certain, use cases. All the metadata is obviously, expressed in standard formats, standard metamodels, which further increases interoperability between humans and machine agents. All the metadata, as mentioned before, is, is, organized in a domain oriented fashion. Now what does it mean? What is a domain read? A domain is a predefined collection of assets that make sense, and are particularly useful in the context of some range of applications which would jointly, collect in a in under an umbrella of some, business domain. This kind of business domain could be a compliance or could be cyber or perhaps could be health care. In any case, a domain consists of a domain vocabulary. So the collection of terms which we use to describe all important, data relevant data within this domain. It contains of a collection of sources which are catalogued for browsing, for discovery. It also contains mappings that, again, generically map the data from this domain to the domain vocabulary and possibly also a bit, a collection of of, domain specific algorithms or basically some intelligence which is already built in into that domain. For instance, algorithms that can that allow you to perform entity resolution on entities, within specifically within this domain. As a result of of of offering those kind of of building those kind of business domains, the process of constructing a specific application and and, developing a a knowledge graph for this kind of application can be, to a large extent, automated. It's, so once the application is being created in specific domains, we can automatically leverage the metadata, the whole understanding of which sources can be used, how they map to this domain vocabulary, and how they should, how the data from these sources should be essentially processed so that the knowledge graph powering, fueling this application, this domain can be, constructed relatively, seamlessly with a reasonably low effort from the application developer. On the way, we collect we are able to collect another layer of metadata. This is a metadata about where each data item comes from. What do we know about the source it came from? With what confidence and with what method, this specific piece of, data has been extracted, and how it was processed later on to, for instance, resolve, one entity against against the other. All these additional, layers of metadata are something that essentially allows us to to offer higher quality guarantees, to the application consumers. So this is essentially the process of extracting knowledge from raw data where the knowledge basically comes into the picture at the moment where you can start building trust and start building confidence that that the raw data was, was was was sourced and was processed in a way that ensures high quality information at the end of the process. Now when we add these two different layers of metadata, which I mentioned, in the previous slides together, the metadata and the data collected from the data sources and the metadata about the whole about the whole infrastructure, data integration infrastructure, we finally obtain this, this complex twofold knowledge graph or as it's sometimes called, a full data fabric. It is exactly the structure that is necessary for efficiently utilizing all available information and automating all the data processes, that we want to, that we need to carry out in order to gain insights and get knowledge out of the originally disconnected data sources. So to summarize, knowledge mesh is this domain centric data integration and virtualization architecture consisting of a range of advanced data management, data processing, data cataloging tools working together, to, to basically help automate and help manage all the steps required to build efficiently build up to date knowledge graphs from disconnected distributed data sources. It, thus maximizes the effectiveness of data utilization in knowledge graph applications. And, for those and perhaps familiar with so called FAIR principles, principles basically, adopted increasingly adopted by a large data intensive enterprise. Knowledge mesh is really nothing else but metadata first architecture that is intended and it allows to make the data findable, accessible, interpretable, and reusable. And with this, I'd like to finish and together with David, take any questions you might have. Thank you for your attention.",
    "transcript_length": 15762,
    "speaker": "David Amzallag",
    "tags": [
      "ETL pipeline",
      "GraphQL",
      "NLP algorithms",
      "data lake",
      "knowledge graphs",
      "knowledge mesh",
      "metadata"
    ]
  },
  {
    "title": "Knowledge Mesh: From Data Silos to Data Fabric at Global 2000 Enterprises",
    "description": "A common go-to data strategy adopted in large, data-intensive enterprises, is based on creation of centralized, monolithic data platforms, such as data warehouses or data lakes. Such platforms are well-known for their many limitations and lack of long-term sustainability: growing stale data silos, loss of ownership and domain context of data assets, and organizational bottlenecks around highly specialized data engineering teams. The ultimate outcome is the increase of the cost of effective data utilization within the enterprise.\n\nData fabric is a design concept combining multiple modern data management technologies aimed towards supporting \u201cfrictionless access and sharing of data in a distributed network environment\u201d (Gartner).\n\nKnowledge Mesh, a recent product delivered by BlackSwan Technologies, is an implementation of the data fabric architecture, consisting of domain-driven metadata and (meta)data management and integration tools aimed to maximize the effectiveness of data utilization in knowledge graph applications in different domains.\n\nIn this presentation we will:\n\n- Introduce BlackSwan Technologies with its core technology vision and offering\n- Describe the challenges involved in 360-degree entity view use-cases at Global 2000 enterprises\n- Explain how enterprise knowledge graphs (EKG) help addressing these challenges\n- Introduce the concept of data fabric via its implementation as Knowledge Mesh, and argue for its capabilities of scaling and automating EKG development in multiple domains",
    "category": "Knowledge Graphs",
    "transcript": "Hi, everyone, and I'm very happy to be here today. Hope you are enjoying so far from the different sessions. This session will have two parts jointly given by Szymon Klarman and myself. My name is David Amzalag, and I'm the chief product officer of Black Swan Technologies. Black Swan is a growing startup that specialize in data. We have presences in six different countries, more than two hundred and fifty employees, and different type of customers. Why different types? Because every company, enterprises, for instance, is having a very large number of data sources. Sometimes hundreds of different source data sources and sometimes thousands of different data sources that the organization is relying on and using him for all type of usages. Obviously, regardless the industry, those enterprises are belong to, health care, finance, insurances, retail, etcetera. What is common with all of them is the real and big concern they have on how to effectively manage all of those data sources, keeping them synchronized, accessing them in real time, keeping the infrastructure not so expensive, and very important, how to monetize valuable insight from all of them. Sounds like a big problem. No? Let's make this problem even more realistic. Almost all data sources are feed by different applications. Salesforce Salesforce and other CRMs, NetSuite, AWS, Facebook Ministry of Interior, etcetera. Those probably are geographically spread in addition. Now what we are doing in many of the cases is to build a data lake, one centralized data lake from all data sources that this means that the data will be stored at the same place. No doubt that this might help operationally. However, this is not so cheap solution, and data is not kept synchronized in many of the cases. Moreover, once the data is kept in a single place, different personas of the organization might have different usages of the data. Different queries might be asked, and data from different sources must be consistent and aligned. Actually, a very big problem. Black Swan created technology so no data leak is needed, and access to data source is done directly to where they are. Geography here is not so important. Interface is being done throughout the no code type of user experience, So people from all type of organizations, meaning accounting, hospital data scientists, investors in bank, etcetera, will be able to create applications and queries by themselves actually very easily and really to generate insight from the data with without being, let's say, very good software people. The insight is being generated continuously throughout a large infrastructure of knowledge graphs and its generalizations. Thank you very much for your time and handing over to Shiman to speak about our interpretation of knowledge graph. We call it the knowledge mesh. Hi. My name is Shimon Klarman, and I'm a knowledge architect at Dexcom Technologies. As David already explained, the main challenge we are trying to address is how to facilitate utilization of data within large data intensive organizations who commonly own enormous amounts of data assets, and yet they struggle to get the actual insights and knowledge out of those assets. So a prototypical, starting point for those kind of scenarios could be a use case we could call a three hundred sixty degree entity view. So something, that in practice would be known as single client view, know your customer, compliance, a scenario where we want to gather information about a certain, real world entity or collection of such entities, where this information is actually spread across multiple data management systems. It's well known that in this kind of scenarios, organization struggle with a number of, data challenges such as data discoverability issues. Sources are often based in multiple even multi cloud environments. Because of that, there's lack of uniform accessibility rules. There's a heterogeneity of database models and schemas, lack of semantic interoperability across those schemas in which data is expressed. There's a number of data quality issues such as data incompleteness, granularity, or data normalization issues. Now one, natural way of handling this kind of three hundred sixty degree entity view use cases is by employing enterprise knowledge graphs. I guess within this audience, this is, this is actually a well known solution. We know that we would like to get the clean, the duplicated, resolved, reconciled, up to date view of all the entity data. This kind of entity and relationship centric, view is arguably much closer to the real world or at least the way we think about the real world. It presents all the information integrated as it's a very powerful data integration framework under a single consistent data model. Further, this being a a graph, also a semantic graph, which we can explain a bit later, means that we can apply a range of analytical techniques on top of such structure to get insights, things like semantic inference or machine learning on graphs. So altogether, if we add it up, offers us possibility to get a reliable knowledge, something we can trust, something that gives us a lot of contextual information out of just a plain raw data source. Of course, we know where we'd like to get as the ideal, end of the journey, but how to get there is a challenge in its own right. And knowledge graph construction process, according to some textbook recipes, could be, under some simplification presented as this kind of, ETL pipeline going from the sources to the final knowledge graph by the process of extracting, transforming, and loading the data, most likely to a graph database. The extraction process would most commonly be achieved using type, sort of extractors depending on the specific data sources. So for instance, SQL extractors for structured databases, scrapers for, HTML websites, NLP algorithms for processing text documents. The data extracted from extractor using extractors would then be mapped into a target knowledge graph schema that we want to use in the application, And extraction results would be further reconciled, meaning, quite literally would basically connect the dots, resolve we would resolve entities ensuring that there are no duplicates, which in the same way would resolve relationships between those entities and normalize the attributes. At the end of the day, we would have a a nice, clean, consistently duplicated knowledge graph. This being an ETL pipeline, however, is actually where big problem is. Yes. We are getting a nice, clean knowledge graph at the end, but, actually, it's a knowledge graph that, again, is very hard to maintain and evolve over time, and it doesn't exactly solve the data siloing problem that we wanted to solve to start with. Instead, it just creates yet another data silo. Now why is that? Well, if we compare this ETL approach to some other known ETL based, centralized, data integration platforms, such as based on the concepts of data lakes and data warehouses, we can see that, there's a lot of commonalities. Okay. The final results might might look a bit different, but it's this centralization of this whole architecture And, the the the fact that the process is based on ETL, one directional ETL process, is what causes the issues. And the issues are basically, that we generate a lot of ETL and ELT code, which, actually encapsulates a lot of meaning, with that we attach to this data, a lot of semantics. And this semantic is only accessible and manageable for developers who build this code. And the result of it is that this meaning of data becomes essentially lost, and there is no real end to that end data ownership, that would, allow different, domain focus teams to be able to manage and, govern different data assets over longer periods of time. Now what's an alternative? Following years of experience with single ad challenges, we arrived in architecture, which we call the knowledge mesh, which essentially tries to challenge all the most important pain points of those, previous centralized, data integration architectures. Instead of centralization, knowledge mesh assumes a decentralized data world. Instead of moving data to a single central storage place, we only virtualize the data at the query time, at the, analytic insight time or put it generally at the right time when the data when the integrated data is needed. Instead of developing bespoke ETL pipelines, we provide generic capabilities driven by metadata, which can be managed by nontechnical subject matter experts. And instead of focusing on specific applications of particular data assets, we focus on collecting and maintaining general knowledge pertaining to broader business domains. So overall knowledge mesh is this domain driven architecture of metadata and data management and integration tools, which are aimed to maximize the effectiveness of data utilization in knowledge graph applications in different domains. Let me now walk you through very briefly through, different layers of this architecture, to highlight the most important aspects. So firstly, there's a uniform data access and virtualization layer. Essentially, data fetchers are these simple, services that allow to extract the data at query time. They are not basic they are not just data extractors as in this bay previous ETL, approach to to knowledge we have constructors. They are little database interfaces, essentially, which can take any data source and expose it using a uniform data description language, basically, a uniform schema of the source, and offer a generic query language, again, which allows you to formulate queries that that are needed by the application to satisfy certain, data requirements. We use, GraphQL as this interface, although, obviously, this could be achieved, in number of other ways. The important part is that the source, the the data fetcher that exposes the source is agnostic about the application. It's the mapping, explicit declarative mapping layer that connects the, the schema, exposed by the fetcher to the application , which provides this essential glue between the sources and the application layer. Now, the one of the most important, if not the most important, aspect of this architecture is the rich metadata layer. What is this metadata? It's basically the data about, all the data assets that are involved, in the, in the application. But even more than that, it's about all the infrastructural components that are necessary to efficiently conduct whole data integration and data interpretation process end to end. So it's the information about the data sources, about the fetchers that serve them, about the domains where those sources are assigned to, about the mappings, which map, the data as served by the fetchers to specific domain vocabularies, which are also associated with those domains. All these components form essentially a connected metadata graph, which provides a lot of powerful capabilities to the system. It allows to catalog different kind of resources to increase discoverability. It opens up a whole range of, low code, capabilities, where we can actually, instead of writing the spoke, code support certain functional functionalities, we can offer, generic solutions which are consistently driven by this metadata to achieve certain goals and support certain, use cases. All the metadata is obviously, expressed in standard formats, standard metamodels, which further increases interoperability between humans and machine agents. All the metadata, as mentioned before, is, is, organized in a domain oriented fashion. Now what does it mean? What is a domain read? A domain is a predefined collection of assets that make sense, and are particularly useful in the context of some range of applications which would jointly, collect in a in under an umbrella of some, business domain. This kind of business domain could be a compliance or could be cyber or perhaps could be health care. In any case, a domain consists of a domain vocabulary. So the collection of terms which we use to describe all important, data relevant data within this domain. It contains of a collection of sources which are catalogued for browsing, for discovery. It also contains mappings that, again, generically map the data from this domain to the domain vocabulary and possibly also a bit, a collection of of, domain specific algorithms or basically some intelligence which is already built in into that domain. For instance, algorithms that can that allow you to perform entity resolution on entities, within specifically within this domain. As a result of of of offering those kind of of building those kind of business domains, the process of constructing a specific application and and, developing a a knowledge graph for this kind of application can be, to a large extent, automated. It's, so once the application is being created in specific domains, we can automatically leverage the metadata, the whole understanding of which sources can be used, how they map to this domain vocabulary, and how they should, how the data from these sources should be essentially processed so that the knowledge graph powering, fueling this application, this domain can be, constructed relatively, seamlessly with a reasonably low effort from the application developer. On the way, we collect we are able to collect another layer of metadata. This is a metadata about where each data item comes from. What do we know about the source it came from? With what confidence and with what method, this specific piece of, data has been extracted, and how it was processed later on to, for instance, resolve, one entity against against the other. All these additional, layers of metadata are something that essentially allows us to to offer higher quality guarantees, to the application consumers. So this is essentially the process of extracting knowledge from raw data where the knowledge basically comes into the picture at the moment where you can start building trust and start building confidence that that the raw data was, was was was sourced and was processed in a way that ensures high quality information at the end of the process. Now when we add these two different layers of metadata, which I mentioned, in the previous slides together, the metadata and the data collected from the data sources and the metadata about the whole about the whole infrastructure, data integration infrastructure, we finally obtain this, this complex twofold knowledge graph or as it's sometimes called, a full data fabric. It is exactly the structure that is necessary for efficiently utilizing all available information and automating all the data processes, that we want to, that we need to carry out in order to gain insights and get knowledge out of the originally disconnected data sources. So to summarize, knowledge mesh is this domain centric data integration and virtualization architecture consisting of a range of advanced data management, data processing, data cataloging tools working together, to, to basically help automate and help manage all the steps required to build efficiently build up to date knowledge graphs from disconnected distributed data sources. It, thus maximizes the effectiveness of data utilization in knowledge graph applications. And, for those and perhaps familiar with so called FAIR principles, principles basically, adopted increasingly adopted by a large data intensive enterprise. Knowledge mesh is really nothing else but metadata first architecture that is intended and it allows to make the data findable, accessible, interpretable, and reusable. And with this, I'd like to finish and together with David, take any questions you might have. Thank you for your attention.",
    "transcript_length": 15762,
    "speaker": "Szymon Klarman",
    "tags": [
      "ETL pipeline",
      "GraphQL",
      "NLP algorithms",
      "data lake",
      "knowledge graphs",
      "knowledge mesh",
      "metadata"
    ]
  },
  {
    "title": "Legal Knowledge Graphs",
    "description": "Creating and negotiating legal documents and contracts usually requires legal expertise which can be prohibitively expensive for small businesses and unlawyered individuals.\n\nAccessing what\u2019s in a contract and knowing which contracts have been signed is complicated because of the traditional text/pdf structure of documents.\n\nLegislate is helping bring structure to documents thanks to knowledge graph technology. Legislate is also improving the visual representation of contracts with knowledge graphs to allow non-lawyers to better understand their rights and obligations as well as other implicit information which can be derived from contract data using reasoning.\n\nLegislate is used by small businesses, landlords, letting agents and individuals who can\u2019t justify the cost of using a lawyer to create and negotiate contracts. Small businesses use Legislate to create and manage employment documents and non-disclosure agreements. Property businesses use Legislate to create and manage tenancy related documents.\n\nLegislate uses knowledge graphs to model contracts and legal expertise as rules to empower non-lawyers to create lawyer-approved contracts by themselves. Using a knowledge graph to model contracts unlocks additional benefits for managing documents at any point in their lifecycle.\n\nBy knowing exactly what terms are in a contract and where they are, Legislate contracts are highly searchable, know how they connect with each other and can aggregate statistics which would otherwise need to be manually extracted and calculated in a legacy spreadsheet.\n\nLegislate\u2019s knowledge graph contract metrics give deeper insight into a portfolio of a landlord or a small business owner.\n\nFor example a property owner can see how much rent they are earning each year from all their properties as well as the distribution for their properties and corresponding rental prices. Employers can better understand their real time payroll across employees, interns and consultants.",
    "category": "Knowledge Graphs",
    "transcript": "Hello and welcome to our presentation on legal knowledge graphs. My name is Vaishali I'm a Knowledge Engineer at Legislate, and in this presentation, we will be discussing how knowledge graphs are used in the legal sector and how they can be used with documents in general. As we all know, knowledge graphs have a variety of applications from technology to retail because of their flexible nature. And we'll be exploring how and why knowledge graphs are useful for legal documents and explaining some of their use cases. So a bit about legislate. Now legislate is a contract management platform that makes dealing with contracts easier for all parties involved. We were founded in twenty twenty because of how long it takes to make contracts that often aren't very complicated and because of the pain in negotiating contracts. So negotiating contracts can often be difficult. It can also it can often take a lot of time and there's often a lot of back and forth that happens with this. It also usually requires, a lawyer to go through. So sorry. The documents the contracts need to go through a lawyer, which can often be very costly, But legislator removes the need for us to do this. We're a team of eleven based between Oxford and Seville, and we consist of a legal, technical, and sales team. Our aim is to make the end to end contracting process as simple simple as possible for anybody who decides to use the platform regardless of their background. One of the things that we've come across is that contracts often contain a lot of legalese and can be difficult to understand. So we're really trying to reduce the amount of legalese within our contracts to make them easy to understand for anybody who reads them. So we and we aim to make our contract as usable as we can by using knowledge graphs to, connect contracts together and make contracts easier to deal with at scale. So our current customers consist of small businesses and property managements, property managers or letting agents who deal with contracts on a regular basis, but can't necessarily justify spending huge amounts of money on a lawyer when a lot of their contracts are a lot of the same template, but tailored towards individual, properties, or individuals. So they are often exactly the same with just some key terms being changed. And paying money for this or paying a large amount of money for this can often seem, unnecessary. So what we're trying to do is make this as easy as possible with legislate. And so a bit about me, I'm a knowledge engineer at legislate. I am primarily responsible for dealing with knowledge graphs and data management surrounding knowledge graphs. So we're using knowledge graphs to organize our data in the best way possible so that we can manipulate and extract insights from our data. Knowledge graphs have given us the ability to navigate through our data in a way we never have before. And one of the key things that we've noticed is to ensure we set up our data in the best possible way so that it's easy to look at, easy to navigate through, and it makes it as easy as possible to get the most out of it. That's one of the main areas I work work in, and one of the main challenges I'd say I've I've been encountering with knowledge graphs, especially because it's a new sector, but I've really enjoyed the process of dealing with them. So why do documents need to be structured and linked? So as you all know, legal documents can be tedious and difficult to understand for both humans and machines. Generally, most people will skim through legal documents, struggle to understand the terminology. They may not even bother reading the document properly. And we're trying to use knowledge graphs to break down this information to make it easier to read for anybody who is involved. A lot of contract data is in unstructured format and is in unstructured formats, and converting this into a digital computer readable format can be a challenge. Businesses also can't always rely on solely computers to do this for them because of the nature of the documents, because of the accuracy and security of the documents as well. So for example, in, in a industry like healthcare, the nature of the information is often sensitive, and you may not want you may want some human supervision over this. And so using knowledge graphs to, automate this process, but also have human input, it really helps make this a more trustworthy process for anybody who decides to use it. And contracts are not natively machine readable, which makes them difficult to search and keep track of. So by using knowledge graphs, to organize this data, we can make them easier to follow. Contracts are difficult to understand and difficult to understand in an individual format. And if we have a hundred hundreds of thousands of contracts, that adds another layer of complexity to this, and that in itself is another challenge to manage. So if we can use knowledge graphs to better organize documents, we can allow everybody to get the most most out of their documents and not worry about what they're getting themselves into. So over here, we just have a a little diagram showing you what the problems with, documents can be. So if we have one contract, this is easy to, this is easy to process for a trained human. So, generally, somebody in the legal sector. If we have a few, this is manageable for a trained human, so somebody from the legal sector. But if we have hundreds of them, this is difficult for even a somebody from the legal sector. It's, having a knowledge graph to help make this process easier, makes it more manageable, and makes it easy for anybody to understand, not just necessarily a trained human. So why should you structure documents with knowledge graphs? So contracts contain highly connected information, and knowledge graphs can make them easier to process and search at scale. So contracts can contain connected fields. And as we all know, knowledge graphs are flexible and allow you to make links, between nodes. So using knowledge graphs, we can organize contracts in a digestible and easy to follow format and use this data in a variety of ways. So, essentially, knowledge graphs allow us to organize contracts and break them down into smaller chunks that anybody can understand and use it in a variety of ways. So over here, we have an example of a employment offer letter and an employment agreement. So when you hire somebody, you'd often be issuing them an employment offer letter and an and an employment agreement. But a lot of these fields will be exactly the same across both contracts or across both legal documents. The job title would be the same. The salary would be the same. The line manager would be the same. And instead of having to copy this over, we could essentially just link these two documents and copy the fields over by using knowledge graphs. So this is another diagram of how we can link documents. As I was saying, when you hire an employee, you will often issue an offer letter and a employment agreement. And instead of having two separate documents that aren't linked, we can use a graph, which is natively machine readable and understandable. It's also easy to search and query to link these documents. So let's say this was our employment agreement, and this was our employment agreement, and this was our offer letter. Now these fields might be the same. So this might be the salary field. This might be the job title, and this might be the line manager. We can then create links between these two contracts because they will contain the same information. And by linking them, we make them easier to search and query. It's easier for an employer to keep track of all their employment contracts and keep track of all their employment, letters or contracts with with a specific person and makes it easier for them to keep track of. So reasoning is something that also allows us to, add more information into our database and enrich our data. So legal documents are bound by the law, and, hence, they have rules they need to follow in order to be compliant. This is where rule based reasoning can be useful because we can use it to validate contract data. So legal documents, may will have clauses, and by using rule based reasoning, we can validate the clauses that are within a document or ensure that clauses are written correctly by using rule based reasoning. Rule based reasoning generally uses an if then format, whereby if the condition of the rule is met, then the latter part of the rule can be applied to the dataset, which can help us validate contract data because we can validate certain fields within a contract. So in an employment agreement, we can validate that the salary field was there, the job title field was there, benefits, and insurance details were there. We can also use reasoning to connect contracts as we've previously discussed and infer new relationships based on the information available to us. Querying, as you all know, is traditionally the way in which you interact with a knowledge graph. So with reasoning, querying then becomes really insightful to us because we can use rules to determine a lot to predetermine a lot of the information that is tied to our documents. As you all know, querying generally uses sparkle to interact with the data store. And based from based off reasoning, we can use sparkle to interact with our data store and the new insights from rule based reasoning. One of the other things that I think is quite useful with rule based reasoning is it allows you to act with the benefit of hindsight. So if you have access to your data store, you can look at all the information you have available to you and, like I said, enrich your data, with further relationships because you now know what you're dealing with, and you can add more relationships into the data to make it intuitive to understand. I also personally think that sparkle and queries are an intuitive method of interacting with data. You're looking for patterns and relationships and matching them up. So using queries to interact with your database can be easy to pick up and easy to understand, which is something I found as well, when I first started working with Agistay and working with Monograph, because I found the pattern nature really easy to follow and easy to pick up. And now we're gonna go through some use cases of using knowledge graphs, in the legal sector and giving you some examples of how exactly this can work. So we'll start with an easy example and then work our oh, whoops. Sorry. And and then work our way up. So one of the easiest ways is to ask questions and retrieve answers. And we can do this simply by querying our database. So it can be as simple as asking your database how many of our contracts are employment agreements. And as I was saying previously how we can add, we can enrich our data with the benefit of hindsight. Here, we're asking our database how many contracts are employment agreements, but we've added this employment agreement class by looking at all the contracts that we have in our data store and adding in further rules to create classes and enrich our data. And although this can be done by other databases, this is a lot quicker with knowledge graphs, and we can use the reasoning to help clear up some information. We can also use queries to calculate metrics. So like like I've just said, you can calculate metrics using other databases, but we can make this a lot quicker with knowledge graphs, and we can also provide users with tailored metrics, that are specific to them. So this metric is looking through the database to find all the active teams within team two. So it's looking for all the contracts that are a part of team two, and then it's looking for the start and end dates attached to the contract and filtering for if the end date is after the date and time right now and if the start date is before the date and time right now. We also have an optional field for the end date. So for a contract like an employment agreement, that may not necessarily have an end date because an employee can choose when they wish to leave. The end date is optional. And so if the end date isn't bound, we just look for if the start date is before the date and time right now. So that's just a little breakdown of what this query is doing, but then we can then append this information to a metric and use it wherever we like. Building upon this, we can also use knowledge graphs to provide users with contract specific metrics. So provide them with metrics that are tailored to, their contracts or to their their dataset, and this doesn't necessarily have to be an employment metric. It can be a property metric. It can be a business metric and look at NDAs and the average term. This is just an example that I've picked out. So this metric is for a given team, it's looking for the total salary based on all the contracts within that team that somebody has created. And we can also build a histogram that automatically arranges itself based on the values of the contracts. So say we were to hire another team member who has a salary of eighty thousand pounds, this histogram will automatically arrange itself, to not only include include them in the in the distribution and post very likely at the end of this distribution, But, this this distribution here will also rearrange itself, to organize the data based on the information in the data set. So it will work out the median and the quartiles accordingly based on the contracts within the database, and, it will re rearrange itself accordingly, which is really handy and just makes sure that this this information is tailored to a user and gives them all the insight that they require. As we've discussed previously, we can also use contracts to validate contract structures, and verify that contract configurations are compliant, with logical, legal, and user defined compliance rules. So an employment agreement, for example, in order to be classified as an employment agreement, would need to contain a salary, a job title, a work location, and a restriction period. These are just some examples of fields. They can contain more or less, but these are some of the fields that would define the contract to be an employment agreement. And this is important because the employee needs to know their necessary obligations as an employee, and the employer is obligated to provide them with this information. Using knowledge graphs and reasoning, we can verify if a particular contract's if sorry. Using knowledge graphs and reasoning, we can verify if a particular contract should have particular clauses depending on the type of contract it is, and we can we can say whether they should be included or whether they shouldn't be. One of the last use cases we can have for knowledge graphs are smart folders. So using we can further classify our contracts by using smart folders, that automatically filter contracts based on active data. So they'll filter through documents by looking at the information inside them and, organizing contracts into folders accordingly. So they look through your data and they look for key information and they can filter by a start date, a particular line manager, by contracts with a specific individual, by looking at contracts within a specific year, or by look by a group of contracts, with a company as a specific party. So it will look at the entries of a contract and then make these groupings accordingly. It can connect contracts, and it can draw insights that would be time consuming to do on our own without the use of a knowledge graph or rule based reasoning. To do this manually, you'd have to look at the contract terms, analyze them, and link them manually. And using knowledge graphs, this completely removes the need to do that. So we've spoken about some use cases, and we're gonna dive into another example of how we can use knowledge graphs, which is by visualizing contracts. We've also spoken about how contracts can be difficult to understand and difficult to read through. And so once you have access to the fields of a contract in a knowledge graph, we can derive insights from contract data to make it it accessible to non lawyers. And one of the ways we can do that is to make the contract more visual or to just simply make the contract easier to understand by presenting the information in another way. So by storing the information in a contract by its specific contract fields, we can easily access the information in these fields and use it in multiple areas. One of the examples of this is that letting agents will often provide their, student let lets with a a key information sheet that will be provided based on each individual contract and tailored based on each individual contract. It might have the key information that student will need to know, such as when the rent is due, how much late fees are, how much notice is given for reviewing. And this could be done via a template that a letting agent may have where they just simply replace the information by looking through the the linked contract and adding this information into the template. But, again, this would have to be edited according to each individual contract. And using knowledge graphs, we can make this process a lot easier. So using knowledge graphs and reasoning, we can derive this information using domain expertise. We can use reasoning to access the information depending on the type of field it's being stored in. So, basically, if we know what type of contract field a field is, such as if it's a salary field, we can access the information in this field, which will be the entry to for the salary, and, use this in a variety of formats, either by visualization, so by showing it in a more visual manner, or simply, showing it in a partially visual manner, but a easy to understand manner. So that instead of looking through the whole contract, a individual can just look at the key summary, the key summary fields and get exact get the information that they need. And, yeah, this can be presented in any way, that a user decides. So this is just an example of us showing you that, we've got this information for my knowledge graph. This could be a string that is just stored, and then this value is filled is sorry. This value is filled with the, value from the contract. And this is what will change, according to each individual contract. And this is made a lot easier using knowledge graphs, because it can just look for the the fields stored within a contract and change it accordingly, which again ties in with our aim to make contracts easier to understand, and easy for anybody to deal with no matter what their background is. So as we've discussed, knowledge graphs have a number of different practical and helpful uses in the legal sector, and we've hope we've been able we hope we've been able to shed some light on this. It allows us to extract insights and information from legal documents. We can use rule based reasoning to infer new relation relationships or simply build more relationships into the graph and make our data more insightful. Using rule based reasoning, we can also get more useful queries, and interact with our data in a more useful manner and find exactly what it is we're looking for. Knowledge graphs have the ability to unlock so much information, and we've only just touched on it. But we hope to continue our research and explore this field so that we can make the best use of them and really show what knowledge graphs can do, and what the what their potential is. Thank you for your time. And if you have any questions, I'll be happy to take them.",
    "transcript_length": 19719,
    "speaker": "Vaishali Raghvani",
    "tags": [
      "contracts",
      "data extraction",
      "data management",
      "insights",
      "knowledge graphs",
      "legal documents",
      "metrics",
      "rule based reasoning",
      "smart folders",
      "sparkle"
    ]
  },
  {
    "title": "Leveraging Graphcore\u2019s IPU architecture for large scale GNN compute",
    "description": "Machine Learning on large scale graphs presents several unique challenges, due to the sparsity of the connections. Exact computation is often intractable on current accelerators, and algorithmic approximations fall short of modelling interesting aspects like long range dependencies effectively.\n\nWe present how Graphcore\u2019s IPU design tackles these challenges, creating the opportunity to accelerate deep GNNs on large graphs. This talk aims at stimulating Data Scientists, Machine Learning Researchers and Engineers to think about different ways to deploy current large scale GNNs and to develop algorithms that exploit the full potential of our new hardware architecture.",
    "category": "Graph AI",
    "transcript": "This presentation will discuss the opportunities for leveraging Graphcore's IPU architecture for graph neural networks. We will review the computational requirements and challenges of graph networks that require both sparse access to memory, a sparse computation, and dense processing, which is associated with the use of neural networks as function approximators. We will then introduce the intelligent processing unit, the IPU, and review the design principles, the processor architecture and the system scale out. Finally we will discuss implementation opportunities for graph neural networks on the IPO. We will start by considering graph neural network processing and its challenges. Graph neural networks have been successfully applied to a growing number of applications based on graph structured data. Applications that range from the small graphs of molecular chemistry and drug discovery to the very large graphs of recommended systems. This application involved tasks including node classification, link prediction, and graph classification. However, graph neural networks come with a number of challenges. The first challenge is related to sparse computation. The implementation of the graph neural network is based on the message passing algorithm, which is implemented as a series of message passing steps or GNN layers. And for each individual message passing step, the embedding of a node is updated using the neighbors of the node. And for each of the neighboring nodes, the computation involves the use of a message function to compute a message associated with the neighboring node, and all the neighboring node messages are then aggregated and sent to an update function, that produces the update, the sequence to update the embedding of the node. And both the message function and the update function are parameterized by neural networks and therefore corresponds to dense computation, where there is at the same time irregularly sparse connectivity associated with gathering the neighbours of each individual node. This GNN sparse connectivity results in lower arithmetic intensity and higher communication costs compared to dense processing. And these computations are particularly challenging for modern, hardware accelerators. Another challenge is related to heterogeneous graphs, which are graphs with multiple node types and multiple edge types or relations. For these graphs, for each of the relation types there is a different set of neural network parameters, in addition to having different set of neural network parameters for the individual layers of the g n n, for individual message passing steps of the g n n computation. This, considering that in general in the order of thousands of relation types for this type of graphs, this corresponds to a significant increase of the number of parameters to train, which can cause overfitting. Another challenge of graph networks is related to the fact that increasing the number of GNN layers, and each each one of these layers, corresponded to an aggregation of neighbors, this causes an exponential increase of multi hop neighbors, which causes an increase of computational complexity and memory cost. At the same time increasing the number of neighbors aggregated over several hops and aggregating information that corresponds to an increasing number of nodes that are part of the graphs, makes the representation of the nodes more similar, which causes over smoothing. Oversmoothing can correspond to a degradation of the task performance. Another challenge is related to dynamic graphs. Dynamic graphs are prevalent in real life systems with applications related to social networks, forecasting, epidemiology and others. And these types of dynamic graphs evolve over time features of their connectivity, have for example additions or deletions of nodes or edges, and have the transformation of the features of nodes and edges. An issue related to this type of graphs is that when collecting a number of examples in a mini batch for training, especially when the there is a large mini batch, the information that is evolving over time related to the last samples in the batch lacks inaccurate up to date information for the early examples in the batch. So there is a staleness not up to date information that again can correspond to a degradation of task performance. A final issue related to training of graph neural networks relates to large scale deep learning. We have already described the issues of deeper GNNs, graph neural networks that have a large number of message passing steps which can cause a neighborhood explosion and oversmoothing. Besides those problems when training very large graphs, there is the issue that training still requires efficient and fine grained access to memory. And given the high level of sparsity of large graphs, when accessing a memory it is possible that only a fraction of the communication bandwidth may correspond to useful bandwidth. We will now review the main features of the intelligence processing unit architecture. The intelligence processing unit has been designed from the ground up to accelerate AI workloads and to achieve the following goals: Fast and fixed cost access to memory, Efficient fine grained computation with low latency. The possibility of efficiently scaling to systems that include a significantly high number of processors. And finally, the possibility of maintaining high performance for low precision compute, when using low precision number formats that deliver high computational efficiency, for low power consumption, and reduced memory cost. This has resulted in the following main characteristics of the IPU process: A very large number of independent cores, and distributed on chip SRAM. Computation that is based on multiple instruction multiple data parallelism and relies on a bulk synchronous parallel programming model, which allows to exchange data between different cores of the processor without memory concurrencies. And finally, in addition to the provision delivered by the hardware of efficient computation for fine grained computational elements. This is also supported by the software with the provision of sparse instructions and libraries. These slides give additional details on the IPU processor, the GC200. A single processor contains one thousand four hundred and seventy two independent IPU tiles, we distributed in processor memory. Each tile consists in other words of both compute and memory, for a total of two fifty teraflops of fourteen point six in compute and one hundred megabytes of in processor memory. The on chip memory can be accessed with a bandwidth of forty seven point five terabyte per second. Four IPU processors can be put together to build an IPU M2000 machine which deliver a total of one petaflops of floating point six in compute and a total of three point six gigabytes of in processor memory. The M2000 machine also contains an IPU gateway and external DRAM, for a total of four forty eight gigabytes of streaming memory. The connection between the four processors, the four IPU processors of the N2000 machine is provided by FastIP Link, that have a bandwidth of sixty four gigabyte per second. This slide compares the memory subsystems of the F2000 machine with the memory subsystem of alternative hardware accelerators. And the LPUM2000 have a larger amount, a significantly larger amount of on chip memory, which is accessed with extremely high memory bandwidth in excess of forty seven terabytes per second, and also have a much larger amount of DRAM with lower bandwidth, which is motivated by the fact that the large amount of on chip RAM, on chip SRAM generally corresponds to a reduced requirement of DRAM traffic. This slide gives typical examples of the use of external memory for training of neural networks. There is on the top the simple case where the entire model fits on in processor memory and there is no need to access external memory for training. In other situations, it's possible to advantageously store master weights and optimized states in external memory, especially for situation corresponding to pipeline parallelism where, there is only infrequent need of the weight update and accessing external memory to use the optimizer state for updating the parameters. And finally, there is the case where there is is useful to have a more regular access to external memory with phase execution using streaming memory and overlapping of communication and IO. And in this case, the periodicity of accessing the external DDR is larger given the provision of a large amount of on chip SRAM. The IPM two thousand and seven machine that we have just described can be used to build even larger systems. The IPOPOD64 is constituted by sixteen IPOM2000 machines corresponding to sixty four IPU processors, and provides a total of sixteen petaflops or fourteen point six in compute, corresponding to fifty seven point six gigabytes of importer memory and around seven terabyte of streaming memory. And these IPU pods can be also used to build even larger systems, aggregating several racks, several IPU pods, to arrive to hundreds and even thousands of processors up to sixty four thousand processors. And these very large systems are glued together by the IPU fabric, which provides connections between the individual IPU processors of each M2000 machine within an IPU pod, as you have described before, and a connection to the host, and connection through the gateway link between different racks between different IPo pods of a much larger system. We'll now consider the implementation opportunities offered by the IPU. Implementation opportunities for graph processing on the IPO are given by the scalable and cost effective DRAM that can be used to store large embedding, but the possible use of large SRAM to store neural network weights and model data, but the use of distributed SRAM rely on the BSP programming model to deliver efficient execution of it for heterogeneous computation and finally the possibility of using the fast APU links for efficient computation in pipelines or for phased execution. The first method that can be used to train very large graphs is based on mini batch training using sampling of nodes with their respective k hop neighbors. Of course this method, for deeper GNN, for GNN with a large number of GNN layers, can face the problem that we have mentioned before, related to neighborhood explosion and oversmoothing, and in general can cause a large increase of computational memory overhead. This is the reason why the earlier implementation used the fixed neighborhood size per layer, reducing the number of neighbors segregated per each message passing step, which of course makes for effective training of very large graphs, but has a downside that corresponds to a reduction of expressivity of the model compared to processing on the original graph, which may correspond to reduced performance, reduced task performance. Alternative approaches have been proposed more recently decouple the depth of the layer, the number of message passing steps with the number of hops for neighborhood aggregation with what is referred to as shallow sampling. This method can compromise the computational efficiency by an aggregating an excessively high number of neighbors with an increased expressivity by training deeper networks. The performance of mini batch training can be improved by sampling of connected subgraphs, which are provided by graph clustering. This method improves performance and also increases the embedding utilization of PDMATCH training. In this case the complexity increases only linearly instead of exponentially with the number of GNN layers, which allows to increase the number of massive passive steps of having the deeper GNNs, which improve the expressivity of the model and correspond to improved task performance. And the performance can be further improved by sampling multiple connected clusters, which reduces the variance across mini batches. Both mini batch training are based on selecting nodes with the corresponding neighborhoods and selecting subgraphs provided by graph clustering can be efficiently implemented on the IPU. Both, in both cases the mini batch, the examples of the mini batch can be stored on on chip memory, which provide efficient and fine grained execution with fast access and for neighborhood aggregation on chip. The implementation can be scaled over a larger number of processor by parallel training, in particular relying on data parallelism and pipeline parallelism. With data parallel training, the mini batch is distributed over multiple processors and using the on chip memory to store the elements of the mini batch allows to continue to have efficient training, efficient access to memory, but increasing the the size of the mini batch and speeding up training. Pipeline parallelism at the same time involves the implementation of a number of pipeline stages which can be implemented again in the on chip memory of a number of IPUs. Different pipeline stages can include one or multiple GNN layers, one or multiple message passing steps, that corresponds to the same computation graph with different parameters. And it's particularly attractive to consider the storing the parameters of the message passing steps on the in processor memory on the on chip SRAM to reduce the amount of communication during training. And both data parallelism and pipeline parallelism for speeding up training can be complemented by the use of low precision number formats, which allow to store larger batch sizes and to be more memory efficient and to store larger amount of computation element, other larger number of samples for node wide sampling or larger subgraphs in the case of sampling of the connected subgraphs. And the lower precision number format not only allow to increase the the size of the subgraphs that can be added, can be stored on on chip, but also, as we said before, corresponds to more efficient computation and reduced power consumption. As an example of efficient computation on the IPO, we consider now temporal graph networks. This is current work in collaboration with Michael Brosteyn and Emmanuel Rossi, who had developed and proposed the original temporal graph network model. Temporal graph networks operate on continuous time dynamic graph that are represented as series of temporal events. That corresponds to the evolution of features or connections over time. The temporal event, as we said before, for a dynamic graph can be can be constituted by the addition or deletion of a note or the interaction between pair of notes, the modification of the edge between between different notes. In the case of the model, of the temporal graph network model, it's the characteristic is that the node, each node is associated with a memory, a state, that is updated after every event. The computation starts by collecting a mini batch of a number of recent temporal events for processing and this mini batch contains the current state of the nodes involved in the in the mini batch, in the events that we consider. And this information is then used to compute a message associated with the the nodes under consideration. And the messages are then used to update the memory of the nodes. The memory update, it's implemented by neural network, typically a GRU, a Recurrent Neural Network. And then the updated states, the data memory of the nodes together with the input information associated with the mini batch is used for providing node and updating node embeddings. Now the node embedding function is particularly critical and is based on aggregating information from the node neighbors. Because this is justified by the fact that in these graphs and some nodes are often infrequently updated. Therefore, it is advantageous to rely on the connected neighbors and information to compensate for the information that is lacking in up to date information that is missing for specific nodes. And this is the reason why the node embedding update implementation relies on, for example, temporal graph attention network or temporal graph sum networks. Once the node embedding has been updated through this type of processing, the updated node embeddings are fed, are sent to a decoder which provides node classification or link prediction in the case of this block diagram. The task is to provide prediction of the future for future interactions. This model had been very successful, improving by a large amount the state of the art in a number of relevant tasks. However, as we have discussed before, when when considering dynamic graphs, they have the issue that for a large batch size using the aggregation of a large number of the temporal events, there is the possibility that the later component of the batch, the later samples of the batch, lack the information from the area sample and have the not up to date information which can cause stale update and degrade the task performance. We have implemented this model on the IPU and the plot on the left hand side of this slide shows in fact that the average performance, the average test precision during training, it's much degraded for larger bite size. During training increasing the number of epochs the performance for larger bite sizes is significantly worse than the one for bite sizes small as forty or sixty. However, using batch sizes as small as forty or sixty typically is connected with a fine grained computation that is not very efficient on the conventional hardware accelerators. But implementing this type of computation on the IPOS is shown here on the right hand side, we have been able to maintain improved task performance at the same time delivering speed up of training. Maintaining a faster training for the smaller batch size corresponding to the case of the large batch size which doesn't achieve the same task performance. In summary, we have reviewed the main challenges of graph neural network processing that requires fast computation, a heterogeneous and fragmented computation with a high memory load. And we have discussed the key features, the key design features of the IPU architecture, which can rely on scalable and cost effective DRAM and can make use of the BXP execution and the large amount of and cheapest RAM to provide efficient computation for fine grained elements associated with the processing of graph neural networks. And we have considered implementation opportunities and methods for training large graphs on the IPO and we have given us a particular example of efficient fine grained computation for a graph neural network implemented on the IPU. And this concludes the presentation.",
    "transcript_length": 18727,
    "speaker": "Carlo Luschi",
    "tags": [
      "BSP programming model",
      "FastIP Link",
      "GC200",
      "GNN layers",
      "GRU",
      "IPU M2000",
      "data parallelism",
      "deep learning",
      "dynamic graphs",
      "graph classification"
    ]
  },
  {
    "title": "Modelling regulation requirements using SHACL",
    "description": "When working for a public agency in Norway it became apparent that the Web Ontology Language (OWL) was not expressive enough to represent a detailed model for requirements described in regulations.\n\nThe Shape Constraint Language (SHACL) came to a rescue!\n\nThis presentation will talk you through how to go from regulation text to SHACL model used for information withdrawal in an ongoing project.",
    "category": "Semantic Technology",
    "transcript": "So it's my great pleasure to, introduce Veronica Heimsback, if if I pronounce that correctly. Heimsback. Yep. Okay. Senior consultant in, data science and AI, insights and data at, Capgemini, who will be talking about quite a hot topic in our space, shackle. So over over to you, Veronica. Thank you, James. Okay. Hello, everyone, and welcome to this talk on how we can model requirements described in regulations with the shape constraint language. My name is Veronika Heinspak, and I'm a senior consultant at Capgemini in Norway. I live, right outside of Oslo with my fiancee and our two children, three year old Edward and one year old Oda. I got my degree from the University of Oslo with a specialization in logic and semantics. And, of work experience, I've been touching into stuff like, writing kernel models in c to semantic technologies applications in Java. I started off as a developer, but in two thousand and nineteen, I joined Capgemini. And here I am working as a semantics nerd, information architect, and advisor. I have worked in several different industries, including oil, life sciences, public data, library, and media. I started working with Chacol in two thousand and sixteen, and, yes, that is before it became a standard. And, as all nerds, I have way too many hobbies, like, making stuff, hunting, board games, and teaching kids to code to name a few. So my current client is the Norwich Maritime Authority, and, all the examples you'll see today are actual data from our current project, simplified for the sake of examples, of course. So let us have a look at our simplified workflow of the semantics in the project. We have several different regulations in both Norwegian and English as PDFs or plain text. And, some monkey read me identifying concepts and relationships using my highlighter pen in order to transform the data into an owl light ontology that works as a term bank. So it could just be a discourse taxonomy, for example, and shackle constraints for detailed instance modeling. We have a sparkle query library that the back end use for information withdrawal that goes through some processing before it's pushed to the front end application. And currently, we have two projects running in parallel at the NMA. The first one is called APS. And, a brief description of that could be automatic detection of requirements connected to certificates for sailors. And the second one is called ET for short, which describe metadata for requirements and make use of the graph as an information lake to ask for specific questions connected to the requirements. An example of a question could be, my fishing vessel is eight meters long, and it's built on January third in nineteen ninety eight. Give me all requirements that apply for me in or order to operate in a regional waters. In this talk, I'll show you examples from the APS project. So let us say we have a sailor that wants to become a master mountaineer. There are then two things you can do. First, you could use the sparkle library to find all requirements needed for the master's minor certificate. This information is found in the shapes in combination with the owl light ontology. And secondly, we could take the sailor's CV and requirement shape for this certificate, the master Maynard certificate, as input. And output will be either conforms true if and only if he fulfills the requirements needed for the certificate or confirms false if it does not. If the output confirms false, you'll get a report from the shackle engine telling you what, the instance data, the sailor's CV, is missing in order to achieve this certificate described in the shapes. So the data can look like this. And this is the regulation text. It's a plain text file where someone must identify classes, relationships, and data values by hand at the moment. This is me and my highlight highlighter pen reading the region regulation text and using the English version of the regulation to translate on the fly. We do actually have a proof of concept running right now to see if it's faster to identify these entities with natural language processing techniques. And I'm quite sure that NLP is much faster than me reading stuff in an unknown domain as I had no prior knowledge or maritime data before I joined this project. So there's a there have been a lot of funny misunderstandings of terms and concepts. So we're going to model things like this. So here we see paragraph twenty eight part two, and it content describes requirements for gaining a master's minor certificate. And it says, in order to be issued a competency certificate, decafusser class one, that's the master millionaire, In addition to the requirements in paragraph twenty three, twenty four, twenty five, twenty six, and twenty seven, a minimum of thirty six months seagoing service as a responsible deck officer on seagoing ships with a gross tonnage of more than five hundred is required. Seagoing service is reduced to twenty four months if at least twelve months are earned as a chief officer on a vessel of gross tonnage five hundred or more. Long sentences. Anyways, as we see, there is a lot of information we want to keep from this short snippet of text, and there is also alternatives described here. The first alternative is minimum thirty six months as a deck officer, where deck officer is a superclass for all kinds of deck officer positions. The second alternative is minimum of twenty four months as a deck officer, where at least twelve of those is served as a chief officer. And chief officer is a subclass of deck officer. So how can we handle alternatives that includes a lot of different combinations and relationships? If we were to model this with the web ontology language, we sure would have to touch into owl axioms and restrictions at some point. We also need to know if data is correct, especially when we are to compare instance data with regulation requirements. And AL can only infer new facts in data, but it can't discover missing facts. And being able to understand OWL axioms and how our interpret data, you will need some understanding of discrete mathematics and logic. Well, my client are seafarers. Even though some players at the anime are trained in RDF, that does not mean we should throw high complexity at the table just because we can. The data owners here are experts in their domain, which is different and not in semantic technologies. The main language we then chose for a graph is the shape constraint language, a language designed for validating RDF under a closed world assumption. Unlike OWL, you can confirm data to given data constraints with Jekyll, which gives us the opportunity of checking a sailor's CV against the set of requirements to see if it fulfills a specific certificate or not. And, the shape constraint language are built up by shapes. And a shackle shape is a collection of constraints for a given RDF resource. We have two kinds of shapes in Shackle. The first is a node shape that describe constraints about focus nodes, usually the subject of a triple, and property shapes describing constraints about predicates and object values of a triple. The definition of a node shape is that it is a shape that is not the subject of a triple where path is the predicate. In this example, we see a vessel shape that has a constraint on target class vessel. So all constraints following in this shape will have to do with instances of the class vessel. The definition of a property shape is that it is is a shape that is the subject of a triple that has path as its predicate. In this example, we see a length shape with some path to vessel length. So for every occurrence of the predicted vessel length in any data, the constraints described here will apply to that triple. We can add property shapes onto a node shape using the resource property. Here we combine our true, previous examples stating that any instance of the class vessel may have some relationship called vessel length pointing to, in this case, some random object value as we haven't any more detailed constraints in the property shape. So there is a lot of constraints available in the Shackle core constraint vocabulary, and here is a brief overview of the main categories. And the nice thing about Shackle is that if you don't find a suitable constraint for your problem, you can extend Chackle with your own constraint definitions and push them to the web for others to reuse if you like. This is not possible to do with OWL as the OWL vocabulary is limited to those resources set by the OWL committee. And, Schectel contains constraints about, value types as class affiliation and data type values, cardinality as min and max count of predicates, value range as min and max inclusive and exclusive, string based as language checking, regular expressions, and length, property pair, comparing the object value of two predicates, logical constraints as not, and, and or, Shape based as the property relationship, we have just seen an example on. And there is also a collection of other constraints that doesn't fit into any of these other categories. And, there do exist more categories apart from the core, like sparkle constraints and check the advanced features, but I will not cover those in my talk today. I can post the link for the shackle master class that was hosted yesterday. There are some links to resources and references on things beyond sparkle core available there. So the main reason or one of the main reasons we chose Shackle was because of the extensive Shackle core constraint vocabulary and being able to extend that that if needed. Being able to model requirements containing alternatives as easily recognizable and and or constraints is a great benefit for us at the anime. Another reason is the verbose description of constraints that makes it easier for non semantic players to read the turtle files. Unfortunately, there is no currently no open source tool, at least to my knowledge, that let us visually create shackle constraints as protege a does for OWL. I know it exists a shackle plugin for protege, but as far as I know, or at least last time I tried to use it, it was, only a circle code view inside Prodigy and not the visual modeling tool that you know Progyny as. So we would have to stick with our editor of choice if we want to keep it open source. And Chackle is a validation language under closed world assumption, meaning that facts that are not known to be true are false. And this is necessary for us in order to say something about our case for comparing CVs to regulation requirements. So let us take this example snippet from the regulation. How can we easily express the requirement that a vessel shall have a gross tonnage of at least five hundred wells? Like, this. And here we have a property shape describing constraints for a given predicate. The predicate in question is the value of half, which is Gross Tonnage. We have a constraint that in a triple where gross tonnage is the predicate, the object value shall be minimum or equal to five hundred. We also have a constraint telling us the data type of this object value, which is unit g t, and a min and max count of one. That means that the predicate is unique and mandatory for resources where it appears. If vessel has a relationship cross to some value, this specific relationship shall only appear, once, at least once and not more than once. And here we see our solution on how to model the alternatives described in the regulation. As we remember from paragraph twenty eight part two, there were two different alternatives. There are also, some other options included in this or constraint, and I'm going to talk you through it. So the or constraint is a list taking constraint that appears at the predicate position of a shape. This or constraint takes in two and constraints as its list elements. The first and constraint are the first alternative. It takes in two items in its list. First, a or list of courses. So at least one of these courses, courses must be present for the whole expression to be valid. No. Not courses. Sorry. It's certifications. So the list of certifications. So at least one of these certifications must be present for the whole expression to be valid. And secondly, a specific value for a seagoing service, And we'll have a look a closer look at this seagoing service, later on. The second and constraints are the second alternative. It takes three items in its list. First, a or list of certificates, and then a specific value for a seagoing service, and another specific value for a seagoing service. We remember from paragraph twenty eight that the, second alternative was twenty four months as, some in some position, where at least twelve of those months were in another specific position. And that is described in this second and constraint. So let us look at the rest of the requirements in the snippet. We have information about duration, position, and a seagoing relationship, which can be some kind of trade area, the gross tonnage, and the gross tonnage that we saw in the previous example. And here are the requirements on the previous slide that described as shapes. The first shape is similar to the one for gross to Nash, describing a minimum or equals constraint. The second, state that the predicate in position shall be any instance of a decoder position. At last, we have a property shape describing the predicate trade area to have a specific value of bank phishing. The plus hierarchy with labels and relationships is found in the owl light ontology at the moment. However, this information could as well be in the shapes instead. At the NMA, they already had a few owl ontologies for their domain when I joined the project, but they struggled a lot, with how to model the alternatives described in the regulation and applying information like mean inclusive and data values. And that is another of the main reasons why I proposed model requirements in Chacel as it is so verbose and concrete. And for this case, on comparing CVs and regulation requirements, it's also the perfect case for Shackle validation. So let us put it all together for our first alternative in paragraph twenty eight part two. We have a node shape describing the class value of this first alternative using the property shape constraints that we just saw. And some good good annotation, shape description is a must. And then we target the class that represent the first kind of seagoing service described in the paragraph, which is the the value for target class. And at last, we connect the necessary property shapes with constraints that apply to this kind of seagoing service. The constraints for duration, position, trade area, and gross to match. So what Checkl really does for our models is it allows us to model data with a closed world assumption. This means that we strive for completeness of our data, and we do not admit that we have any incomplete knowledge in our regulations. And that is the whole point of a regulation, isn't it? A regulation is a set of requirements for one specific subdomain of maritime authorities in the region waters in this particular project. And it is important to remember that with the web ontology language and the open world assumption, we can describe broader and more unknown domains that makes it easier to apply ontologies across various applications and domains. However, even though shackle under a closed world assumption is probably most powerful locally, it is possible to reuse and share constraints across several application, especially in combination with existing ontologies using them as a reference for your constraints. If you're reusing shackle shapes from another application, you have the possibility of deactivating shapes that it's irrelevant for your application and add new ones that apply in order to extend the shapes graph you're reusing. With shackle, we can handle any constraint. And if we don't find what we need in the shackle core, you can simply add it yourself. You can even push your new constraints to the web for others to reuse. And shackle is verbose and gives us detailed explanation of requirements, which is really handy for players with limited experience with RDF. And again, OWL and RDFS are splendid on inferencing missing facts. However, it is not detecting missing data, and that is what we need. Would would you like to go over to questions now? Sure. You'll be pleased to know it's been relentless, the, the number of questions. So I'll try and rewind back to some of some of the earlier ones. What are the hardware requirements to use Shackle as a validation language? Do you need a reasoner, data storage, etcetera? It depends on your application and what you already have, but, you can do Shackle validation in memory if you like without, having to use a graph database. So, you have several different shackle implementations ready in frameworks for programming languages as Java, c sharp, and Python. I'm most familiar with the Java frameworks, and I know that the the two most popular frameworks, which is RDFJ and Jena, doesn't support the full shackle implementation yet. But it's possible to use it for for the most popular constraints in the shackle core. And by using a programming framework, you won't need a database. There is also shackle implementations in graph databases. As StarDog, they have a implementation of the shackle core constraints. And also, top quadrant actually have a full shackle implementation, including the shackle advanced features. And that is probably because one of the employees at Top Quadrant is, one of the leading developers on this standard. Yes. Apart from that, I don't know if I see there was a question on Neo four j or AVS Neptune. I'm not familiar with, if Neo four j has an implementation of Shackle. I'm not sure because Neo four j is operating on other data than than RDF. It's on labeled perfect graphs and not semantic knowledge graphs. But I know they try to map their labeled property graphs onto RDF and vice versa. So if there exists some shackle implementations in Neo four j, I don't know, but it would be funny to see. Yeah. I shared, I shared a link. So they have they have an implementation of shackle, but I think it's it relies on transforming RDF to label property graph, and then it's, it's part of an internal kind of Java plug in, I think, some of the validation functionality. There was a question or some points around shackle c versus shackle. I'm not sure whether that's a thing maybe might want to elaborate on. I don't know whether that makes any sense. I'm not familiar with shackle c. No. No. No. Not me neither. I'm sorry. It's a typo. Okay. It was Yeah. It it may look like shape expressions that, has been kind of a schema language for RDF prior to Shackle, but I'm not too familiar with that. So, someone Yeah. Someone commented Shackle compact syntax is Okay. Okay. That's that's his one. That has to do with Jason Aldi probably because that can be compacted quite a bit. Okay. There was another question, about whether it's possible to model you may have already answered this, but, whether it's possible to model shackle constraints completely via sparkle queries. Yes. All shackle constraints can be translated to sparkle at least. And I'm not sure if you can do it the other way by constructing shackle from sparkle. I haven't tried it. But in the shackle documentation, on the w three c domain, every single shape constraint has a, a sparkle definition as well. So So you can go in there and look on the sparkle definition for every single constraint. I'm sure it probably is possible to turn it the other way around as well if you want to construct shapes using, sparkle. Okay. Great. There was a question also from, Vladimir Alexiev, asking, Veronica, have you considered generating shackle from a briefer logical notation? Your rules are reminiscent of prologue. No. I I haven't considered that, but, I'll, bear it with me. Thank you. Cool. Maybe maybe you guys can, connect in the, in the Slack channel. Yeah. Great. Are there, any other questions? I can see a few people typing. So, I'll just check back through the history of the questions whilst that's those are coming through. I guess whilst, whilst those questions are coming through, I, yeah, I have a real affinity for the the kind of language challenge in, shipping. So we, we do a lot of work. My company does a lot of work in shipping. And the the language the language is particularly unique, isn't it? And there's very little in the way of kind of industry standards, particularly kind of ontologies or at least from from what we've found anyway. So I guess I was interested in how you're applying Shackle for kind of unstructured versus structured data, whether you're using Shackle for you touched on kind of maybe some opportunities around NLP and, Yeah. Yes. The NLP proof of concept was started because I didn't bother to spend too much time on reading the regulation text. It's about, I believe, around seventy regulations in the Norwich marathon sources. And, translating manually one text to anthology or two shuffle shapes by hand took probably around a week. So that's time consuming and expensive for my client. So we wanted to try out to see if we could make use of some simple NLP techniques to, draw information from the text. So we have actually successfully generated all scope descriptions and requirements descriptions connected to those scopes. And one scope could be like a a value range for vessel length or a date before and after one certain date or machine power. And a lot of different things are scopes in in the text. So we successfully do that now. And, the most challenging task around that is classifying entities discovered in text to be of kind material or construction or equipment and stuff like that. And so, yeah, the the proof of concept is still running. It's supposed to be finalized during December, and, hopefully, we will get the result that we want, which is the the class hierarchy or taxonomy, including labels and the the the shapes describing requirements and and scopes for the data.",
    "transcript_length": 22205,
    "speaker": "Veronika Heimsbakk",
    "tags": [
      "c",
      "java",
      "jena",
      "json",
      "nlp",
      "owl",
      "prolog",
      "python",
      "rdf",
      "rdf4j"
    ]
  },
  {
    "title": "Neural Algorithmic Reasoning: Combining Classical Algorithms and Neural Networks",
    "description": "Nowadays, using known algorithms to solve a problem or leverging advances in deep learning to tackle it are seen as two orthogonal manners to reach a solution. However, many problems could benefit instead from a combination of the two.\n\nHere, I will present the Neural Algorithmic Reasoning (NAR) direction and how it aims to combine the guarantees and generalization power of the classical algorithms with the adaptability and possibility of working directly with raw data of neural networks. Moreover, I will introduce NAR's successful application to the field of reinforcement learning.\n\nMore precisely, I will summarise how a neural network can imitate a dynamic programming planning algorithm, that can guarantee finding optimal solutions, resulting in performance gains especially in low-data environments.\n\nIf you\u2019re looking at solving a real-world problem and you think there are heuristics that could be helpful, but not fully solving it and/or that require the data to be thoroughly denoised before being useful, I would say this talk is for you!",
    "category": "Graph AI",
    "transcript": "Hello everybody. I'm Andreea Deac, PhD student at Mila University of Montreal. And today I'm happy to present a talk that I think will, will be interesting to all problem solvers among us. This is called neural algorithmic reasoning. And we'll we'll talk about how to combine classical algorithms and neural networks. This is, as you might think already, there are two well known approaches to problem solving, each with their own benefits and drawbacks. So algorithms are trivially strongly generalizable. You can apply them to different sized inputs, and they will still work. They you can compose them if you have different functions, subroutines. You can you can use them together, change them. They have guaranteed correctness and they are interpretable. They have interpretable operations. We always know what they're doing. However, their inputs must always match a specific format, and they're not robust to task variations. On the other hand, the the more recent, go to, the the neural networks, they can operate on raw inputs. They can generalize on basic conditions. So even if, our inputs are are not precise, they will still work. They can be used across tasks. But a a downside is that before getting a neural network to perform well, we usually require a lot of data to train them, to get them to a good performance. They are also unreliable when extrapolating, and they lack interpretability, although that's actively being worked on. You might notice that these two approaches have attributes that are complementary, and what we, what we might want is to get the best of both worlds. So this is what today's talk is going to be about. And I'll start by talking about a problem that is, quite simple. Most computer science students meet it, at some point or another. So if we have a weighted graph with a specific source, node of interest, and we want to compute all shortest paths from this node to the other nodes in the graph. We can do this by using some dynamic programming algorithm, for example, Bellman form being one of them. And recently, there, there was the, the paper called neural execution of graph algorithms, where, a team of researchers tried to, to propose a neural network that can achieve the same goal. So this was a graph neural network in particular that was supervised to on, intermediate outputs of the Bellman Ford algorithm. So this was this is also known as trunk supervision of at, at every step of the of the iteration. And you can see then the what what we the the operation in the dynamic programming algorithm, aligns quite well with the message passing update rule, in a way. So this observation was, was something that was, explored more in, in a concurrent work, where where they they discovered that graph neural networks align well, in general with dynamic programming algorithms, not just Bellman Ford. And some other, findings were were were pointed out. So, for example, this strong supervision I was talking about is actually quite important to a good performance of the graph neural network in imitating the algorithm. We it requires things like, a different aggregation, the max aggregation to perform well, which aligns, again with the with what we know the dynamic programming algorithm, does. More recently, there's also been a lot of work on proposing better architectures, for example, for for, achieving some computation classic computational goals, and the some even more recent insights, for example, linear algorithmic alignment is highly beneficial. And if we learn multiple algorithms in a multi tasking way, this can also improve performance. So this is also very useful, especially if we want a component that learns multiple algorithms, not just one. So this led to a blueprint being proposed by Velich, Komichi, and Glanton, in in patterns. So under the the name of neural algorithmic reasoning, we have this blueprint, which starts from from the assumption that if we have a task where we know an algorithm could be could be useful to solve this task, Then we can pretrain the processor on some abstract inputs to learn this, how to execute this, how to simulate this algorithm. And then we can take this processor and plug it into our natural pipeline. So in the, in the case of navigation where we're interested from, from getting, in getting from point a to b, c, or d, we might want to use something like Bellarmine Fund, which computes surface path to see what is the best way to to get somewhere, to to do, to to navigate. So keeping this blueprint in mind, you might be wondering, does this work in practice? Has this been used in in some tasks already? So in this talk, I'll talk about, how this was used in reinforcement learning. Our our proposed agent, Excelvin, was done in collaboration with Petar Velish Velishkovich, Piotr Bakon, Jan Tang, and Vlad Nikolic. And even more recently, explored this blueprint was also explored in a self supervised learning setup, in the paper named reasoning modulated representations about which I won't have time to talk, but I recommend you you go check it out. So to jump straight into it, in a reinforcement learning setup, we have an agent that, acts upon a word and receives observations back from this word. This agent can decide to build a plan, and update this plan based on observations, a plan that can then be used on deciding which actions to take. So formalizing this a bit, this is usually found under the Markov decision process framework, where we have a state space, an action space. We have transition the transition matrix. So what's the probability of being, of getting to a state s frame given that we're in a current state s and we take a specific action a. Also rewards. So what's the reward that, the agent will receive by, doing action a in state s? And the goal is to find the policy, pi that tells me what what is the sequence of actions that I need to take in order to optimize the discounted cumulative reward. So get, as much reward over a longer time horizon as possible. So policies that act purely through adapting to rewards are called reactive. And in many cases, to to get them to work well, they require a lot of data and are quite slow to adapt. And planning is something that was proposed to ameliorate these issues by maintaining an explicit model of the world. So what this means is that we would have a state transition model and the reward model, which would usually train from some observed trajectory. So having the agent, acting to the world and gather some data. And using these motors, a planner can simulate the effects of actions before taking them, which is very important and comes with some important benefits as as we are going to see now. So some of these benefits well, the first one is data efficiency. And this is very important if we're planning to use our agents in real world where we don't have access to millions of frames as we do in the case of games. So if if we have the good model, that would mean that we'd have to interact less with the environment in order to get, a good policy. Then strong models will also allow us to adapt to unseen situations. So that is very important, if if anything is changing in the environment, for example, And also very important for safety reasons because if we have a strong model predicting if some action would be would lead to a unsafe outcome, that that means that we can avoid taking that that action to to learn, that would happen. So, this would this is quite important from a safety considerate. It will also allow us to account for, external factors such as human interactions. And in fact, this was already used in game training, alpha go, which you've probably heard about, and across the sciences, for example, for chemical synthesis. Planning has some, theoretical, results that that are very encouraging. So if we had the perfect model, this would allow us to plan for perfect policies, and this is quite important. One algorithm that is very useful for planning is value iteration. This is a dynamic programming algorithm, which is we'll see why why it's important in a bit, that can allow us to perfectly solve a reinforcement learning environment. So what this does is it, it computes values of states based on the values of neighboring states. And it starts with, for example, a random estimate and gradually improves this estimate until it reaches an optimal solution. So this is guaranteed to happen, which is very convenient. And if once we complete this optimal solution, then the optimal policy can be found just by taking the actions that maximize these values. So once we now now that we know about this very useful algorithm, well, we we we could think, what what can we do with it? So could we actually use it in a reinforcement learning environment? That would be a a great thing. Assuming for now, we'll assume that we have the reward function and the transition matrix. And in a bit, we'll talk about what what to do when we don't have these two things. So we assume the transition matrix is fixed and known. By known, for example, each state has no neighbors, up, down, left, right. The actions are deterministic, and computing value doing value iteration would amount computing sums of neighboring values. And this might already remind you of something that's that's familiar. So convolutional neural networks are quite well suited to computing this convolution like, computation. And this idea was, leveraged in value iteration networks, which got the best paper award in NeurIPS in twenty sixteen. So to to summarize, they assuming the MDP is discrete, fixed, and null, they perform value iteration computation by stacking, shared convolutional errors. And while this is great, it has the the downside that this is quite restricted in the sense the MDP needs to be discrete, fixed, and no. And Givens already take a step towards, more general cases extending to to the case where the the MDP doesn't have to be agreed for it. But it is still fixed and known. So in that case, like, so far, we didn't need to estimate the transition model. We didn't need to to find one that the the p would be, and we didn't have to deal with continuous state spaces either. But what would happen if we did another mdp? Well, we can think a bit, what would the human feature engineer do to use validation, which you've seen with a very useful algorithm. So the human feature engineer would take the game frame, the, which we call the natural input and compute some abstract input from it on which it they would be able to run the value iteration algorithm to get abstract outputs. However, we've seen that deep learning does a very good job when we don't have to manually compute features. So we'll we'll use some of this, some of this insight and have an encoder, that, bypasses this part of, human engineering. So this would stick our natural input, to an embedding. And that is great, but that that will still not be everything we need to run validation if we didn't have the transition matrix. For that, we can use a transition model, to build this local MDP that where we need in order to run value iteration. And this was already studied, quite a bit, and there exist many popular methods for for training this transition model. I'll just name one of them, which is what we used in our in our proposed agent. So contrastive learning, is is one way of training t. And this would be this would mean training t to discriminate between true next states and some fake next next states, some randomly sampled, s tilde. So if we have a traditional model that learned how to how the next state s primes will look like, this transition model then can then be used in a in a in a generative manner in a way to to build the the tree over which we had run value iteration. So if we have this tree, we have the p, but we still don't have the reward model. So for now, we'll assume that this is given. We'll assume that the nodes in it in the in the tree I I've previously shown are no. And that would be everything we need to run value iteration, directly over this tree. And this is exactly what was done in pre q n and value prediction networks. And to summarize their architecture, we have the state and the encoder, which is then, is then used repeatedly to build a tree. And from industry, the values are estimated in each of the nodes, and then this then this can be used for value iteration to to do the backup and and use this. So to to summarize a bit what I've talked about so far, we mapped our natural inputs to the space of abstract inputs using an encoder. Using a transition model, we build a local MDP, and we estimated the rewards in in each of the nodes in in the case of GQN. And this allowed us to run value iteration directly on this tree. And this is quite convenient because it aligns in the direction of implicit planners. That is a very neat direction. I I recommend you read more about, if you're if this sounds, like something that would be. However, doing this running value iteration directly on this tree, presents us with the algorithmic bottleneck. So real world data is often incredibly rich, and we still have to in the previous case, we still had to compress it to scalar values. So the the value iteration solver committed to using the scalar the model predicted and assumed it was perfect. And if there is insufficient data to estimate these scalars, then the algorithm is running on incorrect inputs. So incorrect outputs will very likely go out. So this means that we need to have correct inputs for the value iteration to do what we want, and we hit data efficiency problems again, which is precisely what planning was supposed to solve. So trying to avoid this bottleneck of the algorithm gives a perfect solution but in a suboptimal environment, we propose our agent exfiltrim, which is aimed at breaking the bottleneck . And, the main, idea behind it is that neural networks derive great flexibility from their late representations. They are inherently high dimensional, which means that if any part of that of that high dimensional vector is predicted poorly, then the rest of the vector can be can be used to to compensate and still lead us to a correct answer. So to break the bottleneck, we replace the value iteration solver, like, by we we train the value iteration directly on the scalars with the neural network. So that combination I was, I was mentioning earlier on. So what we'll do is we'll have a neural network between natural inputs and abstract inputs in that encoder, and we'll also have a neural network between our abstract inputs and abstract output instead of having the value iteration algorithm. How do we do that? We do it by pretraining a graph neural network to perform value iteration style computations, on synthetic graphs. So something we can easily generate. And then we freeze this. We keep this as a trained reasoning component and deploy it within our planner. And you might remember as value iteration is a dynamic programming algorithm, this is quite easy to do, thanks to this algorithmic alignment that, was pointed in in the in the I clear twenty paper. So putting all together, our Excel agent looks like this. We have the the state, our neural network, encoder taking to a latent representation, our transition model that is used to build that local MDP that in the case of value iteration networks was assumed no, but now we don't have, as as we won't have have in most of the reinforcement learning environments we're looking at. Then after we build this tree, we use our pretrained executor, that we pre trained on synthetic graphs. And we use this to predict values and actions and plug this into your reinforcement learning algorithm of choice, in our case, BPO. So to look a bit on the on the empirical side, our our agent is in green. We compare it with HCC, which is the the the model I talked about earlier, which, actually runs value iteration in scalar space rather than, in latent space, as Excel VIN does with our pre trained executor. And we also compare with a component that, is just model free, doesn't have, any planning component. So we see that usually, in in a lot of the cases, the XAV and the ATC do better than the than the PPO baseline. And something else that is very interesting to notice is the Excelfin does better than HCC, particularly in the first half of the training. So it does well in low data cases, which is one of the main things why we're looking at culling in the first place. So this is quite an encouraging result. To draw some conclusions, we we looked at we presented this this blueprint, that is applicable if we know a specific algorithm will be of interest for solving a natural, a real world task. So we, we see that real world solutions can benefit from combining classic algorithms with neural networks, and this is one way of going about it. Then we build on the on the findings that graph neural networks are particularly well suited to imitate some algorithms, in particular dynamic programming algorithms. And one example is backward forward and other is value iteration. Value iteration, which is, a very useful algorithm in reinforcement learning as it allows allows us to plan, and it can lead to to perfect plans. And using this, this component that learns the immediate value iteration in a, in a reinforcement learning agent, led us to find that it can, it can have great benefits. For example, having good performance in low data cases. Thank you for listening. Some of this, work was already presented in in some some nice articles. So, do check them out if you would like to hear to hear see more about this. And, of course, these were presented in in research papers, neuroalgosmic reasoning, again, neuroalgosmic reasoning as an inclusive planners. I'll also link them here if you'd like to read more about them. Last time, lastly, but not not least, I'm happy to take questions. And in general, if you if you have thoughts, feel free to send me an email. Thank you.",
    "transcript_length": 18054,
    "speaker": "Andreea Deac",
    "tags": [
      "bellman ford",
      "classical algorithms",
      "contrastive learning",
      "dynamic programming",
      "excelvin",
      "graph neural networks",
      "markov decision process",
      "multiple task learning",
      "neural algorithmic reasoning",
      "neural execution of graph algorithms"
    ]
  },
  {
    "title": "One graph to bind them all! Linking data & apps for event-driven interoperability",
    "description": "IT, and our world in general, is increasingly fraught with false binaries intended to divide us. Reifying data over applications, or vice-versa, simply reinforces silos and obscures holistic solutions. It\u2019s time for a unified approach!\n\nThe solution to increasingly fragmented business operations is a digital business platform that brings together data products and business capabilities. A unified interface provides a consistent experience, shared metadata & domain semantics, and common tools across teams so everyone can benefit, contribute and collaborate. After all, the digital business is about IT productivity, service velocity and business agility.\n\nThis session will demonstrate how EnterpriseWeb\u2019s platform provides an umbrella, graph-connected abstraction over diverse service elements, distributed endpoints and federated components to enable discovery, interoperability & automation.\n\nThe presentation will feature a short use-case demo. It will show how a no-code platform can leverage a graph language to rapidly model complex domains, onboard objects to a catalog, compose objects into services and chain services in to event-driven, end-to-end processes across business silos, ecosystem partners and cloud-services.",
    "category": "Knowledge Graphs",
    "transcript": "Hi. I'm Dave Duggal, the founder and CEO of Enterprise Web. I'm glad to be here with you today at CDW twenty one, to present my talk, One Graph to Bind Them All, Thinking Data and Apps for Venture and Interoperability and Automation. And I'll start by saying that, you know, One Graph to Bind Them All is a bit of clickbait, and I hope it brought you to this meeting, and created some, curiosity. But I wanna, you know, set the record straight. I am I'm not here to talk about a new form of, centralized monolith. I'm not here to tell you to burn down your, your legacy systems. What I I am here, to do is to propose the graph is actually an ideal way to coordinate end to end, processes in a distributed system in a way that traditional middleware and even cloud native tools are not capable. So let's start. So the problem statement we're going to address today might be a little bit different than some of the other talks at CDW. We're gonna focus today on business operations. And I think it's fair to say that today, in the world that's increasingly, you know, dynamic, distributed, and diverse, business operations are increasingly fragmented. It's difficult for businesses, organizations to coordinate across business silos, cloud hosts, and ecosystem partners. I don't think these are news flashes for anybody. I think this is a general experience. And, again, graphs are well suited for describing complex real world relationships. I think everyone at CDW already recognizes that, but their use has been generally limited in visualization, analysis, and interoperability of data. What we're gonna explore is the use of graphs to integrate data, applications, services, cloud hosts, networks, and physical devices. Essentially, not just look at data and properties, look at, data and functions. To look at, data and behaviors. So we're going to look at building information systems using graphs, entirely on graphs. So my company, company's no code platform, essentially makes it easy for customers to rapidly model complex distributed operational domains as a graph knowledge base an enterprise web. The scope is a bit bigger than what you might think of as a an a data fabric or a data mesh or virtualization of data, you many of you are gonna be familiar with. Here, we're talking about an umbrella abstraction that provides a unified interface for weaving distributed data and functions to a logical view with aggregated system wide entities, the index catalog of type domain objects. So a a broader scope, a a complete scope actually, allows us to take use data, to use graphs, data and functions to generate highly contextual, processes and services. So my thesis is that businesses need an enterprise web, a single source of truth across a complex distributed system. And what we're talking about here is a graph that helps organizations bring order to increasingly fragmented operations, higher level model, higher level abstraction. And this way they can share leverage, leverage shared metadata, relationships, and state to not just drive human and system discovery, but also declarative composition, intelligent orchestration, closed loop automation, and policy based management. So just think about how powerful a system would be if you actually had one pool of metadata and relationships implemented in a cloud native way that's fully distributable. So it's not, you know, centralized. It's not monolithic. It's fully distributable. And then you can actually have common pools of metadata and relationships and state information, to manage your operations. It would allow you to personalize customer experiences. It would allow you to synchronize your, operations and optimize your business transactions. So at the heart of enterprise web is our patented language. It's called the Graph Object in Action Language or BOAL, and it is a common machine readable language for non normalizing heterogeneous solution elements. And this is sort of the fundamental problem of the twenty first century, at least it's the software. But actually, since software is eating the world and everybody wants to be a digital business, it really applies to almost everyone. Is how do they connect their disparate systems, their disparate and distributed systems that are also changing very rapidly, evolving very rapidly? How How do they manage? How do they work across that? How do they connect across silos? How do they connect ecosystem partners? How do they work across clouds? How do you how do you work across this very fragmented environment where, you know, different endpoints are using different protocols and different formats and different schemas? And the answer really is high level abstraction. The user lies above this. So our language provides a graph data structure mapping properties, behaviors, dependencies, and constraints to a of of, solution elements to a graph knowledge base. So we're talking about a very rich graph, beyond data types, it's also functional types, which makes our language goal more than just, you know, a modeling tool, a domain modeling capability. It's a domain language, a DSL. It's also also gonna be modeling interfaces, so we can manage interoperability. Configuration language, so we can, you know, connect and configure, distributed, applications. And a workflow language so that we can actually use graphs to actually describe sets of tasks that are very dynamic, that they they have relationships, and they bind on bind on events and policies. So we're talking about a single language that's a DSL, an IDL, a CDL, and a WDL in one. My picture there is blocking that. So this is the conceptual architecture of Enterprise Web. We start with an upper ontology of system concepts. Right? And then enterprise web presents, customers with a baseline model that already understands, you know, generic enterprise concepts and, IT and cloud type or system types. And so we we give that to everybody out of the box. Right? We understand what people are and units are and facilities and locations and various aspects of, typical, enterprise operation. We also, of course, understand, you know, what, protocols are, formats are, types are, so that we can manage those as well. Then we have an ability for customers to model their own domain as a graph knowledge base. And, here what we're talking about is a knowledge base that not just describes their environment, but connects it to this higher level upper ontology, which includes the types that so they can actually generate implementations out of them. It can generate full blown interfaces out of them. It can actually, connect processes, can, you know, expose services using this system. So now we're talking about a graph knowledge base to model the domain connected to a rich system description of, distributed systems. And then at the bottom of this is a catalog of objects, the objects of the domain. Right? So you model your domain and then you have objects of your domain. These are actually the solution elements that are relevant to a particular customer. It might be applications, artifacts, systems, services, databases, devices. It doesn't really matter what they are. It's the things that that is that matter to that business that they need to connect. And we create make it easier for them to do that as well. So we're talking about a single a a graph approach to all three layers upper ontology to graph knowledge base, to also describing objects as so she abstract data types as, as, objects with relationships, concepts, types, and policies, which essentially map them back up to the domain, then in turn up to the, upper ontology. So but having a single language means that this could be highly efficient because at the end of the day, an object is a solution element that's mapped to graph knowledge base, which is also a graph, which is also match mapped up to the upper ontology, which is also a graph, which means that the whole thing is just a function of graph processing, and we essentially have one approach to graph processing the whole thing. That means we can do very efficient, real time, contextual processes where it might be much more difficult to do that given the latency with cloud native toolchains, which are middleware stacks. So to get into that a little bit deeper, for modeling the operational domain, first I said, we give you a baseline model out of the box. It includes generic device concepts and types. It's fully inspectable and extensible. Then customers can actually start by they can jump start their modeling by importing a model, like an XML, RDF, JSON, etcetera. They can even give us pseudo UML that might be in a Word document or a PDF document. And we can import those through algorithmic entity extraction, and map those to concepts in our upper ontology to essentially see the graph knowledge base. And we do that in seconds. And, from there, solution architects can continue by manually curating the graph knowledge base, right? They can tune it, they can extend it, they can modify it as need be for their operational domain. And another way to get model information into the system is actually by onboarding solution elements themselves because as you onboard more elements and as your as your use cases expand, you're effectively going to be expanding your domain as well. Right? So that leads to this row as you go concept. Because everything is done in graphs. Graphs are optimally situated not only for flexibly modeling relationships, but, you know, with graph, we all know that graphs can actually be modified much more easily. Static hierarchical models, right, they're easier to evolve over time. In enterprise web, everything also happens to be immutable, so we version control everything. We're actually version controlling the the objects in the catalog. Version controlling the graph knowledge base. We're also version controlling the upper ontology as we put out new releases. So everything is version controlled. Everything has audit history. And this all allows you to essentially model the use, start with maybe a narrow use case, implement that, and as you succeed, expand the use case, add more use cases, and evolve with your, domain. These are all challenges in a hard coded world that I think, many of you probably recognize. So, as we when we talk a little bit deeper into object modeling itself, now we're actually talking about how we onboard a singular, you know, it's an application, maybe it's a service, maybe we wanna onboard. What we actually already have, we give customers a catalog of a couple hundred existing, objects like mappings to Amazon cloud or Google cloud, etcetera. We have mappings for a lot of common, you know, cloud native tools and, etcetera, Kafka, MariaDB, other things like that all out of the box. But, you know, customers can also model their own, their own solution elements. And essentially what they're going to do is whether it's a code package, it could be an application, a function, an algorithm, or it could be a third party orchestrator, a controller, a system, a database. We can model adapters using enterprise web. Or if it's an endpoint like an actual service, that somebody wants to model. All of those can be modeled in enterprise web. Essentially, they're going to become typed objects, and the method is effectively the same. Essentially, there's an interactive API or a dynamic form. When you're looking to onboard something, the system essentially gonna present you with cyclically, either a UI driven wizard or interactive API that's gonna look for you to enter information that's gonna essentially progressively type the, object, the solution element that you're looking to onboard. And as you do that, the system's gonna be, in it's the type system that's gonna be interacting with the graph knowledge base in the upper ontology to autofill properties and behaviors that the system already knows Because the presumption is the graph knowledge base is in place, the graph knowledge is already in place. So we already know a lot of things about this domain. So we know a lot of things about different kinds of types. So we can auto fill the properties and generate very rich interfaces. So taking, thing things that might, onboarding things that might have taken, you know, days or weeks. We can, you know, even really complex things, and we can do it in in its towers. So we're really trying to simplify the modeling exercise itself at the graph knowledge base level as well as the onboarding level as well. Everything, like I said, is persisted in a backing store. It's an immutable log style append only storage. It's cloud native in its nature. Right? All the objects in Enterprise Web are dynamically indexed, tagged in version control. So all those things are just done as, in the background for the customers. You don't have to worry about those concepts. Essentially, we're, abstracting storage from them. So, we're also providing a process that facilitates updates and upgrades because all the objects that you onboard, all the solution elements that you onboard are gonna be version control, can anticipate that they're gonna change over time as different partners and vendors, etcetera, update their service endpoints or their application code or, or you update a technology somewhere or change a protocol. When it's time to update that, you just go back to that object, you update it, the version controlling it will take the new information, we'll put that new information, and yet we'll also preserve the history so that we know all changes to that object is the time and who made those changes. So this is what Enterprise Web looks like as a platform. Right? So we take that architecture, which I thought would be most familiar to many of you given interest in the type of people who attend a CDW meeting. So we focused on the graph architecture, but essentially we're wrapping that with the capabilities of an information system, right? There's, you know, object modeling as I just described. This is a design environment for declarative composition. So once we have objects, go to the catalog to carefully compose objects into, services, and then we can chain services into event driven processes, all with no code. Right? We're using the metadata and the, relationships, domain semantics as well, from the graph knowledge base and the upper ontology, to, compose all of these things so that we can do them with no code. And of course, the system also offers a runtime. Right? It's a cloud native, asynchronous, concurrent, so it's massively parallel, event driven system. It's reliable messaging, transaction guarantees, and state management all again included just like we abstract the storage we're actually extracting a lot of the complexity of being cloud native itself. Right? And this is a real struggle, even for advanced engineers. They're largely doing these implementing these ideas like reliable messaging, transaction guarantees, state management on a per solution basis. And here we're providing it on a platform basis. So it's available, you know, just as, that's something that everybody can rely on in the background. We're also wrapping it with platform services. Essentially we're replacing if you look at this and you'll, see a lot of common terms here, things that you would expect in a common middleware stack. We're essentially replacing the middleware stack with a set of serverless patterns. Right? Serverless middleware. That's what this really is. Instead of having different components, each have to implement and then manually integrate, and then then you have to write your code your applications to the top. Here, enterprise web says, you know what? Actually, no. Model everything declaratively, and then either the type system will attach behavior should background, or, depending on the use case, you might need to specify a certain behavior, and you then can call these functions directly. But what you're getting is this very lightweight, low latency, high performance middleware that's all served up in a cloud native cloud. So, this is, you know, our enterprise web platform is served at of fifteen awarded patents, and is deployed around the world with, you know, multiple variety of industries from telecom to life sciences. Now I'm gonna go to a concrete use case. It's an SAP use case. It's the SAP is the world's largest enterprise software company, so I'm sure many of you are familiar familiar with them or, if not, directly experienced with working for SAP. And, of course, they have great products, but like with any large company offering a wide variety of solutions, whether that's Salesforce or Oracle or, or Amazon or Google, for that matter, it's not always easy to work across all of these. Right? It's, they weren't all architected together. They have discrete sets of tools and products. And, you know, often today, we're not just connecting, one vendor's products and services. We're actually connecting them with, SAP and non SAP enterprise. So this use case is gonna explore that. So, SAP customers nowadays, like I said, have to struggle when confronted with an increasingly complex array of SAP and non SAP endpoints. So they need to integrate, orchestrate, manage, and maintain across a set of them. Now SAP has a lot of great assets, a lot of great development assets, right? They have this, thing called the Graph API, which is the One Domain Model with virtual data model. They have these TIG files called the IMG SCRO. They've got a lot of great assets that we can leverage, but in and of themselves, those assets aren't fully connected. Right? They lack the semantics to fully connect a solution in the kind of way that I'm describing to you. And solution delivery over SAP at this time still requires a mix of custom code, manual integration, BPMN workflow modeling, interface development, and complex configurations that are gonna be done with a mix of model and code. And, of course, the problem is is once you've done a mix of model and code, the model's not driving everything, and that means that you're gonna have these black holes where you don't know where things break, and you're not gonna have end to end transparency. Distributed systems, you really want that. You wanna know where things go wrong. You wanna know how your services are being consumed. Right? You wanna be able to analyze that for security. You wanna analyze that for optimization. You want to make sure that your customer is having great, experiences and that you're supporting your SLAs. So transparency is non trivial. So, let's look at a service, right? This is sort of a, you know, in the process world, they would call something like this, like a three byte kebab or a five byte kebab, which is the industry kind of jargon or describing a simplistic process. But, you know, there's gonna be a third party customer portal where somebody's gonna order a cloud application. Right? So this is gonna be a a a cloud native or a digital business use case. Somebody's gonna actually order something that is in effect, a digital product, a cloud application, And the delivery will also then be a system delivery. Right? It's gonna be it's not it's not selling jeans or a car. We're selling something that's, completely, like, visual end to end. So and then delivery is gonna be done, over, Google in this case. And so we're gonna have a third party product, and a third party delivery system, but the, ERP type services in between will all be SAP. So what we want is some sort of model connect across those, because without that model, we're gonna be doing manually integrating, and that's all we have to do. And especially since, you know, these aren't all fully connected even the SAP products aren't fully connected by a model and let alone connecting across the others. So instead of, you know, writing interfaces, doing manual integration, doing manual code, and then, of course, maintaining all that coding and recoding, integrating and reintegrating as things change, because if any one of these elements change, your solution grows. We're gonna do this all in a model. We're gonna do this with Enterprise Web. So here now you see Enterprise Web over the top. It's providing the abstraction layer I described, the middleware services, right, called the platform services, and the cloud native platform runtime. So we're not it's not gonna happen magically, right, we're gonna leverage those SAP developer assets to begin with, right? So we're gonna take those assets that I told you that were disconnected, and we're gonna connect them. We're gonna create one unified graph SAP domain, and we're gonna do it in minutes, and, we're gonna then show it to you. And, we're gonna so we're we don't have to throw away assets. That would be silly. We wanna leverage those assets, because they're good, so it it streamlines and accelerates our solution delivery. So we'd be fools not to, leverage the existing, documentation and models, that partners provide. So in this case, you know, an architecture architect can call these APIs, call this documentation, they're going to import them into the system, the system is going to do the entity extraction, and it's essentially going to set up that that initial, graph knowledge base, right? It's gonna set up your, graph operational domain. It's gonna generate generate a unified graph connected SAP domain model mapped to our upper ontology just as I described. And by that mapping of the SAP domain, which by itself wouldn't be connected to the enterprise web of ontology, where it's getting it's being wrapped essentially in additional concepts, types, and policies, which enable the declarative composition and the intelligent orchestration. From there, with, now that we've onboarded the models, we're gonna onboard these objects. Right? We're gonna onboard these, domain objects, which is the service themselves. Now at some level already, since we have Graph Knowledge Base, and we import when we imported the SAP assets, we learned about their objects, their interfaces, their properties, and behaviors. We're already gonna be able to generate a lot of this information here as far as these three services are concerned. So even a lot of that work is gonna be done. We're even gonna generate, configs and things, for these, out of the documentation that was provided by SAP. So, again, we're always seeking to leverage things that exist so that we can accelerate solution delivery and that put people in this sort of very agile graph domain model. And the last step, in this case was because we also talked about processes. Now, we could model from scratch an enterprise web a process. It's essentially an enterprise web. Processes are data flows. A data flow is, by the way, is a graph. Right? It's a natural graph. Right? An enterprise web, a process, the definition of a process is a set of tasks with relationships to events and policies. So there are the tasks then bind. It's not a static flowchart. This the tasks bind based on conditions, which is very exciting. It allows you to have, really, creative, policy based, process, processes, event driven processes that can respond to a lot of variance, a lot of what you would call in a traditional flowchart exception paths, which become very difficult to maintain, will make that much easier. In this case, actually, though, we're just going to import SAP actually had a log, called Action Logs. We're going to import the Action Log, extract the logic, and map it back up to the objects in the model, generate an event driven data flow process with all the service integration points and the corresponding UIs completed out of the box on import within seconds. Again, leveraging assets to great effect to accelerate solution delivery. If you do everything manually into enterprise web, but where you have assets, leverage them. The point here at at its core, enterprise web is a no code integration automation platform. It's completely open for federation and extension, so we're creating this environment so that you can onboard whatever you need to onboard into the domain as you describe it, and we're just trying to make it easy for you to have that domain and use and share that metadata. So, we're gonna go to I'm gonna pass the baton here into a demo. And in a second, my colleague, Bill, is a chief assistant architect. He's gonna provide the demo. He's gonna do that use case that I just described. He's essentially gonna walk it backwards. He's gonna show you the process that we generated. He's gonna run the process, which will be a mix of system steps. And he's gonna drill into the process tasks, and he's gonna show you how it was connected by metadata. You're gonna see the objects being referenced. He's Just gonna click into an object. You're gonna see all of that metadata. You're gonna see how the meta that object is mapped up to the knowledge base, to the SAP domain. You're gonna see how the SAP domain is actually mapped up to the upper ontology, and you're gonna be literally be able to walk that graph seamlessly. If you and if you have the right permissions, you could just do that as Bill Kent will do that in the demo. So I hope you enjoy the demo. I hope you enjoy the presentation . Thank you very much, and have a great",
    "transcript_length": 25356,
    "speaker": "Dave Duggal",
    "tags": [
      "action logs",
      "boal",
      "bpmn",
      "cloud native",
      "data fabric",
      "data mesh",
      "distributed systems",
      "enterprise web",
      "event driven processes",
      "graph"
    ]
  },
  {
    "title": "Protecting vital public health programs with AI and Knowledge Graphs",
    "description": "Health and social services are complex domains that have a direct impact on people's lives and where vast amounts of money are spent globally.\n\nWhen funding intended for public health programs is lost to Fraud Waste and Abuse, vulnerable citizens are ultimately the victims. In challenging times, ensuring financial integrity and fairer distribution of services by reducing disparities are among the top priorities for healthcare systems.\n\nIn this talk, I will provide a perspective from my journey in adopting research in Knowledge Graphs and AI to address significant societal problems in the healthcare industry.\n\nIn particular, Knowledge Graphs emerged as a unifying technology that facilitates bringing diverse data sources together to unlock new knowledge and empower professionals to reduce healthcare disparities.\n\nThrough a combination of natural language understanding, deep learning, and knowledge representation for modelling human-expertise and reasoning, we are investigating unique functionalities to automatically extract actionable knowledge from large text policy documents.\n\nWe identify medical claims that infringe policy, either intentionally (fraudulent) or unintentionally (e.g., providing unnecessary services or inconsistent with accepted medical practices).\n\nWe aim to understand the social program levers that drive positive health.",
    "category": "Knowledge Graphs",
    "transcript": "Welcome to the final session of the morning, before the lunch break, with, Vanessa Lopez of IBM Research Ireland. Vanessa's research interests are to investigate and envision technologies to better understand human needs and support us as a society to target complex problems in the health and social care domain, in particular using a combination of semantic, natural language, and learning technologies to capture, integrate, search, and query diverse data and apply it to solve real challenges, like for integrated care, to support the care of the most vulnerable citizens, and to ultimately obtain, better outcomes. And Vanessa is the, research scientist and manager, within the, AI for health and social care, area of IBM. So a a very warm welcome to, Vanessa and, and over to you. Hi, everybody. Thank you so much for the nice intro. So, yeah, I am a resource scientist and manager for the last eight years at IBM Research in the lab in Ireland. And people here have, lots of different backgrounds. Mine is actually academic. I used to be in the Open University in KMI. But, since I am in IBM in our team, we have done a lot of work on applying research to the integrated health and social care domain. So I'm really glad to be here to share with you this research work that we have done jointly with my team at, IBM Research Europe and with IBM Watson Health. Both teams are collocated in Dublin, and we have a mighty common mission, which is to bring AI to help transform how the health care systems of the future will grow. The group peak here is obviously from the old days where we could get together in person. And here, I'll talk, miss mostly about my experience and perspective perspectives on using knowledge graph to address significant societal problems in the health care industry in two projects. One that we work on for almost three years to combat fraud, waste, and abuse to protect people public health problems, and a second new or exploratory one to understand the impact of social problems to drive positive health. There are not yet AI strong offerings in the human and health services space. And this is because while everyone thinks that this is a real competitive differentiator, bringing AI innovations is really hard. And embedding AI requires longer efforts than classical product timelines. It requires validating what is feasible, what is the impact of false negatives and false positive, understand the customization effort, and the time to market to market assistance. So it requires a little bit less, and it needs to be driven very much by the expertise from the subject matter experts. Those are our SMEs. And our SMEs in particular, they work with the government, and they bring experience from the Medicare and the Medicaid health programs in the US. So before I deep dive, one may ask why Knowledge Graph. So health care and program integrity in particular, they are very complex domains where science and AI can really make a difference. It has a direct impact on people's life, and huge amounts of money are spent globally. There have been actually lots of advancements on the scientific community by building models of disease, progression, and outcomes. However, the impact of those models has been somehow limited. Maybe maybe because the models are silo in the source of data, they are silo in the specific use case or in a specific disease, and isolated models provide isolated predictions. And they do not support very well the comprehensive insights that you need behind complex decisions and the best practice patterns among health care providers and payers that remain hidden in the data. So analyst graph emerge as a unifying technology that may facilitate creating interconnected models so we get better at characterizing and predicting. Our long term goal is to empower care professionals with augmented AI so they can ultimately improve patient outcomes at a sustainable cost, which is a very ambitious goal. And this talk is about our journey at IBM Research. And as a spoiler, we are still far from solving many of the challenges, starting with proving how well it can scale in a real world environment. So a top priority by the World Health Organization is to reduce disparities on the delivery of health care and to ensure a fair distribution of funds, that there is enough for everyone in need to get the services that they are entitled for. Even more at this COVID time when the needs are so great and the funding is stretched, it is key that the money intended for vital public health programs are not great by intentional fraud or by unintentionally providing services that are unnecessary or they are inconsistent with good medical practices or in the worst case, even harmful to patients. So just to make clear that program integrity is not about limiting funding. It is about making sure that the money goes where needed. At to combat fraud, waste, and abuse is a huge market. In the US alone, health care fraud losses are in the tens of billions each year. These are shocking amounts because of bus toasters and bus providers, and these numbers indicate a significant market opportunity. So in this scenario, the providers rendering services to patient submit claims to the state health programs, such as Medicare and Medicaid so they can get reimbursed. And the payers know how much the providers are spending, but they don't know if this is in compliance to regulations which are described in policy or how much is lost to fraud, waste, and abuse. So the investigation units, they validate the integrity of these claims submitted by providers so they can recover overpayments for claims that infringe policy. This is a labor intense and an error prone task. To build a case, the investigators had to review the policy, and then they had to talk to the data analyst to query the claim data, and they go back and forth till they find what they need. They manually encode some algorithms on top of the claim data, but most of the stuff in the policy, it doesn't get automated. So due to the overwhelming volume of policies and claims, most of the systematic fraud or waste is never surfaced. So the policy is written down in very large text documents that is describing what the providers should be doing, And all information about what providers are actually doing is in the structured claim data. So state of the art fraud, waste, and abuse detection is data driven on top of the structured claims, for example, to identify providers with anomalous patterns. But being another layer does not always mean a provider is fraudulent. And even if so, it is not enough to know a provider is fraudulent. Investigators need to tie the clients for this provider to the policy legislation to build a case. And these efforts are not always successful. The policy may turn out to be too vague to be enforced, and providers are confused about what they should be doing on the recoverables recoverable amounts too little to warrant any action. So we thought it will be very powerful to bring this together, and we are the first ones to automate the process of identifying claims at risk based on policy text. So the unique value lies first on using NLP and AI for the automatic extraction of the actionable knowledge from the policy. It converts the policy text into a knowledge base of benefits rules that can be directly applied to review claims against policy. And these human and machine consumable rules, they provide a common interpretation of the policy, which can be shared across the team and investigators. And that that allows a more thorough, transparent, and consistent review. So this benefit benefit rules that are extracted from policies, they are reviewed by the SMEs, our subject matter experts, before they execute the mobile claims. And this is a significant differentiator. It empowers the investigators to size and prioritize the investigations they may follow based on the evidence and on the likelihood of recovery from funds and as well on the policy by executing those policy rules on top of the claims. And a second big piece is explainability. Each label claim at risk is linked to the benefit rule alongside the natural language part from the policy text that explains what is the violation . So there are many challenges behind automating the claim processing from policy at the scale. And a core challenge is to capture the knowledge from the text and to model the domain. So at the center, there is an ontology, which is an evolving asset that we created manually with our experts to bring to bridge this conceptualization gap on how the experts think about the policy and how the data is storing the claims. The ontology captures the terminology, and it makes explicit complex relations that we are interested on, so it guides destruction. Another challenge is that we didn't have any label data to start with. So an important achievement was engagement and the validation with the users early in the development. So we build basic UI tools to interact and gather feedback and ground truth data from the experts and to learn from that human feedback. Lastly, we need to connect the policy information to the structured claim data in order to reason over claims, to quantify, and explain the violations. So, overall, this is a new way that we are proposing to consume rules from policy. And to do so, we have to work very closely with our policy as person. What are the type of rules most suitable and of high value? And this is just here a simple example from a dental policy on a service limitation type of rule on the number of units a provider can bill for a given service on a given patient over a period of times. So there are many ways ways in which the policy text may describe this criteria, but with the with the same underlying semantics. So after ingesting the PDF policy, the second step is to automatically identify the concepts and relations that are expressing the sentence and to abstract that into common reusable patterns or structure templates of benefits rules as they are modeling the ontology, like capturing for which services, whether they qualify in eligible members, whether the limits, are there any other requirements or exclusions. A knowledge graph is then automatically populated with this knowledge funded policy. And because automatic knowledge extraction is really far from perfect, human AI collaboration is crucial for experts to review and validate the strategic rules. Once that's done, the last step connects the policy information to the structured claim data. This is a mapping and configurable step that maps conditions and values from the ontology to column values on the claims. So it can execute on top of claims and label the claims that are valid or at risk depending on the type of rule. So this is just to impress with some boxes, but no worries. I won't go into detail. Yes. To give a flavor here that there are lots of moving parts behind an AI system. And these are components that we build on top of existing technology, which combines NLP, Now let's reason in so we cannot put the knowledge base of benefits rules from the policy and that the users can review. And from them, we we can learn from that feedback based on deep learning approaches. So the instruction improves over time. Here, as a sample on the knowledge extraction, this is built on complementary different technologies, assistant to perform various NLP tasks, like post tagging, ontology based annotations, and to strat functional dependencies in text. We you we do that based on semantic role labeling functionality, like what is the action, who is the agent, the theme of the context, and also dependency trees. These are very useful representations for relation extraction as they collect entities in a sentence through label edges. In some, different strategies are used to extract those potentially relevant textual pot patterns and expose predicate argument structures from the sentence. And from those, we build a semantic graph based not just of concurrency of entities in the sentence, but how the terms are linguistically and semantically connected. So all domain information is inside ontology, and this is a a sample of the subset of the ontology that represents the scheme of the rule described in the sample, which is meant to model a maximum reimburse amount for a service for certain members during a certain period. And OA is flexible. It does not impose a fixed template, but the main constraints can be defined to ensure semantically meaningful benefit rules. Like, for example, this joint constraint that tells that a service cannot be both a cover applicable service and an excluded service in the same benefit rule. So the goal is to populate the ontology with a knowledge graph with the benefit rules, if any, extracted from the text. So after matching at the level of entities to classes, instances, properties, or data types, the system search for matches at the level of patterns, transforming the text all dependencies into graph patterns in a knowledge graph. By reasoning over the anthology, it can infer implicit relations and a path connecting the entities. And this is a simple example where we have thousand dollars, which is annotated as a monetary amount, and up to, which matches an ontological property, which expected range is monetary amount. So they are linguistically connected in the sentence, and we can also find a path that semantically connects them to build a knowledge graph. So that's the idea behind. Right? And the results is an olive graph that in all information needed for these benefit rules attached to this sentence. Now the goal is not just a distraction, but to empower the investigators to interact with the tool and to build trust in the AI. So we needed to hide the complexity behind this knowledge graph and present an unambiguous flat representation of a benefit rule. And on top of those, that user friendly representation, the user can correct any errors. And through these validations, they are incrementally building institutionally institutional knowledge based on policy rules that we can also use as a ground truth for evaluation purposes on one hand. So we can evaluate in terms of precision and recall and publish results in conferences. But, also, as we work with SMEs and as this shared knowledge base is created, the question becomes, how can we learn from this human feedback to predict benefit rules in a nondeterministic way? So we use the ground truth data to fine tune transfer language models such as PERT. Now learning from a small grown small ground truth data is challenging, so we combine knowledge graph with this deep learning approaches, enhanced with semantic embeddings with two goals. One is classifying with greater accuracy the sections in the policy that are most likely relevant with the benefit rule and also to predict the spans in a benefit rule. So to be able to annotate unseen mentions of entities and relations and to annotate them directly with relevant relation. So it is not just saying that, let's say, an oral evaluation in the sample is a service, but to say that in the context of this sentence, it has two roles. It can be an applicable service payable once every three years, or it can be a nonreimbursable service if given at the same date and other listed reimbursable services. So you can see here, the structure is quite complex. So it is the learning. So this is work in progress, but we are getting preliminary, potentially good results. So now to combat fraud, waste, and abuse is a huge market, and it is hard for humans to scale. But machines can do it if we capture the right knowledge and the sophistication in the models. So to prove the feasibility and validate the results, we build call to standards with our investigators. They manually select it and hand coded a set of twenty representative algorithms that correspond to some of our policy rules. It is a very limited set for now because encoding its algorithm to build the voltage standard, it takes precious investigations investigators' time. As for those, the system can automatically take the validated rules to label claims and quantify funds at risk with the same accuracy, same precision recall of our SMEs on top of a hundred million claims. And this validates the semantics behind the model. See, for example, rule seven on the table, it has a relatively small number of claims at risk, but they accounted for very large amounts of money and funds potentially recoverable. In fact, out of a hundred million claims, we can identify almost five hundred thousand claims at risk. That is point zero five percent of the claims for a total of twenty nine million value. This is pulling needles out of a haystack. This twenty nine million is from a small set of rules selected for evaluation purposes, but we have hundreds of policies containing hundreds of rules across areas like physical therapy, medical equipment, behavioral and mental health, vaccines, you name it. So the good thing is that the execution of claims is configurable and transferable. So there is no need to write a unique algorithm to cover each rule. Given the same claims as schema, the rule is executed on the same way across benefit rules. And that's another advantage. Yes. So the investigators can approach investigation from multiple angles. They can search the policy, relating to suspiciously high bill services, and they can see the payment rules that have been extracted on what policy says. And they can review them and publish them into a dashboard to immediately visual visualize how the data looks like based on those rules and the impact in terms of invalid claims and amount of money. And underneath all that, the ontology, what it does, it provides a shared interpretation of the policy and makes this process of rule creation more transparent and comprehensible for machine and humans. This increases the productivity, basically. It also provides an abstraction that allow us to reuse what we learn in one place to somewhere else and minimizes the gap between the policy intent with inter protection interpretation and the execution, which increase the confidence in the rules and the consistency of how they are playing. So we are aiming to capture more and more diverse rules over time and to interpret more complex requirements, like if a service was medical medically necessary that requires linking health records to claims. And there are many opportunities to apply this technology in other scenarios, perhaps to assist pro assist providers to know what they need to attach to claims before submission so they don't get rejected by the payers because of missing information or to identify gaps in policy so we can improve the communication between payers and providers. And now let me just briefly move to other cool exploratory work in progress. This is on using knowledge graph as a foundation for understanding the impact of social determinants of class. This is a use case example that we published very recently on exploring social drivers of health during COVID by leveraging knowledge graph and population trends. But first, what are social determinants? So those are all the things that are in the fringe of the traditional health care system, but that have a major influence on health and are hugely important to determine the treatments with successful outcomes. Take, for example, homeless of poverty. So they have an impact on admissions to hospital and the complications of a disease and are often a hidden driver of cost behind vulnerable population. There are many examples in the news. Here is million dollar married. He was a homeless alcoholic man in Nevada. And for ten years, he was on the streets, and he ran a medical bill larger than anyone in Nevada. And it is one of the samples why social problems may be cheaper to solve than to ignore. It costs one million dollars not to do something about malware. And despite research showing this strong link between understanding social determinants and health outcomes, there is a gap in the market for orphanings that work with unstructured tests and that cover social determinants of health. Our goal, not there yet, but in the past too, is to bring understanding and evidence on what is the impact of tackling social aspects to improve health outcomes and to reduce the cost on populations at risk. Now when when we think about COVID, we first think about the disease, the symptoms, how it spread, how to cure it, or even the global economic crisis. But there are also significant societal consequences. COVID basically escalates the need of the vulnerable populations, and the social problems are made worse because of it. So what we are looking at here is perhaps the less study social and other health concerns surrounding the pandemic. Take food insecurity. You may think of, several areas where people have not access to food, or you have you have a Spanish mama that will tell you about children in Africa at every meal time. But in the US, food insecurity is also a big issue. People that may live in food deserts where they only have access to McDonald's and fresh, healthy, real food is unaffordable for them. So our contribution in this paper was to build a proof of concept where we monitor concerning trends in social determinants that are arising from COVID and link them to other potential health and social issues to identify those populations that are vulnerable or that are at risk of becoming vulnerable during the pandemic. Now differently from clinical data where we have many standards and terminology, there is not yet a taxonomy for social determinants of health, but there is an ongoing processes. It is really a big effort because the domain is really broad. So for this purpose, for evidence discovery relation evidence discovery, we structured associations from PubMed between social and clinical terms, and we mined those relations into a knowledge graph. To start with, we look at the social determinants that were trending high and which ones are trending higher when COVID. In particular, we look at Google Trends to get a glimpse of the social concerns on people's mind during COVID. We found trends like jobless, food insecurity, and shortage, which coincides with the lockdown and thus hardly surprising. So in a second step, we look at for those social trends of interest, like food insecurity in scientific literature, particularly in PubMed abstract, to mine the evidence that they could affect people's health. So we tag the text in this abstract with a tool called MetaMap to highlight concepts clinical concepts. And once we have those sentences containing both food insecurity and other clinical or social related issue, we use a birth based classifier to annotate the sentence as is it it has a positive association or a negative one or no correlation or something more complex. So for example, food insecurity is positively associated with h b a one c. That is one of the markets for diabetes. So we capture those into a knowledge graph to build our first catalog. And this is this is a graph between food insecurity and unemployment with the most relevant neighbors, like anxiety, obesity, depression, and a bit of noise too. So we verify some of these terms like obesity and coping, and we also find higher trends in Google Earth, which show that we are able to capture relevant relations between the social and the health. Now, current and future work is to keep improving this relation identification to catch more complex relations and not ratios so we can assign weight to the ages. And the value of this knowledge graph is, in fact, to bring it together with clinical records or population records to create foundations for new analytic offerings offerings and to be able to do further research as to quantify the impact of these relations in clinical outcomes and to improve current test risk models and identify, for example, what kind of interventions we could have for those high cost high needs segments of the population. In my experience, using ontologies and knowledge graph, they help us really to develop technologies that are able to work with an open set of features and that they can be reused and they can scale to broad domain knowledge such as including health records, claims, social determinants, and so on. So we can build models that are more human understandable and machine consumable. And just to finish, I hope I convinced you that health care is one of those areas where AI and knowledge can really have major impact and that there are lots of open challenges to keep researchers happy. But it is in particular time that we had a conversation about what real benefits users final users get from the AI systems and to start designing system for the users rather than assuming that a high accuracy is sufficient for them. We need to define and measure impact, and that requires finding this wow moment where you are able to go to a user and tell the story so you engage them since the early stages. In my view, there are many benefits, at least in this domain. Like, for example, care professionals turnover turnover is high, knowledgeable experts are in short supply, and there is a need to share and scale expertise for which knowledge is a good tool. There is also an aspiration to support the experts with explainable and concise concise evidence on the point of care to back up their decisions. Like, whatever it is recommending interventions or whatever is identifying ways or improving policy to reduce health care disparities. Like, for example, you can give fifty blood text analysis to someone per year, but that won't make a difference in any outcome. And the evidence is there in the data. But in some, it is not much about what technology is underneath, but about empowering the health care agencies and the governments to reboot society with a better way to approach health care and a more adaptive post COVID health system. And that's the end of my presentation. I think, there's some questions coming through at the moment. I can see, a couple of people typing. I'll just read the first one out. It appears you use OWL, and you have complex and explicit SME rules. Question, are you using OWL DL? Which rules engine do you use? How do you translate rules implicit in the ontology to those you run against the knowledge graph? Question from people. Yeah. Yeah. That that is, that is a very good question. I didn't cover that. And, yes, we are using actually all, behind, so that's how we are defining the ontology. But then what we do is transform this knowledge graph into a flat representation that we store actually in a post in a Mongo, sorry, in a MongoDB. And it's a simple representation where you show just the conditions and the values so that the SMEs can in a very easy way, with their own terminology that we actually define in ontology, curate those those rules. And then what happened is is these conditions and these values, they are mapped for to claims, columns and to claims values. Right? So and this is happening in odontology as well. That's domain knowledge that we live in odontology saying, for example, that the service about topical fluoride describing the policy corresponds to codes zero zero one and codes zero zero two as they are billed on the claims. So those that linkage information is behind the syntactology, and we use that to basically send a set of queries. So we aggregate the claims. We filter claims based on the service or based on the period of time. We apply some operators like you have to be between this h and this h and so on. So then that executes, and the selected claims, they are marked as valid or as invalid depending on the type of rule. So it's it's a deterministic approach. What we are now investigating as well is to be to use a probabilistic reasoning engine. So to be able to execute that knowledge even in the cases where maybe we are not fully sure or fully confident on the rule. So there may be rules that we have a very high confidence that they are accurate because they were validated with by the users, all the stuff are not. So a probabilistic engine will be a good tool to to handle those cases, but that's that's currently work in progress. So integrating a rule engine is is work in progress, and we'll see what results we get. Cool. Interesting. Thanks for that. Whilst there's a couple of people typing questions, Keith says thank you, just whilst that's happening, one of the questions that came up in, some of the workshop shop sessions was around I don't know whether you you may be able to answer this is, if someone was starting to look at open source biomedical ontologies, I guess, what common ontologies there are that you would recommend working with? Yeah. So we have, work a lot with ICD ten. It's a very medical medical terminology for, you know, all these, like, symptoms, diagnosis, and so on. We work a lot as well with UMLS, which is a unifying medical language terminology that gives you also a lot of information about health care services, a lot of information about diagnosis. Like, for example, you are a high risk of cancer or highest risk of something else, information about treatments and so on. There are also ontologies for procedure codes as they are bill on the provider claims. And those are the ones that we are mostly working on, but, is, yeah, it's it's great to see all these anthologies that are appearing there every every day, basically. There are more and more. Cool. Thanks, Vanessa. That's really helpful. So we'll just give it a second. I can see Douglas Moore is typing. So just to see if we can make for his questions to come through. And then if that doesn't come through, then, we'll probably break for lunch. Of course, any anyone that has any follow-up questions, please do, link up with Vanessa on the on the Slack channel. You can you can ask them there. Here we are. Couple of questions. So was the claims processing, a point solution for validating claims, or has the effort to map source data to greater benefits? Not sure if So it seems to be a question about the claim processing. I'm not so sure if I'm not gonna answer this question correctly. But, so the claim processing is something that is happening already on the state of the art, and in particular for fraud, waste, and abuse detection approaches. Right? They look at outliers. They look at calculating the risk of providers and so on. So the claim data is usually in a very well structured format. Right? And in some cases, it's also linked with health records that give you extra information. Like, you can look at the care pathway of a person, the outcomes, and so on. What we try to do here is a bit different because we want to understand the claims, not by looking only at the claim data, but by understanding the policy. If the policy says you can't do this and you cannot do that, and then to see if we can execute that on the claim of data to see actually what claims they don't seem to be doing that. So that gives you an indication to the investigator to have a look and to explore more and to create. If that is the case that they are really, infringing policy, they can build a case. And in order to build a case, they had to tell. This is a policy lay legislation that these claims are breaking. I'm not sure if I answered that question well. Yeah. I think, I I think that that, answered that. And then there's another comment, from Frederick Frederick, Landquist, using mesh UMLS ICD, etcetera, medical vocabularies. But have you also created one context specific for the speaking about the domain and mapped this with the other large vocabularies? Yeah. Yeah. That's correct. That's the case. So we actually created an ontology manually with the domain as per us. So we can this ontology is the the one that is guiding the knowledge extraction, and it's basically focusing on things that are actionable. So I care about these entities and these relations because these are the things that I can then go to the claim data and see if something is compliant or not. So that ontology was direct was created within that schema. And then, however, there is a lot of instance data. And for that, we use other sources. There is a lot of tabular tabular data. For example, it thinks about describing places of service. Right? You can give this service in the of in the hospital, or you can give this service in this condition. So all those codes and all those sources, we list them into the ontology, just by creating some mappings as in as in fancy. And we also, through the ontology, links to those other terminologies. So for example, there may be an eligibility criteria that says, in order for you to get topical fluoride, you need to be a high risk of caries or cavities. So to extract this high risk of caries, we use UMLS. We say that we are expecting something there that may be a medical diagnosis. Right? And then that is the domain of that property, and that's some part of the eligibility criteria. So this is how we do the linking to UMLS. It's based also it's based basically on the semantic hierarchy that UMLS has. Perfect. Great. Thanks, thanks so much for that, Vanessa. Really fantastic talk. Thanks to you, and enjoy your lunch. You too. Thanks. Bye.",
    "transcript_length": 33415,
    "speaker": "Vanessa Lopez",
    "tags": [
      "AI",
      "ICD ten",
      "MetaMap",
      "MongoDB",
      "NLP",
      "PERT",
      "UMLS",
      "deep learning",
      "knowledge graphs",
      "learning technologies"
    ]
  },
  {
    "title": "RDF Leveled the Advantages of LPG and Keeps 3 Key Benefits: Standards, Semantics & Interoperability",
    "description": "Knowledge graphs are the next generation tool for helping businesses make critical decisions, based on harmonized knowledge models and data derived from siloed source systems. Due to the huge value generated by their data standardization and semantic modeling capabilities, knowledge graphs are most often associated with data integration, linking, unification and information reuse. As more and more organizations are turning to knowledge graphs for better data and content analytics, search and graph exploration become key requirements also.\n\nFor many years, two main advantages of labeled property graphs (LPG) have been pointed out: they can deal with properties on edges in the graph and they are good for graph traversal. They are gone now, given that the leading triplestores support:\n\n- RDF-star, which offers a simple and efficient mechanism to attach metadata to the edges of a graph, e.g. weights, access restrictions and provenance information.\n- SPARQL extensions that allow for exploration of multi-hop relationships in graphs.\nThe support for these extensions of the RDF and SPARQL is not implemented as a patch allowing us to check the box. RDF-star is already used by tools downstream and evaluations that prove efficiency improvement in managing Wikidata. RDF-star goes beyond the expressivity of LPG offering not just key-value pairs, but rather the full flexibility of making statements about statements.\n\nEver since version 1.1 SPARQL property paths support graph traversal, allowing you to discover relationships between resources through arbitrary length patterns. Property paths uncover the start and end nodes of a specific path, but not the intermediate ones. There are standard complaint extensions of SPARQL now, which offer exploration of the paths and support all the different variants of the task, e.g. shortest path vs. all paths. And there are RDF engines that take advantage of their reasoning capabilities to score well at the LDBC Social Network Benchmark.\n\nHaving graph-exploration covered, let us go back to the core requirements for knowledge graph management. RDF is recognized as the better option for knowledge graphs, because its web-native syntax supports data exchange and sharing and because its formal semantics allows for easy alignment of meaning and structure across sources, unified views and unambiguous interpretation.\n\nOn the other hand, LPGs lack many features that are an absolute must for enterprise data management, e.g. schema language, data serialization formats and federation. On the semantics side, they lack ontology modelling language and data validation. What\u2019s most important, there are no standards in the LPG space to guarantee interoperability and reduce vendor lock in.\n\nRDF engines check all the boxes: simple-yet-powerful graph model, standard schema and query languages, formal semantics, efficient graph traversal, analytics and reasoning, packed with all the enterprise features. There are a couple of cases where LPGs still have an edge: micromanaged exploration using Gremlin and heavy analytics for wardrobes with TBs of RAM.",
    "category": "Semantic Technology",
    "transcript": "Hello. My name is Atanas Kyriakov, and I'll provide you an overview on how RDF overtook the advantages of property graphs in the recent years and what are the key benefits that it still has and bears for knowledge graph applications. To continue with a quick introduction, I'm the founder of OntoText. We, Samantha back in year two thousand as semantic web and semantic technology pioneer. Now we are best known as the developer of GraphDB, one of the most popular database engines. We are an enterprise knowledge graph top leader, and we enjoy being the center of an ecosystem of more than twenty partners all the way from portfolio partners which complement our technology to, consulting and and delivery partners. We are profitable in growing. We have, among our customers, yeah, the leaders in many fields, like in financial services, in in, yeah, media and publishing, in government, aerospace, health care life sciences, or infrastructure management like Johnson and Schneider Schneider Electric. We are also involved in all sorts of standardization bodies that have something to do with it because we we care about the future of this of this technology and, this this trend. To give you, yeah, a graphy, introduction to what we are doing, We are here to connect the dots of your enterprise knowledge. We do this by, yeah, basically combining and fusing and and, making your proprietary information smarter. Our special way to do this, special sauce, a special ingredient is that we use deep domain knowledge to enrich your proprietary, information. And, the way in which we do it is that we we put together, we create, we craft, we help you craft, rich knowledge gaps that enable unified data views and this way, more more more deeper and more more more fruitful analytics. The way in which we create these knowledge graphs is by, yeah, linking and, linking data across different sources, reconcile reconciling data, converting strings to things, the usual things to to to end up with the with the body of body of knowledge that is easy to query and to explore and and and to deal with. Well, this requires a fair amount of text analysis, data analysis, machine learnings, all the fancy things. And that's what more or less everyone does. Again, our special associates that we we we've been maturing for twenty years how we can use, almost exhaustive domain knowledge, big bonus of domain knowledge, to to to provide context, and to to to help us better interpret, better recognize, better better classify things. And that's that's from the bigger picture, the way in which domain knowledge, help. Get connecting to those of your enterprise knowledge and that's first very important ingredient of talking about knowledge graphs at all. We store this knowledge graphs. We manage this knowledge graphs with our semantic database and search engine, Graph DB. And what's still missing on this picture is why we do this. Well, we want to give you better insights, and reach results for for for results as much as possible in less time. That's what Graph DB is designed to do altogether. On the bigger picture, a knowledge draft management platform should cover a bigger set of capabilities from yeah, building, build build building these big data data artifacts, storing index in operations, accessibility, federation exploration analytics. For each of those there, yeah, more specific capabilities that should be there. We develop a lot of these, the orange boxes ourselves and, we we we we made decision to to use our partner ecosystem tools from our partners for other capabilities. Right? Like, some editors, some toll generators, data catalogs, chatbots. So that's our way of working in this field because we can't, cover it all, at at at matching our our our criteria for metro software. We are, the leader of the RDF space. You you know, the like, the graph database field and the graph technology field has, like, property graphs chapter and the RDF chapter. We're in the RDF chapter. And from this position I want to to tell you what are, what were the historic advantages of property gas and and and how, RDF address this. So, the easiest argument for someone to make, on on on the property graph side was, well, in in in in and everything on top of it, you can attach, properties to to edges and say things about the edges in the graph. While in RDF, you can only do so for for the notes. And then the second big big argument was, well, in thinking of everything is designed and thought of, yeah, providing efficient graph traversal. And historically, this wasn't the case in in RDF. But over the last three years, yeah, RDF, yeah, addressed this in a very, very comprehensive manner. The first thing was, RDF star. That's specific specification for attached metadata to the edge edges of the graph that started two thousand fourteen, and now there are plenty of implementations across different vendors. The second topic is, yeah, the SPARCO extensions which allow for graph traversal path search and generally speaking exploration of multiple relationships in graphs. So I'll continue this presentation with a bit more flash and technical detail on the how RDF star is implemented and how RDF engines implement the graph reversal. And then, I'll give you my view on, yeah, what's the added value of knowledge graphs and finally how RDF enables this this added value. To start with, yeah, here are the example. If you want to encode the information that Abraham Lincoln was, president of the United States, that's a very easy way to to do it. You have two nodes and, one relationship, one edge labeled with president of you can have a slightly more advanced version having labels or names for the two nodes. But the reality check says, well, actually, you you you have to be able to do much more than this. So, you need to to provide contextual temporal information, as well as provenance metadata so that you have all the all the important information about this fact, this edge. Historically, in RDF, there was no easy way to attach this information to an edge in the graph. But yeah. Now now it now you can see how it works in RTS star. So, you can have the statement, Abraham Lincoln, position held president of the United States. And through this entire statement, you can make another statement saying, well, this thing, this fact, this statement has start starting date whatever whatever, literal that is the starting date. So you can make statements about statements. If you want to look at this, at the at the diagram, think of having an edge and then being able to make another edge that starts from the first one, and making statement about it. That's much more expressive than the key value pairs that you can do in property graphs. That's the most consensual part of, what what is coming for some optimization in RDF one point two. Before before RDF star, there were other ways to do the same thing like standard reconciliation in RDF or any relationships or singleton property, singleton name graphs, but those were not very handy. So RDF star made, attach metadata to the notes really simple. And we also made sure that the graph implementation makes it makes it efficient so we have efficiency gain. We we took a a Wikidata fragment that if you encode it with the standard verification it takes almost four hundred million statements. It takes a bit less than an hour to load it and so on. And then if you encode the same data in rdf star, it is almost twice smaller, less statements and and it takes much less time to load and it takes much less time on your hard drive. So we also have, this, RDF star support now for for several for kind of couple of years. And, we have some of our partners already using it. To give you the use case, Synaptica is the developer Graphite. That's one of the leading vocabulary management, taxonomy management tools. They use Graph DB to store, all these taxonomies and vocabulary seen as cost data, but historically they had problem of how how they manage their access control lists. And without RDFStar expressing which users can control if you have access to, what access to which properties in which schema for what project was, like, what you see at the top. It was encoded with five different statements. And now with RDF star, it is much simple. There are no auxiliary notes, and you can you you need only three statements to encode this information. So it's it's obviously much simpler. We also took care to to, we made our homework to see, how the other engines handle different corner cases like, statements, valid statements, valid statements, nested nested nested embedded triples, or how how they deal with the situation. Imagine you delete the triple from from your repository, and you want to maintain information about who and when deleted this one. So the statement is not there, but you should be able to keep metadata about it and this kind of thing. So we we took care to others then well. So by now you should be convinced that, yeah, edges, edges and properties are covered very, very well in the RDS space. So let's move on to the graph traversal and how it is implemented. It it is a computationally very complex problem. It comes in different flavors like, checking whether there is a path between two nodes or finding the shortest path or finding all paths or the neighboring nodes and stuff like this. And what they have in common is that they're computation like, heavy problems. Doesn't matter whether you do a deep first search or breadth first search, it it is complex. And complexity goes exponentially with the length of the of of the path that you're searching for. So still there there are a number of good reasons to to find a way to implement it because it's very useful for, yeah, navigation or for knowledge graph analysis, supply chain management, and so on and so forth. There are plenty of use cases. So, historically, yeah, SPARCO was able to almost do this, but in Sparkle one one, you can do property pass to discover whether there is a path, but you cannot get the intermediate notes. And it's very hard also to to to implement in in in spark in plain spark one one shortest path. There are workarounds, but none of them is something you are going to like, they're also slow. So to address this, all the all the vendors of the databases did, yeah, some extensions of sparkle to address this. That's how our extension looks like. So you you you you have a service clause in in in in sparkle that where you you tell I want to do all paths search, and you provide source and destination if you want to or just the source. And then you can specify the maximum path optionally, and so on and so forth. You can also specify that the the pattern that that you want to to to trace so that that you can specify specific conditions, patterns in the graph that need need to be to to be there for each step of this path. You can also do shortest path and, you can do bidirectional search also. So we covered all all flavors of this task. Again, we did our excess our homework to see how others do the same thing and explore cases. And, yeah, you see that our competitors have different advantages and disadvantages in this field. But what's most important for us is that we try to keep keep compliance so not to not to implement it by some hacks in the sparkle process. So, this table wasn't oh, yeah. In a way, we are checking the boxes, seeing whether whether we can do this and that. What really matters for us is also whether we do it well, whether we do it in efficient manner. And that's where, yeah, benchmarks help to to see what what is the actual complexity with specific hardware, with specific volumes of data, what are the choke points, what are the inefficiencies, and address those. We use the link data, benchmarking councils, social network benchmark for this purpose. LDPC is a TPC like industry body of the graph the graph database vendors, on space, Neo four j, CVI. We're among the the the the founders. And then now you have all the all the, yeah, major graph database vendors in there. A few words about the specific benchmark. SMB is the most comprehensive graph on it is benchmark. It simulates the kind of questions that you would ask for the data behind the social network platform. And there were, many many person years invested in making good data generators. So this graph is both realistic and and challenging and covers all the distributions and connectivity patterns in an in an interesting fashion so that you can really challenge the engines and and compare them. To give you feeling of the data model, so you have relations between between people like, knows relationship. You have attributes for people like addresses, these cities and so on. You have relationships also between people and companies or university and stuff like that. There are different sets of queries, and, yeah, the interactive queries, you have, like, one of them saying, well, let let's get all all the people that you can get within three steps following the nose relationship from a given person or find the shortest path between two persons. So this this is what these queries are like. And Graph DB is the first RDF engine to pass the to pass this benchmark. We do this in a stepwise manner so we we are essentially push pushing the boundaries of what we can get out of workstation or mid range server with yeah, the thirty thirty two to sixty four gigabytes of RAM. So far, we got wonderful results with, at scale factor ten, which is half a billion edges graph with social network graph with half a billion edges. And we're working now to to to to to the next scales and get it to the five billion edges. So we benchmark Graph DB against the one of the most popular top three graphs engines, and we we score very, very well. We we get better results on the interactive queries. And I gotta admit we we are a bit slower on loading the data and and and updates. One way in which we can take benefit of what RDF engines do well is to use inference and we extensively use inference for this purpose. So for instance when you have to to follow a a a a path that involves two types of relationships, knows and has person, you know, on each step, in the chain like, right here. We use inference to to do some sort of shortcuts, these direct nodes, relationships which are shortcuts of the relationships that we're trying to follow, and this allowed us to simplify the query. And this way, yeah, we made a query that takes half a minute thirty times faster and gets sub sub sub second responses. So materialization using inference could be could be very, very useful, for some of these tasks. So we we we get also the craft reversal and then the finding part in the craft to recover. So, let's see what what the benefits, what what are knowledge graphs graphs all about and how they differ from other ways of doing similar things. So a quick quiz, let's think a little bit on what are, what is in common between these, these IT problems, these information management problems that that that I list for you. So think of a a a very, very big, retail bank that wants to consolidate information because it's IT systems to do root cause analysis whenever there is a problem like an ATM not functioning. Or a market market intelligence agency, that is that is the, provides reference price for oil and gas. They need to to have a really rich set of signals about everything that may have something that may may impact the price of oil and gas on some markets sometime. Or big investment advisory that wants to have the best m and a intelligence database on earth with all the companies and all the transactions and all the sections that you can do analysis by industries and technologies and so on and so forth. Or the fourth case here, building management system, yeah, consolidating the information about all all sorts of systems in a big building, like, the elevators, heating, ventilation, electricity, access control, and, everything like that. Then finally, I think of a pharma company that receives a compliance, a regular inquiry from FDA about side effects of drugs, and they gotta find relevant information in, within really thousands of long reports from clinical trials of drugs. What these problems have in common is that we have proven through the years that the knowledge graphs can help for each and every of those problems. And what then what makes them specific and and complex is that, well, you need a unified view across diverse information to be able to address them. So you need to combine several databases developed with us without in in different departments of finding the same organization, and to also use information extract information from, documents and unstructured content. And quite often to to to to get this right, you need to use external knowledge, global domain knowledge in combination with the proprietary data. And, this is this is very, very specific and very important for, domains where you have to deal and tasks where we have you have to deal with hundreds of thousands of concepts and entities. Or even if there are not that many, situations where you have very complex interconnections and lots of, relationships, not just the taxonomy, but more complex relationships between between, concepts and entities within the domain, as well as situations where, the the semantics of the relationships really matter to properly interpret the data. So, yeah, to to to to wrap it up, what what knowledge graphs do in these cases is that they they they provide the business with competitive insights through better intelligence. And better intelligence means, deeper understanding of the data based on a deeper knowledge, copy of signals that you can derive from diverse data, and also instantaneous updates that that that that come if you have data fabric or another another data integration platform that gives you flexibility and and sustainable updates. This is all these all these capabilities is what makes knowledge graph, helps knowledge knowledge graphs, disrupt several existing information management fields like content management, data management, knowledge management, and nowadays more and more from the business process management and automation industry and infrastructure, and and and manufacturing. So you you see that in the terminal nodes of this map of graph, you have plenty of plenty of different applications, that that we have seen knowledge gaps being being successful so far. So, the added value, to to again wrap it up is that knowledge graphs can, serve as a helper across different systems, data management, content management, and all sorts of metadata across them. And what's more important, we we we we layer semantic metadata, on on top of all these all these knowledge to make it easy easier to use. So on the technical side, what what it means in practice, is point one linking data together so that you can interpret them better because a conceptual network of thousands of concepts can, provide much more information, and computers can take much more meaning out of it as comparative tables, having the same data in in tables. Then the second thing would be is, to overlay semantic metadata to avoid unambiguous interpretation because when you get data from different sources, it is very likely that in a different context, the information can be misinterpreted. Having proper the specification of the meaning of the different data pieces is very important. Knowing what kind of price is this, whether it's with or without VAT, whether it's before or after sales commission, all these small things is what you need to describe in a very good manner so that you can get a unique useful useful unified use. And the worst thing here is that you you you, if you have a a stable semantic data model, and and the reference data, that's that's the way to to basically have continuous updates of your data and to be able to reuse data prepared for other analytic purposes beforehand. So finally, what is that RDF does better to to to cover all these capabilities that are necessary for for knowledge graphs, as compared to any other data management paradigm. So explicit semantics that that allows you to align the meaning of the different, modeling consumptions across different IT systems as well as you can use semantics for to validate data on a semantic level to check consistencies and this way maintain the quality of this combined combined data set. There are plenty of features in RDF that foster interoperability. So that's federation protocols and remote access protocols as well as, different flavors of syntax, that that that really help easy easy exchange of data. Global identifiers, which make sure that when you put two date two two piece of data from different sources together, the the the things that must be linked there will will click and link automatically, and there will be no no identifier clashes. And, finally, there is there there there are thousands of datasets available as RDF as linked data. Life science is good example, but now we we see more and more of this also in the, industry and the infrastructure. You may not know, but, now you can get the entire information about the electricity grids and network, in in Europe, in RDF and do quite a lot of interesting things with it. Finally, it's about standards, standards about everything, standard serialization syntax formats, schema languages, query languages, update languages, and everything on top of this. Using standards is we have can can come at a cost at the beginning, but, this is what what what makes your data management enterprise data management setup future proof. And also standards is what what what gives you great, yeah, much lower levels of vendor lock in. And what is that property graphs lack to serve such knowledge graph platforms? Well, it's all of this. So there is no formal semantics, there is very little to help you in terms of interoperability. And, yeah, standardization, they just started. So probably in five or ten years, it will be there. So RDF engines, yeah, the best platform to implement knowledge graphs, they check all the boxes and now also traversal and analytics, along with, yeah, all the all the enterprise requirements. And, yeah, I have to be fair that there's still case in which property graphs are better. So if you want to do some very specific programming Gremlin to explore graph, yeah, that's the way to do it. Or if you if you want to do really really heavy analytics with petabytes of data, then you you you take a rack full of, yeah, servers with terabytes of RAM And again, property graphs are likely to serve you a bit better. For everything else, RDF is what you need to get your graph infrastructure right. Thank you.",
    "transcript_length": 22841,
    "speaker": "Atanas Kiryakov",
    "tags": [
      "data analysis",
      "data integration",
      "enterprise knowledge",
      "graph traversal",
      "graphdb",
      "inference",
      "knowledge graphs",
      "link data benchmarking councils",
      "machine learning",
      "metadata"
    ]
  },
  {
    "title": "Reasoning for the Answer: Who is the Greatest Formula 1 Driver of all Time?",
    "description": "RDFox, the high-performance knowledge graph and semantic reasoner, is used to answer business questions by manipulating and enriching data, extracting insights.\n\nAt the highest level, questions often take the form: \u2018what is the best _?\u2019. Today we ask a question appropriate for its unmatched speed, finding like for like as the fastest out there. Who is the greatest Formula 1 driver of all time?\n\nAffected by so many variables, how can you compare Hamilton to Schumacher to Lauda when they raced decades apart? With half a century of race data, RDFox gives the answer.\n\nDesigned around a highly optimised in-memory approach, it affords query and loading times that are orders of magnitudes faster than alternative triplestores. RDFox is developed by Oxford Semantic Technologies\u2014an Oxford University spin-out founded by leading academics and backed by decades of cutting-edge research in semantic web technologies, recognised with a Lovelace Medal.",
    "category": "Semantic Technology",
    "transcript": "Hello. My name is Pete Crocker, and I'm CEO and cofounder of Oxford Semantic Technologies. In this session I'm going to begin by showing you a short demonstration that answers the question we posed for this session, who is the greatest formula one driver of all time? This is a question that is frequently debated, and a quick search will reveal many contradicting answers. This application is powered by AudioFox, our high performance knowledge graph and semantic reasoning engine. This demonstration will be followed by a quick run through of a series of use cases making use of AudioFox today. And finally, we lift the hood on the demonstration and give you some more detail behind the engine. If at any point you have any questions, please ask them in the chat and we'll be happy to answer as many as we can. Otherwise, come and find us at our booth during the conference where we will happily go into far more detail. So without further ado, let's dive straight in. Our dashboard includes a series of controls where you define the question, here on the left hand side. And over on the right, a set of information boxes that show you some of the statistics behind the question being asked and the data that's returned. In the middle, we have the answer to our question. First, how long the question took and then a chart showing us the score against the time that the driver was racing. So what we're showing here is the race points for the top twenty drivers and the year of the last race in which they participated, all the way up until just a few weeks ago when the last race was taken here in twenty twenty one. Scrolling down we also get a tabular view of the results and we can see here the top ten and a lot more detail and and statistics all organized and ordered by the score based on our criteria. To vary the definition of our question, we can change the scoring system from points to position gained in a race. So what's happening here? For every single race ever recorded for formula one, we are looking at the starting position of a driver. We're taking where they ended up, and we're taking the difference to calculate the positions gained during the course of that race. We're totaling up all of those differences across the entire dataset, and then we're ordering to find a person, in this case, Fernando Alonso, who's gained the most positions over the entire history of Formula One. Now that'd be great, but perhaps Fernando Alonso was running in a in a fabulous piece of equipment, a fabulous car. So what we can then go and look at is a comparison of Fernando Alonso only with his teammates. How did he do against the others running in McLaren or Ferrari, for example, when he was driving for them? So by clicking on teammates, that's exactly what happens. And here, Jonathan Palmer jumps to the top of the list. So why why is he at the top? Well, his positions gained over the course of history when compared with his teammates gives him a score of hundred and eighty six. Okay. But let's have a look. He's raced seventy six races. What if, someone else has had a shorter career but, again, has, has recorded more positions gained against teammate? Well, again, we can change our calculation, change the definition of our question, and this time flip over to calculating an average. Now we start to see our our first sort of problem. We've narrowed the system such that we're gonna be favoring those that have had few races, in this case we're seeing pretty much the entire field occupied by those with a single race, and they may have won that race or come very high in their positions gained. But, really, are they the best? They've only raced in one race. So let's change the filters of our criteria. We're gonna up the minimum races to five. And while we're here, why don't we put in some a requirement for podium positions. And there we see our driving results drop from twenty five thousand to now a field with fourteen thousand results. Our drivers have come down from eight hundred and fifty three to a hundred and eighteen that are qualifying with that minimum of five and three. Okay. So what have we shown so far? Well, we've been able to make our decision for the question based on race points or position gained, whole field teammates total and average. Let's come to the final thing that we're gonna consider for this demonstration. When discussing this particular application with a former CEO of one of the teams, he suggested that weather really tests a driver's skills and ability on the track. So we can filter based on the conditions on the day that the race took place. At the moment, we're showing results for all conditions, but we can change that. We can go for mixed where there was some evidence of rain either on the track before the race or we had some rain during the race. Or, alternatively, we can go for full wet race conditions. And here you see there haven't been very many races with wet fifty four that are recorded in in our data, but here, this arguably is a really good test of what makes the best formula one driver. Now maybe we think that fifty four is just too low a number to consider in terms of number of races. So we'll flip back to a mixed conditions and settle there. And what we see here is Ayrton Senna, one of the greatest drivers of all time comes to the top shortly followed by James Hunt. And in third place, not too far behind, Max Verstappen. If you'd like to try this application for yourself, please head to f one dot rdfox dot tech, and I'd be interested to hear how you define the question yourself. Please note I've been running this application locally as I wanted to show you the true performance of IDEA Fox. Given our speed, your query times are most certainly going to be dominated not by the time it takes IDEA Fox, but by the network latency between you and the servers where we're hosting that service. Okay. So what about, what about the company that that produces Alia Fox? We were founded back in twenty seventeen. We're headquartered in Oxford in UK, and we're a spin out of the University of Oxford. We're an independent software vendor, and we're proud to be backed by Samsung Ventures who saw promise in the technology back in the early days. AudioFox itself is what we think of as a a knowledge graph, and plenty have been spoken about knowledge graphs during this conference. But we also consider ourselves a very powerful reasoning engine, and more on that to come. We're open standards based, built, as you've, as you might have already guessed from our heritage, on leading research, and we pride ourselves on on speed and performance along with the reasoning. We have fast query times, fast load times, fast materialization times when it comes to reasoning. And all of this is built on top of the engineering and hard hard work that's gone into AudioFox. We're based in memory. We're built on c plus plus We'll start in finance. Here, rules are used to chase down chains of money flows within transactions for a bank. This is in support of anti anti money laundering or AML applications. Chasing these kind of chains are not patterns that you can simply detect with regular graph type queries. They require vast amounts of numerical and date time reasoning, and most importantly, recursion as we go through those changes. Next up, we have some work done by Samsung Research where they have built recommendation systems that run directly on the device itself. Here, they're providing answers to personalized recommendations that take data from the device out on the edge and perform the reasoning without that data ever leaving a user's trusted device. Samsung Research, in collaboration with Oxford University, have recently published a paper demonstrating how this can be achieved using RDFox on those devices. Festo, the automation company, were among the first to see the power of reasoning for building their electronic drivetrain configuration engine. This solution computes all the possible ways in which their components can be assembled to so solve their clients' needs. Built on RDFox, this replaced an old legacy relational database system that took over sixteen hours to compute the capabilities those compatibilities, sorry, with something that now takes ninety seconds. And so this is all supported with incremental reasoning. This approach completely transforms how they think about the problem, and as it turns out, has been demonstrated to be more accurate than the old system. OST also partners with Sirius Research Center in enabling the digitization of large scale engineering tasks. Here, we're supporting the design process. Doing things like checking engineering constraints at massive scale, building industrial digital twins to support their applications. In a medical setting, RDFox is being built into power decision support. Here, clinical knowledge captured in captured in ontologies assists clinicians and makes suggestions as they perform investigations. In this clinical setting, embedding ARDFox within the device allows these devices to operate in real time and potentially disconnected from the network, providing both resilience and protecting patient confidentiality. Finally, Ulm University, in a project sponsored by BMW, have used AudioFox to integrate data in the field of autonomous driving. The need to do so on device is clear as is the need for a highly performant decision engine, of course, AudioFox. So what makes all of this possible? We believe semantic technology is best suited to these applications. It provides a highly flexible and adaptable data model. When, for example, we conceptually want to map out ideas, we naturally turn to drawing graphs. Ontologies then support a layering of concepts and extend instance data with domain knowledge. Integration, in particular, declarative integration, bring all of this together, the sources needed to answer our questions. And finally, reasoning is what makes sense of it all. It's required to bring together the logic perhaps captured within our ontology, the domain logic logic with data and provide intelligence to our query answering. AudioFox is our answer to these semantic needs. AudioFox provides you the power to unlock applications previously undreamt of, flexibility for how you wish to shape your data or the ontologies you wish to bring to a problem, Performance, unrivaled in the market at the time of data importation, reasoning, the analytics, and query answering. And finally, scalability and versatility. The ability to scale from the smallest device up to billions of facts. I'd next like to focus our attention on reasoning. Here I list many aspects of reasoning that are used in the applications I covered earlier. As you can see from the table, others in this space often define reasoning as ontological reasoning, and some provide even constraints through the likes of Shackle. These are all important. However, AudioFox goes beyond this with graph analytics, data, and numerical analysis, the kind of thing you can't imagine some of those financial applications being possible without. As well as dealing with constraints, both locally, we can also think of constraints in a global setting, considering the constraints across the system as a whole. Finally, we provide the ability to combine all of these. You can bring aggregation together with recursion, negation with class based inferencing. If you get a chance later on during the day to join the session with Marcus from Siemens, he has some really nice examples where he's using reasoning to perform many of the many of the above. Next, let's lift the hood on my earlier demonstration. Every time one of those controls down the left hand side is changed, queries are being issued to RDFox. And as you saw, live results are being computed in milliseconds. Behind these queries, we have, as we saw on the info boxes, over a thousand races, over eight hundred drivers, and fundamental to our question, over twenty five thousand results consulted and calculated against for the score to your question. There is no additional cash. We are achieving these results through the hard engineering and in memory approach that has gone into and is at the heart of AudioFox. Reasoning also plays its part in precomputing some of the relationships in advance. Note, given the variations possible in our questions, we cannot compute the scores in advance, just some of those relationships. This graphic shows us how we assembled the data. Let's start with the sources. On the left hand side, we have a database containing all of the race data stretching back to the very beginning of the sport. We then supplement this with images and weather reports drawn from Wikipedia. Once that's done, a small NLP task categorizes the weather text into RDF concepts. That's where we get our dry conditions, mixed conditions, and and wet conditions from. There are a number of rules in the system. Here I'm showing just one of those, one of the more interesting ones where we establish a connection between teammates through through common race and constructor relationships. Essentially, what's happening here is a rule is running as soon as the data is brought into the system. A rule is running that tech compares every single result with other results in the system and where it locates common races, that's circuits, and common constructors, a new relationship is materialized into the graph, the teammates result. And this is used for those queries where we were running, where we comparing where we change the fields to all field to the teammates. Through that application, we've seen some of the ways in which RDF Fox can be integrated into your system. What I'm showing here is a much bigger picture with all the possible integration points for for the technology . Over on the left hand side, we have the external integrations, and we were doing some of this when we loaded RDF through files and through through SPARQL for our for a demonstration. In addition to RDF, we can be loading relational data through through SQL. We can be loading file based sources and, text based sources through Solr to provide a a text index as well. On the right hand side, we have various development and operations aspects to Ardeafox. We provide a console where you can run queries and visualize results to those queries and explore your data. We provide command lines for controlling the running of AudioFox. And of course of course, we provide developers and knowledge engineers with the tools they need to modify the core assets of any solution, Datalog, OWL, Shackle, and Swell. In the middle, we see the components that make up the runtime of ArduaFox, the sparkle API we've already, described. The reasoning engine, the piece that's performing, the rules, act actioning the rules, for our applications and materializing those results in memory into the graph. We also provide a a persistence layer. So we're not only running in memory, but the results, the data that's loaded and the results of of materialization are persisted to disk should you need to restart the system. And as we've seen through applications, all of this can run out on the edge on the smallest of devices, on premise, or in the cloud. For our cloud support, we provide Docker images, and we have customers running on AWS, Google Cloud Platform, and Azure. To conclude, we've seen that Ardfox takes you from question to answer in milliseconds. Through reasoning, we unlock a new breed of applications not possible before. We're available on the smallest of devices to the largest to cope with concerns around privacy and tackle some of the largest scale problems. I'll leave you to decide who the best driver is, and we've provided the tools for you. If you can define the question, the answer is only gonna be milliseconds away. Finally, let me leave you with a quote from one of our customers, and I'll paraphrase. Adia Fox makes it possible for us to do the impossible. So do you have something you consider impossible? If so, we'd love to hear from you. During this conference, please visit us as our booth. Alternatively, if you're not able to join us there, here are some links for you to follow-up after the event. Thank you. And remember, f one dot r d f fox dot tech if you'd like to try and answer that question for yourself.",
    "transcript_length": 16371,
    "speaker": "Peter Crocker",
    "tags": [
      "audiofox",
      "aws",
      "azure",
      "c plus plus",
      "datalog",
      "docker",
      "google cloud platform",
      "nlp",
      "owl",
      "rdfox"
    ]
  },
  {
    "title": "Sales AI: Building and maintaining a knowledge graph",
    "description": "The staggering increase in private data stored by corporations is driving numerous efforts aimed at utilizing the data for business purposes.\n\nOne compelling approach is to merge private data with the plethora of available public sources into a seemingly single unit that is searchable and regularly updated.\n\nIn this talk we demonstrate how we achieved this vision using a scalable asynchronous stream processing architecture that can handle Terabytes of data and ensure data relevance via regular updates.\n\nEran Avidan offers an overview of a novel architecture based on Kafka streams, Kubernetes and Neo4J that easily enables the transformation of any piece of information into a knowledge graph structure while maintaining its freshness over time.\n\nThe solution is based on a series of distributed asynchronous steps that \u2018listens\u2019 to changes in private and public data sources including sales information, marketing activity, social media and commercial websites, extracts knowledge, structures the knowledge into a multitude of appropriate graph formations, and inserts that knowledge into a large and growing graph database.\n\nThis solution serves as a knowledge base for new AI models that are used by Intel\u2019s Sales and Marketing Group to aid in detecting otherwise difficult to find links between potential clients and as a result directly help Intel in better serving its customers.\n\nThe Sales AI knowledge graph currently holds hundreds of millions of connected entities with thousands being fetched, enriched and connected to the graph by the hour.\n\nThe approach is highly generalizable and can be applied to a broad range of settings that could benefit from integration of large private and public data into a rich graph of knowledge.",
    "category": "Knowledge Graphs",
    "transcript": "So it's my great pleasure to, introduce Eran Avidan of Intel. Eran is a machine learning engineer and architect of the advanced analytics group at Intel, which delivers AI and big data solutions across Intel. His work at the Advanced Analytics Group includes research and development of cutting edge distributed architectures in the AI domain. So let's give a warm warm welcome to Aram, and, and we can get started. Over over to you. So hi, everyone. So I was introduced, very formally, very excited, to be here, starting this, presentation day. And, today, I want to share with you and give you a bit of a overview of what we did for our sales department at Intel. Actually, a project, building and maintaining a knowledge graph we started working on two years ago. And, to continue my introduction, my name is Ivan, and, I'm a machine learning engineer and the architect of, Intel IT AI. And been working at the Advanced Analytics Group for the past ten years or so, where we provide, products and solution across many of Intel's division, specifically machine learning and artificial intelligence, solutions from, helping better design the chips through manufacturing, increasing the yields through analytics, up until sales and marketing where we help increase the revenues by recommendation systems, and so on. And we even have some products, working with Mobileye, harnessing insights from cars that transmit data through, Mobileye sensored vehicles. But today, we're gonna talk about sales and marketing, and I would like to introduce you to our sales AI system. But before that, let's talk a little bit about our sales department. Intel sales department, like every other big enterprise, is is huge. It's actually a division. It's huge. It's composed of thousands of people, account managers covering hundreds of thousands of, customer accounts. And they can offer, hundreds of different products from CPUs, soon to be GPUs, and memory and storage. This enormous complexity, may lead to some missed business opportunities. And this is why we've built, this sales AI platform. We're looking to bridge this gap of missed opportunities by first, in real time, scan customer information, store it, detect intent to buy, and maybe identify some potential opportunities, automating and recommending a personal or best action for the account manager, and then feeding back to the system learning based on customer response. So that's the sales AI framework we've been working on for the past six years. And over the past six years, our sales AI program led to a community of sales revenue growth of approximately more than half a billion dollars. And through those years, we've supplied different solutions for account managers. And to give you an example, one such solution is called the sales assist as it assists account managers in identifying new business opportunities, with leads about their respective customers. For instance, an account manager can open his dashboard and see the companies that it, he or she covers and see that company a, we're gonna talk a lot a lot about company a, is about to attend the conference. Or maybe they browse into website for products they never purchased before, and this may lead to a business opportunity. And we all these, we supplied many such solution, but then we started thinking, well, all these insights that we give account managers, are they visible for any future model or system that inquires about company a? Or do we even have a single layer of sensing where we store all the, information we collect for future progress? Do we store back all the insights into that same place where someone can query about company and get all the information back? And the answer, as you can imagine, was, no. We do not. So we decided to start studying now. We decided to record all activities as well as insights into a single centralized place. This would basically simulate in the sensing part what an account manager would do. So we look at sensing at scale in a nutshell, what I'm talking about to, give you an example. It it starts from basic information that the company gives us with enrolls into Intel's program, like location and maybe the field it works in. Then we can start recording some internal information, sales transactions, what they bought, how many, maybe what courses they took in, for Intel products. Up to, collecting customer website information and going through social media and recording these activities as well. So usually, when you take pieces of information, then they're scattered all over the place. But we decided to take all those silos of information, all those silos of data, and and put them into a single centralized knowledge graph. And why would one store store these things in a graph structure? So for us, the motivation was, I believe, similar to a lot of others. The world is composed of entities and relationship between them. So this makes things very easy to explain. When you give predictions or even a simple analysis of a situation, you can show a graph and it makes sense. When you want information about some kind of entity, for instance, company a, everything is really close. So I want things about company a. Like, everything is one hop, two hop, or maybe three hops away, but I get this nice image about company a. And since everything is really dynamically structured and you're not limited to any schema, you can simply add more data easily, connect it to company a if it's relevant to company a. And when you have, started growing a nice knowledge graph, then you can infer knowledge directly from that graph, simply by traversing it or running simple, graph algorithms. And later on, you can use that graph as it grows and grows. They have a really great knowledge base for AI as we're gonna see in a few slides. So we started thinking we wanna grow an old graph, and I'll illustrate how things work, when it comes to sales and marketing at the end. First off, company a enrolled into into this program to become a distributor or buy products for itself. What it was enrolling, get some basic information. It says that it's part of the IoT and, the NAR industries, and it's it's a builder in those industry. Later on, we started saving some internal information. Through sales transaction, company a went on and bought product six and product y, and we connected company a to those products in our knowledge graph. And as a side effect, we got that company a is interested in data center products. Then looking at company a's website, we we've noticed our smart medical devices. So maybe we can add this edge of company a to the health care industry. Then there was a social media tweet that company b have invested in company a, so we can connect company a with company b. And if we're lucky, we may have company b in our knowledge graph, and then we get some secondhand information about company a. Company a went on and browsed intel dot com, and we recorded that activity. And there was more and more activities which we recorded and added edges for. And what's interesting is that we started only with what we were told about company a given by company a. And this union of internal and external data sources can create a really strong knowledge base, which is basically the basic basis of our knowledge graph. You can simply take any piece of information you get about company a and edit as you're not restricted, to any specific schema. And this information, and this is key, will not be hidden somewhere in some other data sources. You can simply query about company a and everything, by the number of hops, and you will get all the information about company a and anything related. You can then if you wanna train a model relating companies, you can say, I wanna extract features about, company a. So maybe I'll I'll do it by proximity, and I can rank those features by any graph logic, I determine or, for instance, by the distance from the company I know. And then I get a set of features to train my AI model. And if we think about it, most of these things are facts. And using the knowledge graph algorithms, we can generate more and more edges with some confidence that as our assumption, is and was and still that we live in a world where we always have only part of the knowledge even if we have all the data. So in a graph, this is is very easy to extend our knowledge by adding more edges between existing nodes, between existing data points, simply generating more knowledge with data that we already have. And let's move, to the more technical part or that the idea or the solution that we built in order to make this graph grow. So identifying information was basically flowing in through the system. We've decided to build a stream processing system. Specifically, we build one asynchronously to make it also more efficient. And we start thinking what would, what we would need for such a system. And we've identified several capabilities we believe are crucial for this process, and we've designed, respective component for each one of them. So if we look at the architecture in high level, and again, this is an abstraction, the capabilities are the key things here. We see that we have a microservice architecture, and we have a component for each capability we're gonna talk about. And we have a message bus since everything is asynchronous. We have a message bus in the middle which synchronizes things. And there are loaders which load internal data. And since we because this is the basic information we have and we knew we wanted to load it, But then we knew we wanted to transform it somehow into graph formation, so created the transformers, which is the second capability. We want to enrich this internal data with external information. So we created the extractors, which go out to the web and fetch new information about internal information where all they have. Everything needed to be persisted into a graph database, so we created the graph builder. And to keep everything up to date, specifically external information, we created refreshers. And let's talk a little bit about each one of those capabilities and, what we believe we need in order to make this happen. So the loaders, they import internal data as I stated, and they support a variety of sources and formats. It's it's kind of a plug in system. It's a microservice based, architecture, and they work with the push or pull. They can query daily from SQL database or can they can get a push notification from the size five system that the CSV have just arrived. They load the data. They convert it into JSON format and publish it to the message bus to later be transformed and persistent. So the loaders are fairly simple. The second capability, this is actually the basic processing unit of the system are the transformers. And these do the most, let's say, probably are the most busy logic wise as they convert information into graph semantics, into graph formations. They're always on. They're all they asynchronous. They're stateless, which make them by nature extremely fault tolerant and scalable. We specifically use the Kafka streams technology, but you can use whatever you want. The transformers know how to transform information into graph formations by configurations per entity. This specific transformer knows how to transform product information, which it gets from internal data or external data into nodes and edges. And it does so simply by configuration. We've created a nice YAML based DSL that tells which fields become which node and which, field becomes which edge and which nodes it connects. And it goes on and on. And this DSL became more and more robust up until a system analyst can simply configure any new, source information it found, it finds in order to instruct the transformer how to transform this information into nodes and edges. So that's, let's say, like the, middleman, the expert in the middle. Since the internal information is not enough, we we created the extractors, which actually trans extends transformers. They also transform the information into graph formations, but they also enrich it with some external information. So on top of being configured per entity, per product, in this case, they're also configured per data source. You would need to instruct them how to get this external information and what to do with it, with what kind of post processing to do with it when they, retrieve that information the information . For instance, running matching algorithms of sorts and scoring the validity of the information they just got. They just received, that is. The next capability is the most b d the like, the busiest place in our system is the graph builder. It's only sure is to translate those graph formations into graph query language. In our case, we support Cypher as we work with Neo four j. It's the most prevalent, I think, GQLs out there. And the graph builder, you can think about it like a microservice data layer. Nobody writes to the graph database by but the graph builder Since you wanna keep everything in check, you don't want any other processes disturbing, your throughput or the integrity of your knowledge graph. The Graph Builder, same as every other mod microservice, is very scalable. So you can scale it out or in, depending what throughput you need and what resources you have. And if you're worried about the graph database we chose, Neo four j. So when we started working at two or three years ago, we've examined many graph databases. We found Neo four j to be the most mature and and fairly stable one. It's really easy to use. It supports Cypher, which actually decouples us from Neo four j as we can always switch to other databases that support Cypher, that work with Cypher. But for me and and a lot of other people, the the best selling point was the great UI. You you remember that we talked I I talked. You listen about, decks. The it's easy to explain. Well, it's easy to explain if you have a UI. If you don't have a UI, it's very hard to explain. So, this is actually a screenshot we're gonna talk a little bit about later that I took a year ago from our knowledge graph. The last component of refreshers, they're fairly simple. They are the only one that query the graph as they simply traverse the graph looking for stale nodes. Once they find the stale node, they trigger an extractor. In this case, a product extractor, in order to fetch and refresh that, data or that information in the graph, and they move on to look for other failed notes. And if you wanna keep your graph up to date, you should have a few running and traversing the graph. They're like agents looking for old nodes and edges and updating them, as time goes by. So this was the architecture. But let's talk a little bit about what we did with and how about what what the graph structure actually looks like. So the first thing that we will approach with, like, some kind of proof of concept, was we the business wanted to get a view of customers and their partner companies. So how would you how would we configure the system to get partner companies for, inter customers or companies we already have. So we configured the system in this manner. Right? We needed a few loaders, sales, company, product. We needed the corresponding transformer for each one of those loaders so we can transform this information into graph formations. And the key configuration here, the key implementation, it was the partner extractor. Right? The partner extractor was implemented by crawling, partner page in the company website, analyzing the logos through deep learning models, and connecting the company with their partner company's URLs. To in order to keep this up to date, we also, put in partner refresh. And after configuring the system this way and letting it work for a few, a few days, we got this nice graph as we can infer knowledge directly from simply by granting it and getting this nice, UI. And what we see here is that we have company URL cell and some other companies. And here we can see an image of two companies that buy things from Intel and their partner companies. So if we look at specifically the graph formation we chose to use and since this is, let's say, quite different than you would expect, basically, in order to keep things dynamic, everything that has more than a single relationship is a node. As you cannot connect a single edge between more than two nodes. Right? URL can be used by several companies, so we deemed it a note. Sale is an action of purchase, you can say. Company a purchased this product, but it's also a sales transaction that may have different relationships such as buyer, seller, product, etcetera. So we made it a node as well. So we were pro nodes, as it makes things more easy to manage, easy to persist, and you would not lose any information. But while but while it's great for inferring simple or inferring knowledge directly from the graph by simple queries, it is not optimal for training machine learning models as we will soon see. Another principle, we found important, was to keep trace, as I mentioned, of information and where it comes from, as well as the option that there could be inconsistencies when retrieving data from external data sources. And this was the key feature, like, enriching with external information. And I'll illustrate. We remember company a. So maybe we went on out to some company a's Wiki page and we've classified that this Wiki page, let's say, predicted that company a belongs to the cloud industry with some sort of probability. So we can do this, but then we lost a lot of information along the way. Like, we lost that company a belongs to the cloud industry through this Wiki page, which also, talks about NAR and, also what if company a has another Wiki page. So we don't want to keep track of all these things, and then maybe company a belongs to the software industry through some Wiki page and the cloud industry through another Wiki page. And maybe later on, we would like to run an algorithm on top of all our Wiki pages class reclassifying them since we got, like, a better algorithm. And we do not have to, touch or change anything with company a since everything goes through those Wiki pages. So that just gives us more flexibility and we don't lose any information, which is basically we want a good knowledge base. So we kept everything. Having said that, what I said about machine learning, this may require to query a fab graph for training machine learning and artificial intelligence models. As knowledge graph algorithms usually aim to capture some sort of correlation between nodes and edges. It link prediction, entity resolution, for instance, deduplication, classification, by using other nodes or maybe even link based clustering. All of these, can be done usually by predicting the existence of a a triplet via some sort of score function. So when you wanna train your algorithm on top of a knowledge graph, this simply won't work as there are these Wiki page intermediate nodes. And they disturb the algorithm because it doesn't need it in order to predict the edge between company a and the cloud industry. So this would be a better choice for the algorithm or the only choice that the algorithm can run on top of. But fortunately, this transition this transition between the full blown knowledge graph into a sub graph for the algorithm is usually very fast complexity as well as code wise, at least according to our data scientists as this is what they do. They query for a subgraph and train the model on top of it. I just wanted to see where I stand, time wise. So the last piece I wanna, touch upon is when you have a really reliable, knowledge graph, we can create new knowledge using the graph we already have and enriching it without going to outside sources using knowledge graph algorithms. And this is a paper we've, published about a year ago showing how we used our knowledge graph in order to perform customer segmentation into verticals, and the specific deep learning and machine learning models used in this study, relay on Wikipedia information as well as home page information and, of course, internal information we already knew about those companies. And all of the above, could be found in our knowledge graph, our knowledge base, is thus and and thus enabled this research. And if you find this idea intriguing, you can give it a look at, Google Scholar or the Intel AI website. So this was, like, an overview of what, we were trying to solve in our sales AI platform and how we grow a knowledge graph. A little bit glimpse of the architecture and the capabilities we found were crucial in order to make this happen. And I've I hope you found this interesting. And, I believe I'm I'm done, and I'm open to questions. We do have a couple of questions for you, actually. Okay. So some people some people were asking, like, if how did you manage to use Neo four j, with triples and the, semantics part and all of that stuff. But, actually, they already answered among themselves that there is a plug in that, for that from Neo four j that allows you to do that. So I'm going to Okay. Give you a more specific question, which is, what is the schema that you keep inside Kafka? And if you serialize your graph data, JSON or something similar. So, what was the schema that what's the schema that you use inside Kafka? What do you mean? This is the question. It's, no. So I I I'll I'll be happy to answer it, but I I don't really I don't really understand the question. So, basically, whoever asked the question, it's probably a good question. So you can ask me, like, in Slack. Maybe we can talk, and you can explain the question, and then I'll answer it. Maybe. Okay. So let's see what else we got there for you. Right. We have a question from who's asking how would analysts or business users, gather insights, from your knowledge graph, or is it only for use by data scientists? Oh, so, for leads, like, the beginning of the process finding, simple insights, like the partner company insights we saw. They can system analysts can, take this query the graph looking for some manually looking for some insights before we start automating them, showing those snapshots to the business units to see if they're viable. And if they are, we can start building, more robust models and solutions that will they won't need the graph image anymore like the the drawing. They will simply get the recommendation, according to the graph. But this is mainly the querying is done for, for the first stages when you're looking for hints of things which you can, get, benefits from opportunities from. Okay. There's another question from, Matthias Sisbuie, if I'm pronouncing correctly, who's asking whether you can speak about the iterative process to build, such to build this from scratch. So, actually, it's not it's as as you've seen, it it's composed of, like, five different microservices with a message bus. So this is more of a software engineering question because we first, we put everything into place, like, all these capabilities, and they were empty. Right? They did, like, simple things or they almost did nothing. And then we started once everything was ticking, we started putting in the logic inside the system. And, so but that's an iterative, microservice framework, development process we use, most of the times. We put, the entire framework, like, the key things in the framework. We mock it, and then we started putting the logic inside. Okay. I have another question from Deborah who's asking if you could further expand on how you handle entity resolution, meaning how the loaders and transformers know that two websites are referring to company a, but they're, for instance, spelled differently. I'm not sure I I understand, but I'll try. Like, the way we transform this information into graph formations, Currently, we did an expert that would manually define using this YAML, DSL we created which piece of information turns into what. But it does so one time as usually we work with, APIs which return data in a known structure. Right? Like, you get the JSON, you know what fields to expect. So we do that for each entity. But we do it one time and we let it run on all of the same type entities, if that was the question. I'm not sure. But even if it wasn't, I think I think the question probably refers to entity disambiguation. Say, which is a very well known and very thorny issue in Novice Graph. So how do you Oh, okay. Okay. So when you get the external information and you wanna make sure it belongs indeed to the company, like the example with the Wiki page. So we ran, when we fetch external information, we always have a matching algorithm that matches the internal entity that we already know is correct by company a with some external information we got from this Wiki page that claims it's company a. And we do this cross matching algorithm and we give it, a score. Like the. What I I I think it's confidence level that I wrote here and we give it a score, how much we believe that company a actually, like, this Wiki page actually belongs to company a. We do not make this a strict connection. It's kind of a loose connection with some certainty level. So if you know, it's query the graph and say, I want only weekly information about company a, which, is above ninety percent, certainty, like, confidence, if it makes sense. Okay. There's a quick one, and I'm guessing the answer is probably no, but I'm going to ask you anyway if you have, like, a gift card for this. Fortunately, no. I'm saying fortunately because, the the process of getting, extremely difficult. I would love to do so. I cannot do it on my own. So no. I I I don't, but I I'll be I'll be glad to contribute any piece of knowledge that I have to anyone that wanna talk. I'll I'll do it for free and for fun. That's that's fair enough. You know, an open invitation to people as well. Yeah. I have a there's another one from Mark who's asking something quite fundamental, actually. Why are you using your message bus? Couldn't you work directly on the graph database? Well, that's, again, a software engineering one. Since you wanna you wanna control one of the basic things with asynchronous system that you wanna control the throughput. You would don't want burst to turn down, like, destroy your system. That's why we have Kafka. You have back what's called back pressure. If you got a lot of information flowing in and you don't have enough connections to the database and the throughput of the database is not enough because there's an IO limit, the bottleneck will probably be your graph build. Right? And you want things, like, to queue up behind your graph builder. You don't want the the service itself trying to create a graph because a lot of services will try to create a graph and they just bring it down. You wanna control the throughput, and that's one of the reasons you use the message bus and the one of the reasons Lambda functions use, SQS, etcetera. Okay. Makes sense. I think we're good, actually. I mean, we can as you said, you're on Slack as well, so you can continue the conversation there if people have even more specific questions because I think some of them were actually quite specific already. So I guess we're good to wrap up. Thank you once again, Aran, and we'll see you. Thank you. Thank you. Bye bye. Cheers. Bye.",
    "transcript_length": 27459,
    "speaker": "Eran Avidan",
    "tags": [
      "cypher",
      "deep learning",
      "dsl",
      "extractors",
      "graph builder",
      "kafka",
      "knowledge graph",
      "machine learning",
      "message bus",
      "microservice architecture"
    ]
  },
  {
    "title": "Secret Knowledge: Visualizing Complexity",
    "description": "In this interactive presentation, where the audience's choice determines the outcome, we will introduce Data Art and demonstrate how it can be used as a new communication medium to tell impactful stories by connecting scientific rigor with creativity.\n\nMost of the presentation will focus on graph-based datasets born from complexity with a small detour through deep neural networks and other surprises.\n\nCome and vote to choose how our journey will unravel in real-time while being comfortable being your screen. Originality and novelty guaranteed!",
    "category": "Knowledge Graphs",
    "transcript": "So today I'm gonna make a presentation about complexity and more specifically tailored, towards like around visualizing graphs because this is a conference about graphs. But this, format is gonna be a bit, different from what, you're used to, I think, because I want to make a presentation and and to be very interactive, meaning that I will ask, questions. And depending on how you choose to answer, we will go through one or the other way. So it's very easy to do. Basically you just go here, you can scan if you have a phone, it's even better. You can just either you can join slido dot com and you put the data art as a hashtag or you can just scan it with your phone and it will automatically join. And so it's pretty cool because you will see that the results as you vote, are completely integrated in the presentation. So it's it's pretty cool as a as a feature. And I will ask you just to make sure that you're ready and to know a bit more about you. You this is the one what it looks like. Ask a question, and you can see on the left, you can always join, the present, like the presentation and the polling part. And so this is like a question because we're gonna talk about visualization and especially around graphs. So it's a good idea for me to, you know, try to adapt also the content and see, as you can see here, we would see in real time how many people vote. And it's cool because you see people are fighting. You can also edit your answers as we will go as we will see in the in the next slides if you want to, you know, go back and forth. So we have eighteen people out of I think fifty three. So let's let's get everyone, you know, get everyone on board And in the meantime, so, yeah, this presentation is really about, data art and I'm not really sure if you're familiar with the field. So I'm gonna first try to introduce what is data art, the difference between data visualization and data art, go through different example of graph visualizations. And, if you have time, if you are nice people, we'll also go towards, another kind of work that I do with more with with ai which is about neural networks. It's not exactly a knowledge graph, but it's still around networks. So it makes sense to talk about this here and at the end also have another surprise. So, let's try to have a bit more people. We have twenty five people. You can just join us. Go here on the website, put the hashtag or just scan here, the the QR code with your phone, and then it's automatic. It's gonna be refreshed automatically. You don't have to do anything. It's just, it gets automatic. And if you're not, if you don't have time to join now, don't worry. The next question, you always have this left panel so you can always join in and, you know, participate. So from what we have right now, we see that most people so you, you dabble, right? So, you know, about database and some people like really, like seven percent of, of people, people really like my life revolves around data and visualization. So you guys are really expert, but, for most people I think it's going to be pretty new. We know I have an idea of what database is and, but we want to know more. So let's, let's begin, just to quickly introduce, who I am quickly. I have a background in data science and, I also, and are like switched after my PhD in more into data visualization. But, today I'm going to go and talk about data art, which is something that I've been, like doing for, I'd say seven years now, professionally. So I basically earn money selling data art. This is, like, the definition of professional. Right? And, so this is gonna be we're gonna be focused about the work that I did on on data art and and then it relates to, graphs because, I did my PhD in graph visualization, basically. So it's gonna be very well connected. So I just wanted to introduce how I really started, you know, to, play and and, you know, work about data visualization. This is an actual picture of my board back in the day and you know, people were asking me, hey, what are you working on? You know, this is like the classic. If you do, like, a thesis, people ask you that all the time. And I was very naive at that time because I thought people were actually they were really interested in knowing the answer. So I was like going go straight to the board, trying to get a look, this is what I do. And then I realized this was just, you know, they were asking me just out of courtesy. So I was I was like a bit frustrated, but I realized, okay, maybe if I could, you know, try to visualize what I do, people will get more interested because you know, you can see it and it's not about math or code or anything. So this is how I started and get interested into the field of data visualization itself. So what is database? Basically, if I were to, like, an outline of three points, database is about to, it's to be the most efficient effective in communicating statistics or objects or numbers in a visual manner. Right? So the keyword here is to be effective, which is different from data art. So you want to have to minimize the time it takes for other the audience to understand what you see on the screen. For instance, here we see a map with dots, we see bar chart, a line chart. And just by, looking at the legend or the the title, like the description of the chart, it should get pretty it should be pretty easy to understand what it is. This is the goal of database to communicate, but also to explore. If you work if you work as a scientist, you you have no you have a big dataset. You have no idea of what it is. You're gonna start by creating some, you know, basic charts just to get your hands on the data, understand what what's the distribution, the length, the size, the clusters and so forth. So it's a very good tool to explore. But basically all in all, the point is to develop one's cognition about the underlying dataset. So this is like my definition of database. Now if you know a bit about database, you can, like look up on Andy Kirk's like website. Yes. Yes. Like I don't know, six hundred different tools, like everything that is related to data visualization. Most people here will be familiar I think with the tableau or power bi or click like there are three main competitors in this space. So So basically, if you're not really familiar with database, you take the tool, right? You go, here, this is tableau, you have your data, you drag and drop on columns row up, you have a chart pre made for you, you try to color, you put the time and that's it. You are doing data visualization. So why do we need to go beyond? Why do do we even care about creating custom data visualization when you have like good software that do it for you automatically and very efficiently. Well, all in all, it comes to impact. If you do like dashboarding all the time, it's difficult for people, you know, to have something like to remember the visualization to be remembered by because it they all look alike. So what if you work in database, you're striving for impact. So you want to be impactful so that the message that you communicate, the story that you tell, you know, helps people, take this new decisions or reason about their businesses and so forth. So if you want to have impact, what do you do in general? Some people, you know, they go on the street and they protest. I come from France. This is our specialty, as you all know. So then when they go on the street, they try to protest to influence people, right? This is one way, I'm, I'm trying to do the other way. So this is supposed to be a mirror and I'm more, you know, maybe, you know, Michael Jackson, man in the mirror song. I'm I'm think if you want to like influence people, it has to come from within and to do that, what better way than using art to do so? Because art is very subjective in its nature. So if you can, if you're touched by a piece, you can really, you know, it can change your the way of thinking and how you perceive life in general. So instead of doing the protest because I'm alone, I cannot really organize anything. Maybe we should try to go more on the artistic routes because it's less frightened, frightening. It's very deeply personal and you can have a greater greater impact. So this is basically how I, you know, started and doing data art and say, okay. Maybe if I can do a piece and I can have more impact, then my goal is complete. And my goal being as a scientist, try to communicate science, make people understand complexity, such as, like like, understand how graphs work, the connections, and so forth. So this this, like, was the whole motivation here, and we're gonna talk about this. So, basically, if you look at at this particular piece, you have no idea what it is because there is no description. And, but you you believe me when I said that this is based on data. So what is data art? If we are to, you know, give a definition, data art is not data visualization because the goal here, the most important part is to create emotions. So first and foremost, you want to maximize emotions. And hopefully for instance, if you see the piece here on left, you want to get closer because maybe you like the colors, maybe you like the shape. But as you can see below here, you also have a description and this is where the artistic message comes. So in some pieces you have no description at all and it's up to the viewer to understand what's what's really happening here. Data art is different because it is based on data. It has a more objective truth than the solar arts. It's imagination. Right? So basically you get it attracted by the colors, the shapes, so forth. Then you read the description and hopefully you reflect on yourself. This is how the message gets to you. And all in all, back to the same thing that we have in database, we develop cognition about the underlying dataset, the scientific principles and other types of things. So this is basically my definition of data art. I hope that it was clear the definition between database and data art. And you could you could like, you know, ask me, okay, why are you are you the only guy doing this data art? And I'm not. This is from a paper from back in the day two thousand and seven on the database conference. You showing the merits of doing artistic data visualization. So this was, like, something that people noticed, like, even even before that. But, you know, trying to understand, explain that if you put emotions, you have a greater impact and people find the charts or like the images more memorable. And so it's actually a good thing to have visual embellishments even though it makes maybe sometimes the data a bit less clear to perceive at first glance, but because it has a greater impact and they can measure that, all in all, it's better. So this is how I try to also connect the artistic part with the, like a field of research as well. So if you are interested, you can, look it up. But basically, how how does it work? What what is the design process to to create some graphs and art with graphs? For instance, what it's very similar if you come from a design background, this should be very familiar. It's exactly the same as database. It's gonna be interactive, meaning that it's not like, oh, I have an idea. I'm gonna work two weeks and then that's it. It doesn't work like this. It's sometimes the most frustrating part is, like, sometimes in two hours, you have you have you actually have the final design, but you spend three weeks refining the thing just to go back to the first one because the client likes it. So it's it's iterative. But the question that you have, you ask yourself is like, are you going to create a static piece, like an image? Is it going to be animated? Maybe it's going to be a video. Is it going to be an interactive data visualization or data art piece so people can click or have like sensors to make it move in real time? Is it gonna be abstract or figurative? What are the different shapes that you can play with? What are the dimensions that you're going to take in the data, the clusters, the different techniques and have to strike the right balance between order and chaos. So exactly the basically, I would say the the things you have to take into account and colors. Of course, people like to do black and white. I like to do colors. So, and because it's data are they have to mean something. So if they have the same color, they should belong to the same cluster for instance. And, so they have a mean. So, basically, if you had to say, like, to describe my job in one side would be data, some code, and basically you create a visual that that that means something. I hope that it's like one data art in one side. This would be it. So now I wanna go back to the example that I showed before, and this is about the Montreux Jazz Festival, one of my favorite examples. So if you're not familiar with the Montreux Jazz, this is one of the most famous music festival in the world in Montreux. Montreux is in Switzerland. And, just to give you an idea, it's pretty big as a festival. It has been going on for more than fifty four years now. And is that it has eleven thousand hours of videos with different cameras, six thousand hours of audio, more than a hundred thousand pictures, but and, basically, the dataset is two point seven petabyte and, like, spread across two different dataset cooled by the water of the Lake Geneva. So this is like the the data we have to, play with, which pretty big. Right? And basically what I I tried to make some visualization to give you an idea of what you have in the dataset. Here, each dot here is an artist that played at the Montreux Jazz, with this, his location, its location. And you see that the the notes here have different shades. They're more, like, the more purple you are, it means that the more connected musically to the other artists. So you can ask, how do you compute the musical similarity between artists? And to do that, we have to analyze the sound itself. And remember that mantra is a whole new setup. It's not the same as the studio version. You have to analyze the rhythm, the melody, the notes, chords, harmony, things like this from the reading, the signal, then try to compare songs together and see if this sound alike, and then grouped all these songs by artists. And this is how you get artistic similarity, which is pretty cool because people have been using this particular map, for ethnomusicology. So to understand the origin of music, for instance, here would be Bikin, which is the most famous festive, like, musician and the jazz and see how he influenced with blues, you know, other people that came in the festival as well. So this is an example of a database. This has nothing to do with art, but just to, as I said before, to give you an idea of what, what is the dataset, and then we progressively move on to the more artistic part. Another visualization that we have, this is about the evolution of musical genres over time in the Montreux Jazz. So as you can see here at the beginning, it's like the the curve is really narrow because we only have, like, the festival was basically three days in one, like venue. But at the end now it's supposed to be fifteen days in three different stages. So we have more in terms of volume and more songs, getting played each day. And what's also interesting here is that the proportion of jazz, which is the pink one in the middle is more or less constant over time, but we add more and more different musical genres. So it's not about jazz anymore. Even like back in the day, as you can see, you have hip hop, reggae, metal, electro and so forth. So it's really like a very international music festival even though it's, you know, it's dead centered around the the jazz. So this is a bit more abstract. This is called a stream graph as a visualization, and it shows, like, the evolution in time with the width being proportional to the number of songs. And now we I want to go back to the image that you saw before, the very which is about the artist of the jazz itself connected if they've been playing together on stage. So basically you concatenate and you you flatten all the concerts in one dataset. And because, you know, artists, they tend to come several times at the Montreux Jazz and each time they come, most of the stars, they come with a new lineup, a new bassist, a new drummer, a new singer, sometimes if you're like a guitarist and so forth. And so you can connect all the different artists of the jazz if they play together, on stage at a certain point in time. And this is what we had here. So this is like a very nice, simplified schematic, but if you scale it to the, like, whole dataset, not the whole dataset, but like the most interesting part at the center, you would see here, all the connection, the big yellow dot here is Vivi King. You have Santana here and George Duke and you see that all the lines seems to converge at the center. And it makes sense if you think about it because the artist at the center are mostly jasmine and blues men because it's the montage jazz, but also because the music itself, you know, calls for collaboration, you want to jam with other people. So if you're a Jasmine, you will change you. And each time you will come up with a new quartet quintet. And this way you have, you get very connected at the center. So all the lines seems to converge here, and you can be very popular as an artist. And I've just played here in this small cluster with your band. But if you didn't really mix with other artists, you will not be connected at the center. So we have this, what we call the Montra jazz galaxy. The whole piece was called jazz luminaries and it was displayed in this, full dome. So it's like a small, very small planetarium. And we also, created this controller here, which is actually a phone inside where you can, you know, play around. So it was interactive. You could select an artist. And when you click on the artist, see all the different concerts and then listen to all the songs. So imagine being able to listen to more than two point seven petabyte of data in your fingertips. So it was pretty cool as an experience. I want to show I I can only show you, like, part of the video because everything is copyrighted as you can imagine, but it would basically look like this. People will, you know, lay on the floor and have this weird controller. And as you will move it, you know, in space, you would create this fisheye effect and on the three d, like graph that you see here with all the different, pictures. And then if you click on the the most, the selected note, you would see that everything will slide and all the different concert for this artist would come up. Then you click on the concert and you'd be able to browse the different tracks. So this is, I hope that I know it's a bit, underwhelming because you cannot see the whole experience, but I hope that you get the idea through this, like, smaller video. So this is, like, the the the two d, render of the three d mesh that we we saw before. And then what we saw just before the BB King stuff is we get the center here. I hope that, this example, and it gives you a better idea of what data art is and how we can go from the the old data, the data visualization, and then create something more abstract, which has a story and and to arrive to this particular piece. But now I want to get back to what I said at the beginning. And I said at the beginning that you were there here, right? So maybe you recall those books back in the day where you, you know, if you want to make this decision, you go to page eighty four and then you, you turn left and you die. And most of the time you have to go back to the beginning of the book, but we're not going to die here today, but you are going to be the hero, meaning that depending on what you choose, we will go through one of the other different artworks. So you are in control of democratically of what we can do here. So I will first ask you to pick a shape. So I'm gonna ask you this question right now, here and it should appear on your phone. And so let me and to make sure that it's true, we can also see in real time. So I'm not cheating. You can see that you can change. It's up to you. So we have thirty seven people voting beforehand. So let's, let's try to have a bit more today. And now in this particular question, what's really funny about this is that depending on the demographics of the crowd, you have different choices. I give one of, like, kind of this talk in the US and in Europe, and it's it's always different. But Square, you know, just really seem to be, like, no one likes square. Either a circle or a triangle and you see people are fighting. If you if you're at a square and you feel alone, you can also edit your answer and choose one or the other because sometimes you have like a it's a perfect match. So I have to choose it randomly. It's not not because I said that square has no love. People are gonna going back to edit the answers and and, you know, changing and go back to square. So this is really, you understand how politics work politics work, right, with this kind of line voting and stuff. So okay. I think so. Circle and triangle. So if none of the square, okay. Triangle is a bit better. So let's go with the triangle. So sorry for the circle. Sorry for the square. Let's talk about this one. So if you look at this particular image, basically, you have no idea what it is. Exactly the same story as what we had before. But what you can already notice is that we have different kind of cluster, which seems to be connected, but, like, still well isolated in different colors. So this particular piece is about papers, papers and patents, actually. And this was a collaboration with the professor at at EPFL working on the in the like, it's a knowledge graph really. And and this field of study is basically knowing, like, innovation, how and how we relate technological products that we have, like an iPhone or anything, to the the research that, you know, originated from, for instance, papers back in the fifties will which led to, like, transistors, screens, and so forth, and to have in the end the technological products that we have in our hand. So all the story of how innovation, you know, evolves in time. And so basically, sorry for the French here, but what you see here is a connection between patents. So if you have a white, border, it means it's a patent connected to papers. Because when you file a patent, you basically need to do the prior art and, you know, you can cite other scientific papers. So it's really interesting to see how long it takes for a scientific paper to get in per field, you know, to get cited and how long it takes to get patents. Sometimes you have tons of new patents which are not connected to scientific papers. So you are really wondering, is it really innovation? Is it like a radical innovation? Is it a very well kept secret? You know, so you all these kinds of questions. And as you can see here, all the nodes are lined up according to the time, basically. So the the last, state the the last story was is, on the fifties and below. So it starts really, like, back in the day, I would say. And here it's two thousand seventeen, I think. So this is like the when I got the dataset, basically. And you can see here, this is about computer science. This is electronics. This is cosmetics. And each time you have a different, I think, thirteen different fields of research. And you can see, like, for instance, computer science, you know, sites more papers and that it goes deeper here. So So it has more ties to scientific research than cosmetics, for instance, here or it's it starts to vary like very this is maybe the eighties and you have tons of patents and less papers. And here, as you can see, we have few white notes, but tons of black notes or black borders. So it means that more research than patents, for instance. So this is, like, the kinds of, like, knowledge that we can extract from this particular, not necessarily this visualization, but also the the dataset itself. It's really interesting because it is based on the the patent office in Bern. It's it has more than eighty million patents. This is, of course, just a small subset where we have we we actually we make sure to have also the link to the scientific papers, It's not easy to do because you need to parse, you know, all the NLP natural language processing to match the authors, the name, the themes and so forth, which is quite difficult to do, but really interesting as a as a field of research. So this is this was one example. Let's move on. Let's take another ask another question. Now we'll ask you for a color and you have to vote, you know, your favorite color green, blue, yellow or red. And, according to your choices, we're going back to the to the piece that matches what you chose and talk about it again. Blue is always, one of the, like, crowd's favorite. I think I don't know. It's okay. I don't want to influence any of you, but I think that we have a clear winner here with the blue. So even though you can you can still change your choices, but I'm not sure we're gonna talk, you know, the, the blue choice. So let's go. Let's, let's talk about blue here. This one this one is actually I'm not sure if it's my favorite piece, but I think it is. So this one is a very personal story, for me. But once again, just think about it. You have, like it seems like the nodes are so small that you don't really see them anymore. You only see the edges. You see there's like there's like a a cluster at the center with blue and red. And maybe you saw this picture. It was, quite popular back in in the day. This one is about Star Wars. Star Wars before eleven, you know, the new episode seven eight nine. Okay? So we forget this and we go back to the real star wars. This is about the star wars character. Basically, what we did here was to crowd, you know, scrape Wikipedia, which is the Wikipedia of Star Wars and try to extract the the all the all the characters of the universe. And maybe you're not like, you know, maybe you don't, not a diehard star wars fan like me, but on this Wikipedia, you have more than one hundred and sixteen thousand pages created by hand by users describing the universe. It's more like forty years of, you know, data of books, video games, comic books, series, everything related to Star Wars. People that make all the description and so forth. So it's really, like, very complete as an as an archive. And why is it really important for us? It's important for us as humans because all the con the corpus here is already digitized. It's in English. It's quite easy to, you know, extract. Where and but the thing is if we can develop the tools to, you know, match the characters and how they progress in in time, because you have, like, star wars era spends over thirty seven thousand years. So it's pretty big as a universe. We can develop the tools and then apply them back to our own history. But the thing is with our own history, most of the manuscript are in ancient language, who are difficult to to read. They are not digitized. So it's it's much easier to start first with a fictional corpus such as this one, and then go back, develop the algorithm, and then try to apply them, on our own history. This was like the motivation. But the real in truth, the real thing is because I love Star Wars, but, you know, I have to keep appearances and say that this is really for us, but I love Star Wars. So what you see here is a whole different characters. And for instance, at the center, you have Darth Vader, Luke Skywalker, David Palpatine, Han Solo. And in yellow here, the yellow nodes are the criminals, which are actually very common in the Star Wars universe. You have Chabalahat. And if you look at his connection, the he is mostly connected to other villains. So people from the dark side of the force in red versus the blue one, the blue guys, the republic or the the jedi's and so forth. So you have this, you know, duality between light and dark side as you can see here on on the screen. Back in so when I did this piece, I think it was in two thousand fifteen, we had more than twenty thousand characters, but now we introduced new episodes. We have the Mandalorian, which is great, with also new characters and so forth. So you can the the universe expands as we progress. Right? And so we we could redo the same work and see how the the the universe would evolve. So this was about Star Wars. Now let's continue. Another question. This one is a bit tricky. I'm gonna ask you whether you want to, you know, if you you feel adventurous or not, and, you feel like opening a Pandora's box, you know, I tried to come up with creatives and I and we see people are, you know, they, they want to risk everything or they, you know, they are more careful. Okay. I think it's pretty clear that you want to see what's inside. So yeah, I think that there's no, people are very, you know, daring. Okay, fine. So you're daring. I wanna talk about this piece here, which, is also about, you know, knowledge graph. And, this one, it's it's interesting. It was also made for a company, and this is about LinkedIn. So this is what you see here are connections between people connected on LinkedIn. And it was really interesting as, insights because so basically how you create this, you go to your connection tab, you can now, everyone can do this on. You can export your data. What I asked is, like, the the people in the company that were willing to all of them export this, remove all the IDs and and and the identifier, like and try to merge everything. So we have, like, the corporate network of the people working in this particular company. And what's what's really interesting is that we could, you know, yeah, let me ask you the question. What do you do you think that people are if you work in the same company, do you share the same network more or less or do you have your own network? Right? It depends. You would say maybe if you work in a big company or international company, you don't know necessarily the same people. But in this particular case, it was about, a digital agency in Switzerland. So I thought that people were, you know, in Switzerland, it's quite small, so everyone will knew each other, but it's not the case at all. What you see here, all the the the notes here at the center are the people from the company. And, the notes that you see at the outside are their connections. And what we are really interested in here is to see how the percentage of overlap. And we found out that only six percent of connection were shared between the employees of the same company working in the same sector of activity Was actually quite surprising. I thought it was, you know, everyone knew each other, but it was really not the case. So in terms of story, it was really interesting for the company itself because now you can say, look. We we bring people from everywhere in Europe and each time they come, they bring their own knowledge, their own network. And, yeah, let me show you the proof. Here it is. We also analyze like, like tons of different stuff about, you know, managers and like very something that HR would love to to have, and this is what we did. But I think it was striking visually to see, we see all the different members and see all their their connection then that we don't necessarily see tons of overlapping here. You have some small edges, but not that much. Only six percent, actually. So this was another, interesting project just to show you the closed box was, exactly the same network, but the previous iteration. So it was exactly the same network. But, you know, it's, this is where I started from this image and try to, you know, expand and to create something a bit more crazy. This is, I would say, closer to what you would have in, you know, abstract modern art, and this is really purely digital. There's no doesn't really have any, ties to another art or the older art movement. So now okay. As I said before, I wanna talk now a bit more because we I think we have a bit of time. We have, thirteen minutes left to talk a bit more about, AI because, okay, it's still related to to networks. Right? But it's also very creative. And I hope that maybe I you know, if I I can, show you some creative applications of AI, it, it makes you wonder how you can also explain that to your clients or other partners because here we all work in tech. But if you talk about AI for the general audience, it's about, you know, terminators and that robots are gonna kill me. I'm gonna lose my job. And I want to change this because this is not entirely true. And there's like very good things to talk about with AI, but, and showing something more creative, you know, it helps people, you know, understand how it works. And they are, as once again, they are not frightened by the technology and they want to embrace it. And I think that's a message in terms of impact, it's much more powerful. So something that you're also very, like, it's very common. It's using generative adversarial networks, GANs, to synthesize new images such as this one. If you go to this website, this person does not exist. Basically, each time you refresh refresh the page, you would synthesize a new face, which is almost, like, photorealistic. Here, the hearing is a bit, weird, but I think new iteration, if you get lucky, you cannot distinguish the real from fake. So, basically, you have a a network that synthesize fake images, and another network tries to to check whether this image is real or fake. And once it's trained, then the network cannot make the difference between real like training data or fake data. And this is where this is how you can create some, like, photo realistic image. Now this is not very creative. This is about face synthesis. So how can we use the same technology to to create something more interesting? I will first ask you to vote whether you are more a micro or macro. So let's see. Okay. If other people will have the chance to, you know, change their answers or not. Okay. Well, I think we we all agree on Macron now, so let let's go. Now if I if I had to tell you if I, in fact, told you that you can actually, train a network on a thousand different objects, and then you can compose them to synthesize a combination of the object that you see. Now that you have this particular image and you have, like, a weight of fifty two percent for a mask, a crab, gorilla, space shuttle, birdhouse, coral reef. You know, all these ingredients that you see here, you can combine them to and synthesize a combination of them. So just look at all the ingredients that you see here and trying to imagine something. I think it's gonna be pretty difficult. I'm not even sure if you know, I like I don't even know what what this was or, you know, But this is what the the power of AI. This is where we can use it as a tool to, you know, sparkle creativity and get surprised by the results. So, basically I combined the old, all these ingredients and this is the result. And to me, this looks like a more like a macro shot, you know, of something that it's both insectoid and our same time, like plant, you know, vegetable or something organic at the same time. So you can create very abstract shapes as well. Right? We saw like very realistic faces, but it's all about the training data and how you combine all the elements. So this is, this was a still image with the different elements. Now I'm gonna show you videos and see how you can evolve and, you know, make this whole animation go very smooth. So but first, I have to ask you, you know, the drill. Once again, whether you're more like the sea person or land person. Oh, it's funny. You know, most of the time, people, like, choose the sea, but, it seems like we have land people here. So you're more, you know, far from the sea. So okay. Maybe I'll let you guys vote to change your mortgage. So let it is. So I'm gonna this is a it's a video synthesized with AI. So I'm gonna mute myself and, you know, pass the the the it has some custom music made for the video. It's on YouTube. You can also watch this at the end, like, on your own. Try to to see if it works. The full video is in four k at sixty frames per second, so it's pretty nice as a resolution. Let me try to to play it to you now. Alright. So this, video, is called and this animation is called evade evading confinement or lockdown. This was created at the beginning of, lockdown. So the idea was, you know, to create, surreal landscapes and then move according to your mood, basically. All of so the quality was not really great, but, if you watch it on YouTube on your own, I think that would be much better. But you see how smooth the transition were because you can really synthesize all the intermediate image images that you need just by moving the weight just a tiny fraction. And and so that it makes very nice, animations over time, which I really like. So this was an example of a video made with AI. This is like another, like, the other choices that you didn't select. As you can see here, you have the abiding confinement. This was projected on in Paris in e p seven, which is like a a hyperbar you see on the on the wall. So it's pretty big and was more than fifteen meters, wide and ten meters tall. And so it was that displayed here for ten days. It was pretty good, you know, to have the also feedback, people asking me what it was about. And, you know, once again, doing my science, work and talk about how it's made with AI. You see it's nonthreatening. It's cool and technology and so forth. And people were really happy at the end instead of just having, it's gonna replace you, your job and so forth. So I try to to emphasize and and show the positive outcomes of technology instead of, you know, just pointing out to to the negative. So that this is like a small video to to have an idea what it would look like. So but, yeah, to sit in this piece here. There was no sound, though. It was a bit shame, but, it's okay. So another application that we can create with AI is that now we can get inspired by the shapes that are generated by the machine and, you know, going back to inspire the database part or the data art part. And this is what I did here. You see, this is to me, it looks like a flower bouquet. Right? But but it was really inspired by it because I saw my network was synthesizing these shapes. And I I realized, okay, maybe I should try to, you know, take this knowledge, graph this network, and try to change it so that it will look like a flower bouquet. And you can zoom in here. Each dot is a person, and they are are connected to to their, hierarchy to to their boss, basically. So this is about a whole group. This is like the global CEO, and you have the top executive, and then you can actually go down the hierarchy to the intern of the intern of the intern and at the top here. And instead of having, you know, something top down, I'm the boss. You are my employees. The idea here was was as a CEO, I let my, you know, my what we do together makes, we grow as, as a company, but also as human beings. So this is more like a flower like shape instead of being, something very hierarchical. This was, like, the artistic idea behind the, the piece, but it was really influenced by AI. So this is how to go back to the to the knowledge graphs and so forth. So I think we have we have a bit of time. I think I want to show you one last, surprise. We have four minutes because, you know, what what have been presenting so far is really based on math. Right? You have either for deep learning or visualization or knowledge in itself, knowledge graphs. This is about, you know, doing layouts and stuff about math. So I have a an example to show you, something that you are all familiar with, I think, in theory, but you are maybe not that familiar with the recent advances of of this. So I want to talk about fractal art. You know, this is like very something that we see quite often. This is the Mandelbrot set. So a fractal is a shape made of parts similar to the whole in some way. This is the official definition and fractals are really omnipresent in nature. It could be in leaves and your heartbeat and crystals and vegetables and mountains and snowflakes, like everything is fractal. So this is what we have, like, a standard modern broad set. But the software that we have now is really powerful and can create very different looking shape. And to prove that to you, I will ask you to, you know, pick a side and choose whether you are more like an organic or geometric guy and and see if you are surprised by the results. So, okay. So it's almost fifty fifty, but we we can have more people. I think we can have ten more people voting. So organic. So you are very, like, interested in knowing how something so, like, mathematic and geometric could be more looks like more organic maybe. So let me oblige this. This is a fractal. This is pure math, mathematical function. And to me, it looks like a virus, you know, and it's was created during the coronavirus time. You can all, like, see that it's a fractal because at the at the edges here, you see that it repeats and repeats, it repeats, but at the same time, it looks very, you know, around and something that you could almost find in the, under a microscope, but it's just pure mathematical functions. Another example here, this called this one is called a brief. So it's about, the virus as well. And here, it's actually data fractal. So it was created five months after the the initial COVID you know, announcement. And you have five ring one, two, three, four, five rings. And the cool thing about this fractal is like now we are almost a year in the pandemic, so I can, you know, change the camera and create more and more you know, circle like shape like this so I can actually make it move over time And to me, it looks like, you know, blood flow and kind of like oxygen stuff. So, this was the idea behind the piece, and I always try to incorporate some small data so that it has more connected to the data art realm. Lastly, I want to show you this example, which is also a fractal, but to me looks like more like a sandstorm. And this was like a three d render, the three d fractal, but the render itself is almost, grainy with particles. And the old to me almost look like like a sense storm, something you would see in the mummy or dune or something like this. But once again, this is just pure, math. So I wanted to show you the contrast between, you know, what we think a fractal is in the Mandelbrot set and something we can which can be very organic. So to, to con I have one more video to show you. And, so this one is a combination of fractal and, data. So I'm gonna talk over it. What you see here is the fractal that moves according to the increase of sea level, and the data comes from the NASA. So as the sea level rises, we see that, the shape at the the center starts to grow, grow, grow, and if it grows too much, then it will, like, basically explode. So this is what you see here. And you also can notice that camera itself seems like to be zooming in and out very slowly, and this camera movement is also, matches the tide in France and some particular dates. So I try to parameterize everything that you see here according to data and try to, you know, use the this message to, you know, warn about the sea level increase. So something that is we are all, we're all in this together. So this is what the the main idea, and as we go back, then we smiled. It's a fractal animation. So this is my as I said before, try to influence both, merge two ramps of data and art at, in, unexpected ways. So to to conclude this presentation, I'm gonna I'm gonna ask you one last question, and then we're gonna maybe have time to, to take some questions. So basically just to let you know that creativity is the most demanded soft skills of two twelve twenty twenty and was also the same, last year. So if you are guys are interested to learning how to do stuff like this data art, I have a YouTube channel, and I also, you know, you can connect with me on different social media, and, I'll be happy to, to to talk it. So to conclude art upsets and science reassures. I hope that you liked this presentation. If you have any questions, thank you. I think we have some questions. Yes. Kiran, can you hear me? Yes. Okay. Great. Yes. We do have, unfortunately, we don't have as much time as we'd like to because we're already a bit over. But there is one specific question asked by Michiel, who's also one of our speakers that I just have to address to you. So, his question is, how can a little bit of FARC help in the process of designing commercial applications powered by knowledge graphs? It's a very specific one because this is what he does. So, he said that for their customers, it's often difficult to imagine that the different types of entities, allow different, and new perspectives. Do you have any advice on how to trigger that? It's a difficult question. How can a little bit of I see it in chat handling process of designing. Well, it's really about the message you want to communicate. Right? So it's difficult to automatize the art creation part. Unfortunately, I don't I don't have, like, a premade algorithm. I click and it's beautiful. But I think it it goes back to the story, right? If you can convey a bit of emotion, then your brain, basically the knowledge graph is read the left side of the brain, right? You about the logic, rationality, and so forth. If you trigger hard in the right side about knowledge about emotions, and basically you combine both of them, the feeling it's much stronger. So So you can use that as a way to convey that your solution is very interesting. It offers new perspective and so forth. But, it's not easy to do. We need to, you know, hand it's something as a handmade for so far. Yeah. That makes sense. Okay. Well, we'll we'll have to wrap up, because otherwise, we'll be eating into the time of our next keynote, which is also very interesting, which is done by Iran Avidan, from Intel. So our operator will wrap up this session. We'll keep your questions and try to address them to offline. So thanks once again, Kiel, then onwards with the rest of the event. See you. Thank you very much, guys, and I hope you liked it. See you soon. Cheers.",
    "transcript_length": 46631,
    "speaker": "Kirell Benzi",
    "tags": [
      "AI",
      "Andy Kirk",
      "Mandelbrot set",
      "Montreux Jazz Festival",
      "NLP",
      "click",
      "data art",
      "data visualization",
      "ethnomusicology",
      "fractals"
    ]
  },
  {
    "title": "Systems that learn and reason",
    "description": "After the amazing breakthroughs of machine learning (deep learning or otherwise) in the past decade, the shortcomings of machine learning are also becoming increasingly clear: unexplainable results, data hunger and limited generalisability are all becoming bottlenecks.\n\nIn this talk we will look at how the combination with symbolic AI (in the form of very large knowledge graphs) can give us a way forward, towards machine learning systems that can explain their results, that need less data, and that generalise better outside their training set.",
    "category": "Semantic Technology",
    "transcript": "Good morning or good afternoon, depending on wherever you are on the globe when you are watching this. My name is Frank van Harmelen, and I would like to discuss with you in the next half hour AI systems that learn and reason. So first, let's take a, a broader look at at where I where modern AI is is going. So here are are just two quotes from from recent blogs and from the web page of recent AI centers that have been initiated. And they all talk about making artificial intelligence more human centered, making AI systems that support people and that are competent partners. So the emphasis in modern AI is is less on replacing, people by AI systems, but more on AI systems that collaborate with people and support them. But at the same time, here's another quote from a popular blog, current AI systems are often incompetent because they lack background and, and contextual knowledge, and they cannot even explain their actions. And so, clearly, current AI systems with their lack of background knowledge and and their lack of contextual knowledge and their lack of ability to explain themselves, they are not very human centered, and they cannot support people, and they cannot be competent partners. Right. So, what's holding AI back? Right. So why are we in the situation where, on the one hand, we would like to build systems that support and collaborate with people, but on the other hand, our systems are somehow incompetent and they lack background knowledge and contextual knowledge about the tasks that we ask them to do. And, a a common analysis these days is that what's holding AI back is that for a long time, AI researchers have locked themselves into one of two towers. Now we all know the famous story of the two towers. Right? And, in the case of AI, we could call these the symbolic AI tower and the statistical AI tower. And as as always in a kingdom with with two towers, one tower thinks that the other tower is evil and that they are wrong and that you should not really discuss, with with them. You should even fight them maybe. And in fact, maybe there's even another analogy that is appropriate here, which is that AI was not only divided as two towers, but maybe, you know, people even had sort of different religions about what AI should be. You could either be of the statistical AI religion, in which case you would build neural networks, and train, machine learning programs, or you were of the symbolic AI, religion, in which case you would build knowledge bases, you would build very large large models graphs, and you do inference over them. And just as with religions, you know, statistical AI and symbolic AI, they had their own books, they had their own meetings, and you didn't really need to talk to each other because, you know, the other people were wrong anyway. And fortunately, I I think in the last few years, we are beginning to see in AI an increasing movement that these, these two different streams in AI, they are beginning to talk to each other, and they are beginning to understand, that we actually need them both because our brain uses both. Right? Our brain does both the the stuff that that machine learning is good at, and it does the stuff that knowledge representation is good at. So here is, a very rough sketch of what's going on in our brain when we look at an image. Right? So here's a a cross cutting of somebody's brain. They're looking out towards the right. It's their eye. So what happens when, light hits the retina? Well, when light hits the retina, of course, there is the optical nerve that go to the visual cortex. Right? We call this the the bottom up signal. Right? And this this bottom up signal, when it hits the visual cortex, the visual cortex analyzes, forms, shapes. It it recognizes separate objects. It recognizes motion. It recognizes object of conclusion, and, it interprets the scene. And these signals then go to our prefrontal cortex, where we have, memory, where we do planning, where we do problem solving. But interestingly enough, in our brain, there is also a downstream signal. Namely, the prefrontal cortex is continuously telling the visual cortex what to expect. So it's not just a matter of the pixels feeding the symbolic representation, it's also a matter of the symbolic representation telling the pixel processors, what to expect and how to interpret all these pixels. And in fact, without such a downstream signal, our visual cortex would never be fast enough to interpret all the visual signals that come in. And, and actually, the downstream nerves, are actually larger than the upstream. So there is a a constant traffic up and down between the symbolic and the statistical parts of our brain, if you allow me that, that freedom. So now we could call the upstream signal data driven, as in neural networks, machine learning, and we call the the downstream, signal knowledge driven. And, actually, the history of AI also shows this. It it is not the case that the history of AI in the past, we did symbolic stuff, and now we realize that the symbolic stuff was, wrong or somehow mistaken. And now we are seeing the light and we're doing neural networks and and, we're making good progress. The actual history of AI has been a constant pendulum between these two streams, and frequency of this pendulum is about a decade. And, and certainly in the last decade, we have seen large major breakthroughs in the the the connectionist data driven statistical machine learning, part of AI. But we have also seen important, scale up, in the the symbolic, and knowledge based logic driven and reasoning part of AI. And, we are now seeing the pendulum swinging back to the middle of this spectrum, and, hopefully, we can this time, we managed to stop the pendulum somewhere in the middle so that we can combine these two. Now to put a little bit more flesh on this comparison, now let's compare, these two streams of AI, and and here is a a table that that very briefly compares the strength and and weaknesses of AI. So let me go through this table. So first of all, the the construction costs. Right? So as we know, symbolic AI and and for this, audience, you are all very familiar with with very large model graphs. These very large knowledge graphs, they are, not they are large. They contain billions of facts and rules, but they are expensive to build and expensive to maintain. So either through crowdsourcing or through corporate, investments. Right, but they are expensive to build at midday. But the connectionist approach also has its construction costs and here the construction cost is called data hunger. So many of you will be familiar with the Nvidia benchmark where Nvidia scraped, millions of, faces of people from the Internet and trained the neural network to generate new faces. Right? Now the neural network is so good that that humans can no longer distinguish between real faces and and generated faces, but it took ten million training samples to train the network for this. And, of course, NVIDIA was very clever in choosing this domain because for human faces, we can actually scrape ten million training examples from the web, but now try to do this for, say, rare cancer tumors. Right? So images of rare tumors. You cannot obtain ten million ten million training sample. The same holds for for AlphaGo. You know, the famous, DeepMind program that, that beat the best, human Go player in the world, it took four point eight million training games, mostly AlphaGo training against itself. And, again, game playing is a domain where you can basically, you have infinite amounts of training material. And, again, this is not true in in most of the the business cases, that you would meet in practice. So I'm always a little bit annoyed when, machine learning people tell their customers, well, we can't really help you because you don't have enough data. Well, it's not the case of the customer not having enough data. The problem is that the machine learning algorithms need too much data. So you see that, they have sort of complementary strengths and weaknesses here. And, as we know, these symbolic methods, they suffer from the combinatorial explosion. Right? So they they get worse with more data. Right? This is called the combinatorial explosion. But the connection is the neural network approaches, they get worse with less data. Right? So, they they suffer from this data hunger. So again, very complimentary strength and weaknesses. Another, you know, by now well known complementary strength and weakness of the two systems are their explainability. These symbolic systems, they have the advantage that their vocabulary is very close to the vocabulary that people, everyday people or experts, use. So here is a rule about a particular medical system, sim a medical symptom, dysphagia, which, then causes, some, a particular conclusion, namely a medical diagnosis. Right? So we have a variable, we have a condition, we have a conclusion, and this is in an understandable format. And, as we all know, these neural networks don't have this. Here's a famous example from the literature where researchers took a neural network that, with reasonable confidence, recognized this picture as a panda, and then they added noise to the picture, noise that is invisible to the human eye, and suddenly, the picture the the neural network recognized the picture as a given. Now you would want to know what went wrong, what happened between this one picture and the other picture when we can't even see the difference. Now what the the neural network engineers did is they carefully added noise from the picture of a given, and they carefully drove the network down into another local minimum in the high dimensional, fitness landscape where the network training does gradient descent. Okay. Now that's clearly not an explanation that you can give to to any reasonable user of such a network of of such a neural network. So this is known as the black box problem. And finally, as we well know, symbolic systems suffer from what's called the performance cliff. Think again of a knowledge graph. If you query the knowledge graph for facts that are in the knowledge graph, you get very high quality, you know, both precision and recall. But if you start querying it for stuff that is outside the knowns graph, basically, you get no answer. Right? So there's a very steep performance cliff. But, also, these connectionist approaches, they suffer from a a performance cliff. Right? Here's a famous example from, an an an image labeling competition a few years ago, and the question was, what is this lady wearing on our head? And the answer was with near hundred percent certainty, she's wearing a shower cap. Now, we all know that this is not a shower cap, because we recognize this lady and we know that she's the queen, and we know what queens do with shower caps, they don't wear them in public, so so this cannot possibly be a shower cap. Why does the system think this is a shower cap? Well, probably in the training set, there were not very many, training examples of ladies wearing tiaras, wearing crowns. And maybe there were examples of of people wearing shower caps. So this is a typical case of an out of sample very poor out of sample generalizability. That's the performance cliff of connection systems. So if we then see that these systems have complementary strengths and weaknesses, can we somehow get them to collaborate? And and just to emphasize, you know, this is this performance cliff is is not an an an artificial example. Here are all reasonable stop signs as you would find them in practice. Some funny guy has put a sticker on it or there is funny shape shadows on it, and these are all recognized as a maximum speed limit sign. So this is clearly a realistic problem. So can we solve such realistic problems with the machine learning systems by getting them to collaborate with the symbolic systems? Right? That's what what the rest of the talk is going to be about. Can we get the symbolic systems, the systems that reason, and the subsymbolic systems, the systems that learn, can we collaborate, can we get them to collaborate in a system that both learn and reasons? And I will give you a number of examples of systems that do such a collaboration. So maybe to this community, the best example, the easiest example is using a learning system to enrich a knowledge graph. This is often called knowledge graph completion. So imagine that we have a knowledge graph about music, and we have many links between artists and songs. We know that, Michael Jackson published Beat It, and we know that the Beatles published Yesterday, and we know etcetera. So we have many links between artists and, and their songs, and we that that has a particular label, namely that that artist has published that song. Now if we take this knowledge graph and we, somehow transform this knowledge graph into a high dimensional vector space where, similarity in the knowledge graph corresponds to similarity in the vector space, then Michael Jackson would become a vector. The song Beat It would become a vector. And the relationship between Michael Jackson and beat it, namely the relationship publish, is also a vector, from the between the two endpoints. And now we might recognize that if we have a vector for the Rolling Stones and a vector for Angie, that the distance between the Rolling Stones and Angie is very similar to the distance between Michael Jackson and Beatit. So this may cause us to learn that, the relationship between Rolling Stones and Angie is the same as between Michael Jackson and Beat It, namely, it's the relation published this all. Right? So in this way, we can learn new facts to add to another graph by using machine learning methods in this high dimensional vector space. So, essentially, now what we are doing here, we are are taking a graph representation. We are turning it into a real vector representation, a high dimensional, real real number vector representation. We do machine learning in the vector space, which leads to new predictions of links, and we translate these new links back to the, to the symbolic space. Right? So sometimes we call this this this pattern, we call it from symbols to data and back again. So we started with symbols, the the knowledge graph. We produced some data space. We did machine learning on the data space, and we translated it back to the symbolic space. So this is one combination where we use learning systems to improve the symbolic systems. So there, we were using, learning systems to rule only facts. Right? So we could learn new facts, but there are also systems where we learn new rules. And these, learning systems are called inductive logic programming systems or rule mining systems. So, here's an example from inductive logic programming. Suppose that we have a knowledge base, a knowledge base in symbolic form, this could be a knowledge graph about parental relationships. And we know that, and we can learn, from this parent and this parent, father, and mother relationships. From this, we can learn the rule that when that somebody is a parent, if somebody is either the father or the mother of that person. Right? So parent x y if either mother x y or father x y. So we can learn these new rules. Now we can confront the system with a new fact, namely father, Carrie, Andy. And together with, this learned knowledge, we can now derive that not only is Carrie the father of Andy, but, Carrie must also be the parent of Andy. And this can also be done on on knowledge graphs. AnyBurl, from, Manheim, is a system that, learns, rules over knowledge graphs rather than just simple facts. And this doesn't only work on, toy examples like the one that I'm showing you here, but this has also been shown to work on realistic examples where we take a, knowledge graph about chemical facts, biochemical facts, and the scientists this is an example from University of Leuven, where the scientists, the biochemists wanted to know which parts of a chemical structure makes a, a molecule biologically active or not. And so some molecules are biologically active, like, and some are not. So, for example, Petrol is biologically active, but plastic is biologically very inactive. But they are both made of very similar, molecules. And by feeding the system examples of molecules and labeling them as biologically active as as in and inactive, according to what chemists know, the system could learn the rule that this substructure that you see here on the right, this substructure is responsible for a a a molecule being biologically active or not. So in all of the examples we've seen so far are examples of where a learning system improves a symbolic system. But we can also turn this the other way around. So we can also use a knowledge system to improve a learning system. Here's an example. Let let's take the previous example. Now where now why is this a crown? Right? Why is this a and and not a a shower cap? K? Now if we have a neural network, that, neural network will generate a whole set of hypotheses, ranked with confidence. And because of bad, out of training, set, generalizability, the shower cap ended up with the highest probability. But if we now feed all these, hypotheses, these possible predictions into a knowledge graph, and the knowledge graph can tell us that there's a very close relationship between queen and crown with a very long distance relationship between queen and shower cap, then the knowledge graph will help us to select the right hypothesis and tell us that, well, if we have to choose between, a crown and a shower cap, then because this is the queen, it's more likely that this is a crown and not a shower cap. So here, we are using a knowledge based system to improve the performance of a machine learning system. And we can use almost the same, setup, not only to improve the output, but also to improve the explanations. So suppose that by, you know, whatever improvement we have managed to get the right output, why is this a crown? Hard for an image labeling neural network to explain why this is a crown. Right? This is just a a large box, a black box of neurons, adding up to the conclusion that this is a crown. So maybe we cannot give a real explanation about the content of the neural network, why this is a crown, but we can come up with a reasonable justification by using an honest graph. Right. Well, because this is a queen, what she's wearing on her head is most likely a crown. Right. So we can use a knowledge graph to come up with a justification for why, machine learning neural network, produced a certain element. Let me give you another example. Well, look at this picture. What do you see here? Now if you are an image labeling neural network, how would you label this image? Well, you would say, this is a flower. Maybe you would say it's a rose. K? Well, actually, you would have been wrong, because if you would know the context of this picture, you would see that it's not a flower or a rose. It's actually a cushion. Right? It's a cushion on a chair. So it's the context that matters here. Right? So what we can do is we can take this picture. Now let's assume that we have already classified the object in the blue bounding box, namely that it's a chair. Then we can inject a little bit of ontological knowledge. The ontological knowledge will tell us, well, a chair is made up out of cushions and an armrest. Right? So, this is a simple bit of of ontological knowledge. If x is a chair and y is part of x, then y is either a cushion or an armrest. Right? Now given this bit of ontological knowledge plus the fact that the blue bounding box contains a chair, that suddenly increases the the prior probability of the cushion as a as a an output, and reduces the prior probability of flower as an output. Output. So the the prior probability of cushion given a chair is much higher than the prior probability of a flower given a chair. I mean, a flower is not impossible. It could still be a flower lying on the chair, but it's much less likely. So here, we are using a bit of ontological knowledge to give symbolic priors to a subsymbolic, object recognition, system. And and, actually, this is not a a a a rare example. There is a a very good survey paper, by, colleagues in two thousand nineteen who, surveyed more than a hundred, different machine learning systems that use symbolic priors to improve, the behavior of a machine learning system. Now this is also known as a symbolic loss function. So so what is a loss function? A loss function is the function that you try to minimize during machine learning training. Right? It's the difference between your output and what your neural network tells you. And that difference between the the between the desired output and what your neural system tells you, you're trying to minimize the difference between the desired output and the actual output. Now it's part of the black art of machine learning to formulate a correct loss function. And there are many of these systems in this survey paper that, use semantic loss functions. So you write down what you know to be true about the world, in this case, or the parts of a chair, and you are training your system so that it minimizes the violation of this background knowledge. It minimizes the conflicts between what your system says as output and what you know is likely in the world. So these semantic loss functions are a great help to machine learning systems. Here's another nice example of, of of the combination of symbolic and, and machine learning systems. So this is an an exercise we give to our first year machine learning students. Right? Train a neural network so that it can read digits and then produce the sum of the two handwritten digits. And you can do this end to end training. But somehow this is not how we end up, how we as humans add up in, digits. We don't add up the visual images. We abstract the visual images into a symbolic representation, namely a number, and then we add the numbers. And, again, an example from our colleagues in Leuven, in Leuven, this is what they do. Instead of adding up the two, handwritten digits, they first abstract the handwritten digits into a symbolic representation, namely, in this case, the three and the five, and then they add the symbols, rather than training the system to add up the how to add up to handwritten digits. So, yeah, this is really cool, and it colleagues in London from, UCL and, and Google DeepMind to train a reinforcement learning agent in a very, artificial world. So in the the the agent was moving around in a world of objects, but rather than making it move in a world of pixels that are perceived by a neural network, these pixels are abstracted into objects and then the agent moves around in this world of objects, rather than in this world of pixels. And what happened in their experiments? Well, this was a very simple world where, you know, the the the big symbol in the middle there, the plus, is the agent, and it had to move around in this rectangle, this grid like world. It had to collect the crosses and avoid the zeros. Right? And, a reinforcement learning agent, the blue line, could could perfectly learn this task in this rectangular world. And they also trained an an agent that moved around in a world of not in pixels, but abstracting these crosses and nodes into objects and then move around in the world of objects. And that agent didn't quite do so well. But then they did something interesting. Then they they changed the world. They said, well, now these objects no longer live on the grid world, but these objects can live anywhere in the world. And then the performance of the agent that was trained to move around in a world of pixels completely degraded. It it fell from a hundred percent to fifty percent or less. Right? But the agent that was moving around in a world of objects, it was stable in its performance. Now it could, adjust to the fact that these objects no longer lived in a grid world, but that these objects could be anywhere in the space, and it was much more robust to learning, to adapting to this new world. Right? So, again, a combination of machine learning and a symbolic representation about the world. Just as we don't move around in a world of pixels, we we move around in a world of abstract objects that we construct out of the pixels. The final example is about, rule learning, where in the traditional world, we would, this is in a medical example where we would feed patient features about age and and, and body mass index and blood pressure and so on, and then try to predict, diabetes. You could do this with, with a a learning system, and then the system would come up with rules that are are very good, almost hundred percent correct, but that will be totally non understandable by medical experts. And as a result, even be though these rules were correct, the medical experts don't really like these systems because they can't understand what they do. So instead, what we did in an experiment jointly with, University of Munich, we injected, not only data driven rules, but we injected rules that we obtained from experts and from textbooks. And these rules weren't very difficult to, obtain. We talked to experts. We read some textbooks. We read some medical guidelines. And the corresponding rule set that you get out of combining the human expert knowledge with the data driven rules is almost as good, interestingly enough, not quite as good, but almost as good as the purely data driven rules, but the rules had became much more human understandable. So instead of a perfect system that nobody uses, you now have a slightly less perfect system, but that experts are willing to use. Let me for the sake of time, I will skip the the mathematics of this. Right. So in the last minute, my, concluding remarks. So what I have tried to convince you of is that machine learning systems can really benefit from injecting symbolic knowledge in in in many cases, symbolic knowledge in the form of knowledge graphs. Right? We can produce better explanations. We can better rank hypotheses. We can do transfer learning. We can get a better sample efficiency, so learn from fewer data. And we can also do this the other way around. So we can have our knowledge based systems benefit from machine learning systems. And the required symbolic knowledge is available at very large scale. Right? It is no longer true that symbolic knowledge is expensive and we cannot obtain it. All the very large linked data knowledge graphs are a witness to the fact that this symbolic knowledge is very well available. So it is no longer, necessary to learn what you already know. We can inject the stuff that we already know into our machine learning systems, and by combining these two types of systems, produce much more robust, much more efficient, and much more explainable systems. Thank you very much.",
    "transcript_length": 27015,
    "speaker": "Frank van Harmelen",
    "tags": [
      "AlphaGo",
      "AnyBurl",
      "DeepMind",
      "Nvidia",
      "inductive logic programming",
      "knowledge bases",
      "knowledge graphs",
      "machine learning",
      "neural networks",
      "ontological knowledge"
    ]
  },
  {
    "title": "Taxonomies: Connecting Data with Duct Tape",
    "description": "Taxonomies are the duct tape of connected data. They seem simple, flexible, and familiar. They are widely used. And they seem to work across many use cases and many domains.\n\nBut when looked at in more detail, taxonomies turn out to be crude tools for knowledge organization that are very difficult to create, to scale, to adapt, to align, and to build on.\n\nThey don't work well for larger or more complex domains and use cases. Experienced talent and flexible tools for creating them are hard to find and to develop. Often taxonomies are built then abandoned for other, more robust approaches to knowledge organization.\n\nIt is essential to re-evaluate your connected data strategies in the context of alternative approaches to knowledge organization.",
    "category": "Semantic Technology",
    "transcript": "Hi. I'm Mike, and I wanna talk about how a lot of us are using taxonomies to connect data. A very special kind of data, knowledge, usually based on language, not on numbers. We still have a lot left to do if we're going to build the foundations for a new generation of semantic technologies and artificial intelligence. I haven't presented to a connected data world audience before, so let me introduce myself. I have an unusual background, including professional tours of duty as a scholar, laboratory scientist, management consultant, and technologist, which seems to baffle many of my colleagues. At this point, I've been working in AI for more than twenty years. And now I've been tech lead for knowledge graphs at LinkedIn for several years. Tech lead simply means that when no one else knows what to do some how to do something, you have to come up with a solution as soon as possible. I wouldn't have any other role. And there's a theme to my varied career. It's all about connecting language and knowledge in wonderful and scalable ways. So it's fitting that I work in cognitive computing at the intersection of human expertise and machine intelligence. Drawing on this very background, I have extensive experience looking at how both humans and machines perform the same language and knowledge related tasks. The title of this talk alludes to Perl, the programming language that was often referred to as the duct tape of the Internet. Duct tape is good because it's flexible and easy to use, but it tends to hide problems rather than fix them, and it's difficult to troubleshoot and extend. I'm seeing lots of teams trying to connect data using taxonomies as duct tape with predictably unreliable results. And this is particularly important because I see a dramatic increase in work on developing taxonomies at leading companies. If, as the saying goes, experience is a fancy name for all the errors that you've committed, then I have a lot of experience to talk about. I'd like to tell you about some of my misadventures in knowledge organization. We're all familiar with simple taxonomies. I see more and more investment in developing them for industrial purposes with each passing year. I see more publications on taxonomies and more job postings for taxonomists. I hear of more new teams and field more and more questions about how to start building a taxonomy team. But are taxonomies the best use of our time? Most of us have heard about taxonomies since biology class in sixth grade. We assume that they're important and useful even when we don't have a very clear idea of what the details are. So what I frequently see is that teams who face challenges of knowledge organization turn to taxonomies as their first choice. Unfortunately, taxonomies are rather murky territory. There's no explicit definition of them that's easily findable. There are no best practices. There are few experienced people who know how to build them. There are no reliable evaluation methods and essentially no notion of done. And since there's almost no reliable reliable documentation to fall back on, we essentially have to rely on what we remember from biology class and hope for the best. But that's not a lot to work with. So this is a classic duct tape situation. It's almost guaranteed to produce flawed results at a high cost. But we forge ahead. We build a tech a taxonomy. And it seems like every time we build a taxonomy, the same thing happens. We put a lot of effort into making it correct and coherent, then get sign off from key stakeholders. Things look like they're going well. It's almost time to move on to the next project, and there are so many of them as more people start to understand the importance of our work. But when we try to get buy in from product partners, dozens of questions come up crop up from the CEO sometimes all the way on down the line. There are a wide range of questions, and all of them seem to be theme and variations on this doesn't meet our requirements. What did we do wrong? Oh, and if this hasn't happened to you yet, it will. Believe me. And when the CEO and product leaders are breathing down your neck, it gets uncomfortable. So this situation really wreaks havoc with our schedule and with our morale too because we have so much explaining and rework to do. So part of my job is to understand what's going on and to fix it somehow. This is the classic tech lead question, why did this happen? And I find it very valuable to listen first and very carefully, to spend time understanding how stakeholders think about our very nerdy, very technical issues. The stakeholders make it sound like a taxonomy really does not meet our needs and that we screwed up. So let's look at the kinds of issues that stakeholders raise when faced with a taxonomy. I don't have to tell you that product managers ask lots of questions. They're usually worried that rolling out a new taxonomy will wreck their products, even if right now they're using very messy data. Their questions often focus on other related entities, like dog houses and dog shampoo and veterinarians and pedigrees. They sense pretty quickly that taxonomies create issues of coverage of the domain. You probably don't want to answer that this kind of information doesn't fit into taxonomy, even though that's true. You might mention, though, that there were parts of the product requirements that weren't so clear. The taxonomists slaving away in the trenches also have lots of questions. Their questions often focus on the many, many exceptions and edge cases of what's correct. They ask, where should I put mixed breed dogs or service dogs of unknown breeds or hunting dogs that are also pets. They sense pretty quickly that taxonomies create issues or accuracy of the domain. Of course, managers of all stripes have lots and lots of questions. And their questions often focus on more, better, faster, and cheaper. They'll ask, how will you make the process faster? How you expand to new domains faster? How can we evaluate the workers better? They sense pretty quickly that taxonomies create issues of scalability. And the users, of course, also have lots of questions. Their questions office focus on, why don't you say it like I do? They ask things like, why do you call them service dogs? I call them support dogs. And hunting dog, don't you actually mean a sent out? And we can see pretty quickly that taxonomies create issues of communication of the domain as well. Taxonomies rely crucially on the category labels to communicate different concepts, but different people use different terms. Given these many limitations, at best, a taxonomy might be a patch or a short term workaround. So far, we've learned a few things. Taxonomies create a range of big problems: Problems of coverage, problems of accuracy, problems of scalability, and problems of communication. But why? Why aren't taxonomies working for us? If we don't understand why, we'll just keep repeating our mistakes. But do we need to fix our communication, or do we need to fix taxonomies themselves? Stakeholders look at taxonomies as a black box. We need to open up this black box. Let's go back to the beginning for a second. Is this unexamined assumption a good one? It's starting to look like it's not, and I can see two possible reasons. Either we're not building them correctly, or we're building the wrong things. So we need to understand taxonomies better to see how well they will work for us. Taxonomies are tools for knowledge organization. Okay. My key question today though is how good are these tools for connecting data? It's surprisingly difficult to find definitions of taxonomy with clear explicit criteria. I really had to dig to go beyond a system of classification used in biology, blah blah blah. It turns out that taxonomies are built with a very specific formal framework that is based on set theory and formal logic. There's a system of interrelated conventions like those listed here that essentially define what a taxonomy is. There are many issues with these assumptions, but for the moment, let's focus on some of the practical consequences of assuming that they are useful. Taxonomies assume that there's only one kind of item in each taxonomy, for example, a taxonomy of dogs. This is just to say that taxonomies are focused. We don't find a single taxonomy that includes dogs, houses, and shampoos. And this makes sense. Taxonomies represent degrees of similarity, and dogs, houses, and shampoo are not similar. But taxonomies make it hard for us to describe other related entities, like dog houses, dog shows, and veterinarians. And taxonomies usually don't go beyond entities to describe events, processes, states, contingencies, or other things. I keep hearing that we can simply use related terms to include other entities. But related terms, as you see here, are subjective and ill defined. The unclear semantics makes it very hard to guide and validate consistent choices of related terms. Taxonomies also assume that categories are essentially labeled sets of items. For example, these are the items in category X. And labels are very often taken to be enough to describe the semantics of a category. But labeled sets are the bane of our existence. When categories have labels and no explicit semantics, it's nearly impossible to annotate and validate their members consistently. This is very, very common in my experience. Annotators most often work from labels without explicit definitions or well documented guidelines. And that means that we can't describe exactly why an item is in a category, or even what it means to be in a category, or how to validate the placement of an item in a category. Taxonomies also assume that items are in one and only one category. Like, final is an example of a setter and is not in any other category. This is the most frequent assumption for standard taxonomies. Of course, permitting multiple categories or parents is an option for extending our taxonomies. But what do we do then with double counting? If each item can appear in multiple categories, then their occurrences will sum to more than one hundred percent. And one key use case for taxonomies is aggregating data for insights. Besides, multiple parents won't help with cases like we don't know, where there's not enough information, or there's a data error. If item x is in category y has to be entirely true or entirely false, then a statement like nursery manager is in category healthcare jobs will force us to incorrectly classify professionals who cultivate trees and shrubs as health care workers. In part because of this assumption, we still don't have good methods for evaluating items that are partially correct or totally irrelevant. Taxonomies also assume that only one kind of relation exists between items and categories. In parallel to set theory, an item is a member of a category. Like Fido is an example of setter. But that means that we can't describe things like Fido is bigger than poodles or similar to terriers. The usual conventions of taxonomies make it very difficult for us to describe other kinds of relations between items and categories. The same thing holds for the relation between one category and another. In parallel to set theory, a category can be a subset of another category, like setters or a kind of hunting dogs. But cats aren't a subset of dogs, so we can't describe any relations between them. The usual conventions of taxonomies make it very difficult for us to describe other kinds of relations between categories like the ones listed here. We've learned a few more things. So now we can see why taxonomies create a range of big problems. Assumptions about items and relations lead to problems of coverage. Assumptions about categories lead to problems of communication and of accuracy. And assumptions about the nature of true and false lead to problems of accuracy and scalability. Taxonomies create problems because they're simply not the right tool for many of our use cases. And we've seen why the field is moving away from taxonomies to ontologies and knowledge graphs, because ontologies and knowledge graphs impose fewer restrictions on the kinds of knowledge we can represent. So my first lesson from all of this is that taxonomies are in fact a kludge, not a fix. We're barking up the wrong tree. And my second lesson is that build a taxonomy doesn't always mean that we should build an actual taxonomy because stakeholders often aren't clear on what a taxonomy is or on the other options that we have. So we need to understand taxonomies better to guide other stakeholders better. The good news is that we can do much better than taxonomies. In fact, we have to do much better. We're building the foundations for a new generation of semantic technologies and artificial intelligence. We have to get it right. I'd like to backtrack for a minute and ask, why did we care about taxonomies in the first place? What are we trying to get done with them? For a long time, we've thought in stark black and white terms: machine only versus human only intelligence. We assume that there are only two options available. Indeed, many AI practitioners assume that autonomous machine only AI is the best goal for the field. And human only intelligence does not scale in the ways that we need it to. In addition, today's standard practice is to keep human expertise separate and leverage it mostly for better AI, and only sometimes to leverage AI for better human expertise. But what about the Fifty Shades of Grey in the middle? Saying that machine intelligence and human expertise are the only two options is fake news. It's a false dichotomy because there are many unexplored shades of mixed intelligence in the middle. We can arrange the options on a scale from least machine participation to most. And from the beginning of the field, there have been heated discussions about whether the overarching goal should be autonomous machine intelligence or augmentation of human intelligence And now the discussion has progressed to talk about hybrid intelligence, where humans and machines work as one integrated system. So be sure to watch Frank von Harmelin's keynote about hybrid intelligence, and let me wet your appetite with a taste of what he'll be talking about. Hybrid intelligence refers to systems where humans contribute insight, creativity, strategy, and qualitative evaluation that machines lack. And machines contribute speed, thoroughness, strength, comprehensiveness, coordination, and reliability that humans lack. That is, we're looking for systems where we get the best of both human expertise and machine intelligence. And what is hybrid intelligence like? It's like the situations where humans and machines do the same task at the same time. Humans and machines interact in real time. This is like collaborative AI. And for that, we need explainable AI. Machines learn from humans and humans from machines, which I call reciprocal learning. And this isn't science fiction. There are cutting edge products with hybrid intelligence already available on the market, like adaptive machine translation, where the machine adapts translations and style in real time according to human input, and humans can learn from the machine's consensual translations as well. We strategically choose the right intelligence for the right parts of each problem to get the best of both. We trust the machine based on understanding and monitoring its reasoning. Of course, we need lots more research to understand better what humans do best and what machines do best in these kinds of hybrid intelligence settings, and this will allow us to partition and assign tasks to achieve the best of both. But it's already clear that for hybrid intelligence, we'll need knowledge in different forms that both humans and machines can use. So be sure to watch Gadi Singer's keynote here about levels of knowledge. And again, let me whet your appetite with a taste of what he'll be talking about. Goddy makes a very significant point. We don't need to choose between deep learning models and symbolic knowledge. We'll need both and more! Neurosymbolic systems are starting to explore this space. Gotti talks about three levels of knowledge, instantaneous knowledge that's directly available in an AI system in the form of embeddings or models, standby knowledge which requires little extra processing and is slower than instantaneous knowledge. But it's applicable to more situations, and it's probably formulated into some standardized representation. Unless he talks about retrieved external knowledge, which is external information that is probably temporarily relevant and often shows up in the form of free text. So let's leverage the concepts of hybrid intelligence and levels of knowledge to articulate more precise goals for knowledge organization and connected data. This way, we'll have a clearer direction to lead us beyond building simplistic taxonomies. Again, we've learned a few more things. Now we can see why we need to move beyond taxonomies and duct tape. We need different forms of knowledge, not just taxonomies or just models. We need to work toward hybrid intelligence, not just machine intelligence. And we need to enable human machine communication with clear semantics. For hybrid intelligence, we'll need Godly's standby knowledge in place of simplistic taxonomies. And how can we get there? Let's make a to do list. First, we need to understand what are more reliable building blocks than taxonomies. And the first of those is what I call concept catalogs, List of distinct important concepts in different domains, along with different ways of expressing them, as you can see in the table below. They have many advantages over taxonomies, and possibly the biggest advantage is that they reduce subsequent processing and human review, often by a factor of fifty or more, because there are often fifty or more different textual forms for the same concept. So with that, they dramatically increase recall for search, instances for machine learning, and reliability of insights and analytics. Concept catalogs are useful whether you build taxonomies or not, and I suspect that they account for most of the ROI that we get from building taxonomies. We're all newbies, but biologists have been working with taxonomies for more than three hundred years. They've learned a few useful things from all of that experience. One is building what they call identification keys, which are essentially feature arrays that constitute definitions of species. They are a list of distinct important features and values for each species. We need something very similar for concepts generally, not just for species. This is very similar to what AI engineers call a feature space. There are lots of advantages of this over taxonomies. Categories are output, not input. We can combine the features in different ways to make different kinds of categories, it's not restricted to one entity type, and the features are explicit and accessible to algorithms rather than hidden inside human definitions. Another building block that we have to talk about is another kind of feature space, one that I haven't seen deployed in recent work. But feature spaces I just mentioned for concepts usually focus on things or entities. This one focuses on defining relations in a similar way. And of course, this has many advantages over taxonomies as well because taxonomies have very few relation concepts, kind of, example of, things like that, and taxonomies focus only on categorizing entities. Other knowledge organization methods also describe events, actions, and contingencies with relations like agent, cause, instrument, manner, temporal order, etc. And we've seen that sets of well defined conceptual relations already exist. They were developed for artificial intelligence the first time around, last century. These are epistemological assumptions, assumptions about the form and nature of knowledge, in particular about the role of different notions of truth. One very key assumption is that assertions are either entirely true or entirely false. There are no other options in any circumstances. So if item x is in category y has to be entirely true or entirely false, then a statement like nursery manager is in category healthcare jobs will force us to incorrectly classify professionals who cultivate trees as healthcare workers. The most important change here, as advances in machine learning and deep learning have shown, is to move from right wrong present absent binary truth values to continuous confidence scores that represent likelihood instead of absolute truth. So if x if item x is in category y is no longer entirely true or entirely false, then we can talk about multiple parents and use case or context dependent confidence scores for category assignment. For example, in the context of babies in hospitals, nursery manager is likely to be a healthcare position. In the context of trees and shrubs, nursery manager is likely to be an agricultural position. We can make category assignment context dependent, which is very, very hard to do with taxonomies. All these building blocks will allow us to accumulate knowledge and connect data by using statements that have explicitly defined semantics, what I call next generation knowledge graphs. Next generation because today's knowledge graphs do not always use concepts with explicit human readable semantics. And, of course, there are many advantages over taxonomies for these. Some of them are things like describing knowledge of events and of relations between statements, not just entities. And knowledge graphs aren't restricted to one entity type, and they enable more kinds of reliable inference. Knowledge in this form will provide a more reliable foundation for the next generation of semantic technologies and AI. So once again, we've learned a few more things. Now we can see why we need these building blocks: concept catalogs, concept models, explicit relation concepts, more realistic epistemological assumptions, and knowledge graphs. But how can we get there? We need to shore up our foundations to reach these goals, and I see significant issues at the base of our efforts. We have to move beyond believing knowledge creation for AI in the hands of naive, novice, non practitioners. We need systematic training programs to create essential talent. We don't have them. I've built taxonomy teams and crowdsourcing programs. I can tell you how very, very hard it is. We have to start with fundamental questions like, do we need taxonomists? What do they look like? Where can we find them? We need to explain this to managers and executives to fund the work. We need to explain this to HR to help us find and cultivate talent. Once we have the teams, what's the best way for them to work? We simply don't know. But we do know that we have to move beyond simply improvising. We need industry best practices to follow. We need industry backed processes to ensure reliable results, and we don't have them. Once we have a clear idea of the best ways for our teams to work, how can we help them? We don't want them to work independently on one item at a time in random order, slowly and manually, like a sculptor crafting a tiny statue. We have to move beyond crafting to manufacturing knowledge. We need tools to design and validate knowledge. We have to have tools to ensure reliable results at scale. We don't have them, and spreadsheets just won't suffice. To make semantic technologies really thrive, we need all of these things: better building blocks and taxonomies, more better trained professionals, explicit best practices, and AI tools for design and validation of knowledge, not just for storage and display. Let's create together a better foundation for the next generation of Symantec technologies and connected data. Thanks for your attention.",
    "transcript_length": 23974,
    "speaker": "Mike Dillinger",
    "tags": [
      "adaptive machine translation",
      "artificial intelligence",
      "cognitive computing",
      "collaborative AI",
      "concept catalogs",
      "deep learning",
      "epistemological assumptions",
      "explainable AI",
      "feature spaces",
      "hybrid intelligence"
    ]
  },
  {
    "title": "The Business Case for Data Management",
    "description": "The data that flows around our organizations and across our industry is the result of complex business processes that can represent legal obligations, client commitments or critical parameters related to research. Content that is extracted from these original sources must be precise and understood in context \u2013 but have often been transformed, modified and renamed to drive the software that propels our applications.\n\nThe result is the use of common words that mean different things and the expression of common concepts using a variety of words. The problem is exacerbated when seeking to align systems and processes because the terms used in one application do not always capture critical nuances that are needed for onward processing.\n\nWe refer to this problem as \u201cdata incongruence\u201d and it is a serious liability. It was created by technology fragmentation and proprietary approaches to data integration. It adds cost, complexity and structural rigidity to operations. It diverts resources away from business goals and leads to frustration by stakeholders. It extends the time-to-value for new initiatives and often leads to more regulatory oversight challenges.\n\nThis is a problem that can be solved. The payoff is significant, and the value is sustainable. There is a real cost associated with the transformation, but the cost is relatively short-term (and the transition can be managed incrementally). And while it is not a \u2018rip and replace\u2019 technology cost \u2013 it does require a mindset shift and does include a series of data integration challenges.",
    "category": "Semantic Technology",
    "transcript": "Hi. Greetings, everybody. My name is Mike. I'm going to present, the business case for Semantic Data Management. This is a consolidation of what I've learned over the past almost four decades of being the therapist, scribe, and analyst for the data management industry. Alright. So first, let's start off by understanding that there is a prime goal, what I call a prime directive for data management, and that is to ensure that all the users have trust and confidence that the data that they're using for their applications, is true to origin. And they should do so, without the need for reconciliation and without the need for multiple data transformations. If you think about it, and we put this in context for you, you have a right to, want I'm gonna say a right to expect the data that you're using to be true to original intent. Right? Not transformed, not, mismatched or incongruent, because it represents real things. Right? So it should be defined at the most granular atomic level to actually be able to capture the reality of the things that it's represented. It should be available when you need it, when, when your, analysts want it. They should be able to get it, themselves as they think about what they need. So it's a key part of your asset inventory along with your processes and your technologies and your requirements. The format should support, scenario based analytics. Right? You you don't want it to be stuck in the rigid technology that, you have to then kind of unravel before you can use. Right? So it should be flexible. We need it to be traceable as it flows across our systems and through our calculations and into our aggregation processes. We'd like it to be testable for fit for purpose, to make sure it matches all the logic requirements for processing as well as the quality requirements, and that should be done automatically. We'd like it to be self describing so that the meaning of the data is embedded in the content itself and reusable. Right? There's building blocks for us to put the data to work. And I will tell you that this is all completely achievable. That's completely achievable right now. Without a huge investment in technology, but it does require organizational adjustment. And in order to do that, the, the stakeholders must understand it. They must have cognition about the problem and the, pathway to solutions. And there must be leadership because, this is an act of change, and we wanna make sure that we can deal with the organizational challenges that exist. And those are the primary obstacles for implementing any new paradigm change that this represents. So let's take a look at the challenges. Right? So first we should recognize that there are, an infrastructure based on relational technologies, that are well over fifty years old. It's a lifetime in technology. That technology sometimes leads to wrong outcomes that seem like they are appropriate, but actually wind up being false narratives in today's technology environment. And that's compounded by cultural obstacles, associated with organizational change. Alright. So let's take a look at these things at in in-depth. Right? And the technology, of course, is the biggest part of it, but we have fragmentation of our technology, and we all know that. Silos exist, functional silos, process silos, business silos, geographic silos. And those silos are managed independently. And sometimes we have technology that is different in each silo. That technology is driven by proprietary data models, proprietary quality engines, proprietary glossaries, and they don't match each other. So we've done things independently, not reusable. We're looking at an environment where we have a couple pair. Right? Location based processing, where you have a column and a row, you know, expressed as a table linked with joins and keys where relationships have been constructed explicitly, and we've done stuff to optimize because we have lots of data that we have to manage. We've been copying data and moving into the silos and torturing it to death and transforming it and renaming it to make sure that it fits the glossary that drives the software that propels the data model that drives the application. That has result in a brittle infrastructure that is inflexible, and sometimes we are afraid to change it, not well documented, and it drives critical applications. This is what we refer to as technical debt, plus we have hard time tracing it as it flows through our systems, both technical lineage and the business provenance for the calculations. You combine that with concepts that seem appropriate, like the single version of the truth, which was appropriate when you only had one repository of data. But now we the reality is there are many, and we're not gonna be able to get our arms around all of the repositories of all of our functional applications across the world. So we just have to kind of accept that standard standardization of meaning is, a better strategy than trying to force fit everything into a single model or into a centralized repository. And that model itself, right, we we we understand the value of a canonical model describes relationships. Right? That's important. But having one model that describes everything, is just not practical. It would be out of date before it was finished, and you would spend all your time trying to catch up with the pace of your activity, and that is not what we're trying to do. These problems, these technological problems and objective problems lead us to the two big challenges. Right? We have, multiple identifiers. We have, codes all over our systems. We've changed and modified those for various contacts. We're we're we're having a difficult time making sure that this is the same as that, which is compounded, of course, because there's no meaning that's been locked down. And and meaning is is different when you have, front office transactions process versus a back office legal and, a settlement process. So identity and meaning, have become, incongruent. You combine that with, you know, it seems logical. Right? We try to create a a, an application for an important customer, but we do that over and over and over again, making lots of micro decisions, becomes hardwired and yet another silo. Without business rules that have been documented, they're not machine executable, and we're operating off of, next quarter spreadsheet with the pace of business, trying to push things, so we wind up doing mostly tactical activities to get us out of the fire rather than looking at the problem strategically as one that you can fix. And we have to be careful about this quest for magic solutions. Right? Everybody that sells technology always says, you know, this is the one solution you need. It might even seem like I'm gonna sell magic, but this is really just conceptual, understanding of how things work. Right? So if you look at those challenges, and now let's take a look now at the implications of those challenges. And these are actual quotes that I've been collecting over the years from people who are going through the, kind of, the data journey. First one is, we can't find this stuff. Right? We don't have a connected inventory of our assets. We made lots of copies. We, we've changed those copies. We don't have a process to prevent duplication, so we don't know where to go to get the data. So it's a big problem of even what exists and where do I get it multiplied by, the fact that it doesn't match. Right? So we have a hard time integrating it, linking it, making sure that this is the same as that, and, we miss new we have this problem of same word, different meaning, different words, same meaning. A nuance is not passed across processes, and it comes from using nomenclature as the key for its activity. I love this quote at the end, right, where it says, my projects end up as expensive death marches of cleansing, reconciliation, manipulation just to make the software work. And that story gets repeated over and over again. Because we're doing it, in our silos, we, began a local IT group to build it specifically for me. So we're not coordinating across processes, and there's conflicts and priorities and winds up, being, a bunch of, independently defined designed activities that leads to, a lack of confidence in usually in technology for being able to implement what we want. And you pull it all together and what we have is the lack of trust in the data. It's not fit for purpose. We spend seventy percent of our time doing data janitorial work just to make sure that the data is right for an application. We do that over and over and over again. I call this the bad data tax. And I've done I've done the research. Actually, I've done the research twice, once with IBM and another time with PwC to, validate that this is at least thirty percent of your operating expense just wasted. Right? And we've allowed data to lose consistency and precision of meaning. We've got it stuck in these rigid, ancient technology, environments so that we can't do what we need to in terms of automation and straight through processing. We're not getting leverage off of our expensive technology investments. We have these complex analytical models that, we're just spending all this time trying to make sure the data works and the models don't match, and we're running around doing fire drills and putting out brush fires. And mostly we're missing opportunities for upselling and cross selling and innovation. We can't understand context for legal or for finance or for marketing. And, of course, I'm in the financial industry. The regulators have a hard time unraveling the aggregates because they're trying to feed different models looking at systemic risk types of, applications. The truth is not gonna fix this by continuing to do the same thing over and over and over again. You're not gonna fix this by adopting more proprietary schemas, managing that rigid pipeline, or trying to force fit data into legacy data models that were created for specific applications. We have to look at this thing differently, which is why I focus on, these two concepts. Right? I'm not gonna shy away from them. One is semantic. It just says meaning, and the other one is standards. So these are wonderful ideas. We should not worry about the the, the dreaded o word of ontology is a fantastic Let me see if I can demystify this for you. Starts with, four concepts, four standards concepts of which is a scaffolding for everything else. Right? And the first one, the most important one is the ability to reconcile identity against a single, permanent, unique, unchangeable identifier in the cloud. You know, it's a, you know, individualized resource identifier or a resource locator. Forget what it's called. It is a ability to have a single precise granular identifier for everything to which all of the other systems get linked to. Right? So you should think of this as the Rosetta Stone capability that allows you to not have to move the data. You just link it up to its identity in the cloud. You combine that with ontologies. Right? So there there are two sides of this coin. First is, can we model precision? And and the answer is yes. We can. Right? We know how to capture what subject matter experts tell us to in the way things work, boxes and line diagrams. But the magic here is that we can express it now in standards using the triples, the other word, the gift of the Department of Defense. Understanding triples is straightforward. Just recognize that couple pair, current relational technology is location based. Right? It's a column row with tables, joins, keys, and explicitly defined relationships. A triple is a sentence structure. Two nouns, they call them subject and object, linked by a verb called a predicate. All of those things are precisely defined at their most granular level. So they become building blocks that you can then reuse for applications because they capture concepts. It's not necessary to be an expert at how to write, SPARQL queries, but, you should understand the basic philosophy behind how it works. And then, of course, the final part is business rules that are also, machine executable in, a semantic language. So rules and identity and meaning come together. You should remember the semantic trinity. Right? Identify, describe, and express. That's the goal. With those standards, we get these fantastic capabilities. Right? So the first one is, quality by math. Right? It's they're all axiomatic statements, and and if it doesn't match the structure of the ontology, then it doesn't get imported into the system. So bad data cannot get into your system in the first place. You're not running around trying to find it and clean it. It's done automatically. These are concepts. They're like tinker toy building blocks that you just kind of create and put together, and you get to reuse them application by application. So you take identity and meaning, and you add to that time, and you can express almost every context with those three things. And source is important because you want to make sure the quality of the data is fit for its intended use. Right? So this is all mostly about describing the data, not about the values. Sometimes it's critical for you to multisource or triangulate to make sure that the value is accurate as well. With semantic standards, we can and actually because it's linked to the identifier, we can track it as it flows across all of our systems, goes through our calculation engines. We can, manage our, privacy and our security concerns. We can prove to our, auditors that we know, how it flows and we have a control environment. Governance, which is mostly about quality and definition and requirements capture, is all done automatically. So governance becomes simplified and valuable. Right? It's not onerous governance. It is what do you need and let's go focus on getting it and and integrating it into our semantic environment. It's all machine readable, so humans can understand it as well as machines, and it's done on a continuously tested basis. So there's no big bang releases that you have to worry about. You can add modular components, and they're always continually tested of whether it is logically valid or of the right structure. So with those capabilities, there are kinda two things that I look at. One is why we do it, and the other one is what we do. And these kinda cross over each other. Right? And we do this for straightforward reasons. Right? We wanna operate efficiently. We wanna automate our processes. We wanna make sure that we have control over the data for privacy and for, regulatory compliance. We wanna give our data scientists flexibility to follow their intuition. We wanna be able to link up products, transactions, and customers so we can upsell and cross sell our activities. We we do that by building connected inventories, by facilitating the integration of data across our organization, and by managing the access to the data at a data point level, not a system level, a data level, so that we are ensured of privacy, management constraints. Let me drill down to this just a little bit deeper. So I think this is really the essence of the business value side of the business. Right? And we'll do this in the standard three c's of cost, capability, and control. So as I said before, this is about thirty percent savings minimum, because we've standardized meaning and standard identity and we have the entitlement control, Integration becomes simplified. We can, get rid of redundant systems and consolidate activities, and we can achieve automation. So the cost savings alone are overwhelming. But, really, we're doing this to facilitate, innovation. Right? We wanna make sure that our analysts can follow the what if scenarios that are required so we can test out ideas. We wanna be able to look at things from different perspectives, and we don't wanna be, forced to have to restructure data just to look at it from a different view, like a risk risk view, not the same thing as a legal view, not the same thing as a financial view, not the same thing as a marketing view, not the same thing as a regulatory view. We should be able to look at all of that, by asking questions of the data rather than restructuring its, its format. And then the goal is to establish what the regulators in our industry called, a control environment, which is defined, attract, with systems of record so we can comply with our obligations, so we can aggregate things on a consistent basis across our organization. So when you look at that, it's kind of an overwhelming value side. But there's a cost associated. There are challenges, and and we have to be honest about the challenges as well. Right? So let's take a look at this idea of data centric, and and I think a lot about this. I'm, like, wondering, you know, what exactly does it mean? So I stole this quote. I think sums it up properly. Right? It's not about the technology. This technology works as advertised. This is about cognition and understanding and making sure that people get the joke and believe in the possibility. Because once you do that, it becomes obvious that this is the way we should be operating our organization. In order to do that, we have to manage two critical, inhibitors to adoption. And the first one is really to full. Right? It's what I'll call in organizational inertia. Because not only do you have to have this data centric orientation of, why we have incongruence and the implications, but you have to be able to, manage the organizational challenges of priorities and politics and processes and give appropriate air cover and and allow for the infrastructure to be created, meaning coordination across your processes and facilitating collaboration. It's a big ask. We know that. But this is really the big challenge is it's not really the technology. There is some transformation costs. Right? You do have to have, triple store triple stores and and the tool sets. There is a skill set requirements. You must build the ontologies correctly, manage pipeline from ingestion through, application. So this is the this is the other side of the coin, and it's a real side of the coin. Mostly compounded by what I call the four horsemen of the data management apocalypse. Ignorance, arrogance, obsolescence, and power. And we all know, they run rampant through lots of organizations, and we have to kind of, manage this organizational challenge because we're talking about thinking about data in a different way. So we paint you a composite picture, and it starts with let's call this the infrastructure, and the most important one is this information literacy concept of understanding the problem and what it means, and why data meaning is not data processing, and why this is not a technology problem, and the possibilities of fixing this, adopting standards, doing appropriate data hygiene. You know, regardless of how you do this, whether you do a conventional technology or semantic, you have to manage the hygiene of your data. If you're going to do all that work, do it strategically. And that makes sure that we have to deal with, organizational, dysfunction and and, you know, I've been tracking data manager for a while, and I call this little g governance in data management. We're doing that pretty well. We've learned a lot about that. But big g governance of getting the organization to adopt a way of thinking, is the challenge that senior execs are responsible for implementing. There are some requirements. You know, as I said, cognition and buy in is critical. Air cover is critical. Understanding why we have a problem and the implications, being able to to think of this strategically, not just tactically, you know, fix the problem once and get it off our plate. Please, adopt this idea of the semantic trinity of identify, describe, and express. Right? Identifiers, common meaning, standards expression with an accountability and policy framework around it that, helps us with our ecosystem management. And then there are three big areas. Right? First one is the architecture. We talked a little bit about this. Right? Identity and meaning resolution, expressed in standards with executable business rules, and managing the pipeline, which is itself a challenge. Right? You have to manage it differently in a semantic world, to be able to, you know, triple ify it, you know, turn it into triples and align it with just the ontologies. The business case must be made over and over again to all the appropriate stakeholders. We have to think about them from the building blocks. Right? You know, that is identity and meaning and time and source giving us context, mathematical quality value validation, reusability of our concepts and how what does it mean to manage in a conceptual data environment, and traceability from, through all the transformations. The audit is very happy about that. The business case of the big three c's, right, cost, capability, and control, plus the language of, business in terms of measurement criteria. Right? Efficiency of use and user experience and time to market, and we can prove all this. And we spend a lot of time worrying about the measurement criteria, which I'm happy to share against the cost of conversion. Right? The inertia of our organization. This is all real stuff. Right? The physical infrastructure and transformation we have to do to map from conventional to semantic. And then there is an operating model that, is required. This is the data hygiene operating model, if you will. But I've simplified this. Right? So the first part of this is to document what exists. Build your inventory, understand the data flow, manage your lineage, then you simplify all that, authorize domains, names, systems of records, provisioning points, classification of what's important, and data architecture, capturing business concepts and translating them into data concepts. Let's not underestimate the value of this, the importance of this. Governance is required. It's gonna be light touch, but you're gonna have roles and skills and and policies and, you know, appropriate funding and prioritization management and escalation processes and all that stuff. And then how we talk about it. We talk about it differently to business, you know, time to market, use cases versus technology, which is about integration and scalability and resiliency versus control functions about transparency and traceability and auditability. So this is, emphasizing the importance of, education and messaging because buy in is the fundamental task if you're going to go in this direction. So here's some concluding thoughts. First, implementing a semantic environment can be done incrementally, use case by use case. Right? Which is that and that that would be the way you would do it, and I would be happy to talk to you about what the appropriate priorities are and how you implement that. There is a foundational level level. If the engineer does correctly, these are concepts, and and we have to be able to capture things like, obligation and commitment and control and party and role. Then you and engineering that is critical because this is the scaffolding from which we build the rest. There is some investment. It's not huge. This is not a rip and replace, but there is some investment in technology. The good news is it plays nice with all of your existing environments. You do have to make a transition, and business value is not always immediately known from the first use case, but all the rest of the use cases become a lot easier. So a little bit of patience is required. That's why leadership is critical, like finding the right sponsor, providing the appropriate air cover, you know, organizational challenges are necessary, including the right team leader for your semantic journey. And then process. Right? This is a ability to engineer, test, add, test, have policies test. Those are all critical. I will make these slides available. I did a good big Ben Franklin, cost benefit. They're both legitimate size. Right? I think the drivers are overwhelming, and this is a strategic value that I would, encourage everybody to consider. But managing organizational cohesion is not, an insignificant problem. So this is me. By all means, reach out. I'm making this course available through my friends at the Knowledge Graph Conference. Joaquin Melara is the CLL. He's been terrific in helping me get this organized. So thank you for your attention, and, good luck to you.",
    "transcript_length": 24604,
    "speaker": "Mike Atkin",
    "tags": [
      "auditing",
      "automation",
      "business processes",
      "centralized repository",
      "cloud",
      "contextual data environments",
      "data centric",
      "data flow",
      "data governance",
      "data hygiene"
    ]
  },
  {
    "title": "The Enterprise Knowledge Graph",
    "description": "In this talk we\u2019ll canvas the Enterprise Knowledge Graph, and how you can apply it using its cornerstones of:\n\n- Foundational building blocks\n- Information model expressivity\n- Machine understandable representations\n- Transcending the relational model\n- How an EKG expands on a graph and a knowledge graph\n- Provides an infrastructure for Machine Learning\n- Contrasting an unlinked with linked data environment\n- Question and answering model emergence\n- Semantic similarity & embedding\n- Focused UI",
    "category": "Knowledge Graphs",
    "transcript": "Hi. I'm David Newman. I'm going to introduce you to enterprise knowledge graph capabilities. First of all, let me, describe this from a very high perspective. New frontiers and innovation require investments in new foundational building blocks. For example, the mapping of the human genome is a representation of a first of a kind model that's a prerequisite step in innovations. Secondly, novel technologies must be developed that know how to effectively exploit the model, such as DNA sequencing technologies that developed to exploit the mapping of the human genome, and their focus is on reintegrating fragmented parts of DNA so that novel innovations could emerge that reflect critical breakthroughs such as, the mRNA, vaccines for COVID, cancer genomics, personalized medicine. Similarly, in finance, we're doing the same basic process to innovate new capabilities. In this case, we're mapping the financial genome using knowledge graph models called ontologies, and new technologies have emerged to exploit these very powerful representational models. Knowledge graph technologies focus on reintegrating the fragmented parts of data. Examples of technologies, that we need to exploit for knowledge graph are graph and virtual graph databases, knowledge driven data catalogs, and data fabrics, and graph algorithms, and graph neural networks, just to name a few. And these will result in some new innovations, such as advanced ways of understanding, customer, customer three sixty, ways to better, solve financial crimes, make risk more visible to better manage it, improvements in regulatory compliance, and significant improvements in data management. Knowledge graph is really the way forward for data. So let's look at this chart. At the bottom are things that humans can understand, and at the top are things that machines can understand. In parallel is the expressivity of the information model. The more expressive the information model, the more powerful will its linkage and insights be. So let's look at this chronologically. So around nineteen seventy, the relational model, the prevailing model today emerged, which focuses on looking at, columns and tables, which describes, basically, information in terms of strings or numbers that are represented as attributes within entities, relational databases emerged in around nineteen eighty five. Fast forward to the twenty first century with the emergence of knowledge graph models and databases, we could look at this area as a subset of artificial intelligence known as knowledge representation and reasoning or ontologies, which looks at data as things and not as strings. So looking at this, graph, data is look is looked at as first class objects or nodes in the graph that are associated with other nodes in a graph that reflect relationships. That's what these lines or edges are, and more about that later. So around two thousand and twelve, knowledge graph models and databases were able to converge with machine learning where we can leverage new analytic capabilities in this area called knowledge graph completion, where the algorithms converge with the knowledge graph database can infer new relationships, new linkages, link prediction, classification, categorization in order to create new nodes and create linkages between these nodes to reflect new insights. And then we can further evolve forward around two thousand and sixteen with the emergence of graph neural networks, which represent the intersection of knowledge representation and reasoning with machine learning, deep learning, and natural language processing to generate what we would call knowledge graph embeddings or vectorized things, which are ways of representing, very concrete pieces of information alternatively in, numeric representation or vector representation so that it aligns much better with machine learning algorithms in order to apply, new techniques, new analytic techniques that can, understand and express data that would otherwise be represented in a more conventional database in order to gain new insights in very precise ways of clustering information. And this ultimately is, right now, a way for machines to understand information that humans could also understand. So knowledge graph really is a strategic capability that transcends relational data capabilities. So when we look at the prevailing relational data paradigm, one of the things that really stands out is what I would call it duality or dualistic, scenarios where the model is segregated from the physical model, which is segregated from the metadata, which is segregated from the data. These are all separate. These are all, nonintegratable. So this also results in, tremendous incongruency and fragmentation, which results in a lot of data management problems, a lot of, complexity, a lot of limitations in what the prevailing relational paradigm can, actually achieve in terms of what we really need to, to support the, you know, very complex data needs today. So the emergence of the knowledge graph paradigm positions us much better for the future because data, because the data the model around that data plus metadata plus knowledge are all linked and integrated together. So I call this nondualistic. They're not segregated. They're integrated in a way that allows us to connect data to knowledge about that data. It's connected through this new paradigm rather than segregated in a relational paradigm. So the ability to actually link knowledge to raw data, and I'll explain that a little further on, is a is a huge achievement. So linking the data to its model, to its metadata, and knowledge about that data is gonna stimulate some very new innovations and tremendous business value. Well, let's dive into first what a graph is so we could better understand what a knowledge graph is. So a graph is a unique form of data organization that describes networks of connected data. So we can look at this, example here of a network or a graph, which is reflected with nodes connected to other nodes by these lines, which we call edges. These are connectors. For example, we could say that person a is a node, person b, c, and d are examples of nodes. And we can label the edges, with the, predicate pays, and we could then get a visualization at a at a data structure that indicates that person a pays both person b and c, and b and c both pay person d. And this is a very powerful data structure that can be used to reflect, use cases such as, anti money laundering, payments networks, ways of better detecting fraud, technology asset management, where, again, the basic organization of the data for the use cases and network. So so the idea here is to use a data organization that better reflects the use case than alternatively slicing and dicing this into tabular views that don't reflect this holistic picture. So what is a knowledge graph? So a knowledge graph is a very highly expressive form of data organized in a graph, except that the knowledge graph allows us to describe concepts that describe the data itself. And this representation of concepts in a logical model is also known as an ontology. It's a way of representing knowledge and describing meaning. A very standardized form of representing ontologies in is a language called RDFOWL, and it's the global semantic modeling standard. It's the a modeling standard that's best used to represent knowledge graphs because it's of its expressivity, and it's linguistically aligned with how we think and speak. And, basically, what this will do is allow us to take unstructured content definitions and translate it into a structured form that not only humans can understand, but machines can understand. So as an example, we could describe a model where we have a customer, associated with a checking account and a credit card account, and we have this notion of this edge that we could label with the meaning has account so that this is very intuitively understandable to a human. We can describe concepts and other concepts and the and the linkage of these is what we would call a predicate. And these could be further defined in a very rich taxonomy where we could describe that checking and credit card accounts are kinds of accounts, and that checking and credit card accounts also have common behaviors or attributes. In this case, they both share common behavior that they will have transactions that associate to another first class object called the transaction, which will also have a fan out of relationships that describe the meaning of the transaction, that it's meaningful in that a transaction may reflect the purchase from a merchant, and it may reflect the purchase amount, which is some monetary value. And that this could be all again represented using a standardized common semantic language like RDFOWL that is also highly reusable, which we'll describe a little more, in a moment, and it's something that humans and machines can both understand. So a knowledge graph elevated to an enterprise level is an enterprise knowledge graph, and this is where the knowledge graph model, is standardized and is highly reusable across the enterprise, and it organically grows bottom up from use cases where new business elements and, business behaviors are represented in this model that has enterprise reuse reusability capability. And this also, positions us for, leveraging a common semantic language across the enterprise for better, data integration as well as for improved interoperability because this knowledge graph model leverages open web protocols. So essentially, we're superimposing the, power and behavior of the Internet across our data and the ability to link data and information in the Internet is very similar to how we link and integrate data across an enterprise knowledge graph. So let's take the model we described a little bit earlier, and we can now introduce data to that model where we can execute new transactions for in this case, a customer has a consumer credit card account where there are one to many transactions that all fan out to the same standardized description of data that reflects transactions, monetary values associated with merchants, and this fan out can further occur. Now with the semantic web, what we're doing is we're prelinking information together using the fan out of the, web of data that all aligns with a common conceptual model, and we can ask certain questions. So we may ask a, some information about a merchant, and we may ask for a given merchant, who are the customers across the enterprise that interface with this merchant even though some of the customers are domiciled in different lines of businesses. So for example, consumer credit card line of business may have different, databases than the consumer debit card line of business or the commercial card line of business or the business credit card line of business. But if these databases are represented semantically as graph databases, then we will be able to, engineer, federation and interoperability across these databases because we all aligned to the same common model and the same web enabled protocols where we could simply ask a question of a merchant, and that question will traverse these, web enabled interfaces and linkages over the web of data in order to basically, identify that the ultimate customers associated with this merchant can be identified even though they may be, stored in different databases across the enterprise. So we could better enable enterprise linkage using the semantic web, and the web protocol for advanced interoperability. So, one more, point I wanna make here is that knowledge graph can apply knowledge to data in order to infer new insights and relationships. For example, the concept of corporate control, Again, it's broken down into this notion of subject, predicate, object, which again is linguistically aligned with how we think and speak. So let's take this example. A corporation is an example of a subject, controls another corporation, predicate, and another object represents this concept of corporate control. And then we can weave more specificity into this concept by saying that controls majority voting shares is a kind of controls. We can add more specificity to subjects, predicates, and objects. And then we can introduce data and link it to the model where we could say that Global Bank, a data assertion, is a type of a corporation, and New York Bank is a type of corporation. Now once we've done this and this is actually a significant leap because within knowledge graph, data and the metadata around it and the conceptual model around that can all be interlinked within the web. So it's all operationalized. We can then say that Global Bank, a type of corporation, controls the majority voting shares of New York Bank. And then we can also introduce additional concepts that is majority controlled by, is the inverse of controls majority voting shares. They're the polar opposites of each other, and we could introduce this into our model. And now you can see we're introducing some knowledge into the model by virtue of how we introduce concepts. And then we may be able to say that New York Bank, because it, is, has its major is controlled by Global Bank. Global Bank controls the majority voting shares of New York Bank. Therefore, New York Bank is majority controlled by Global Bank. We could draw that inference. And then we could further draw the inference that once we model that a parent company, and a subsidiary company have a relationship such that the subsidiary company is majority controlled by the parent company given that New York Bank must play the role of the subsidiary company, and Global Bank must play the role of the parent company. And these are very powerful inferences that we could make by linking the data to the concepts, establishing and defining knowledge about those concepts. And the knowledge really emerges when we draw these inferences to basically infer that Global Bank must be a parent company and New York Bank must be subsidiary company of Global Bank. And having this data linked to concept and knowledge is really a very powerful enabler and bridge to more advanced machine learning and the introduction of more precise, insights. So with that, I'm gonna turn the presentation over to my colleague, Omar Khan, to continue with his, part of the presentation. Thank you very much. Hi. Thanks for joining David and me today to talk about the enterprise knowledge graph. I'd like to talk a little bit about the current unlinked data environment and how adding linked data approaches will help with better integration with an easier infrastructure consolidation. So when you have a linked data environment, what that allows you to do is have various views so that you can have the view of the data if it's materialized, or you can have it run ad hoc. For example, if you see the banking data here, it's named with various different names and different silos for for Acme Widgets. However, when we go to take a look at the data in a federated view, what we see is we can pull these names together and work with them with ease without having to go across different systems. This provides you with a common query facade that you could use with RESTful APIs, SQL, SPARQL, or GraphQL. This could be used to feed a dedicated EKG. Moving on to the next slide. If you have an, enterprise knowledge graph, you could do something even more powerful. You could tie in question answering mechanisms where it uses natural language where you built a model that humans can use to perform inquiries. One thing I believe will be emerging is where instead of creating specific restful interfaces or other sorts of APIs, you will in fact be using a bot using natural language that will talk to the knowledge graph and get information out of it based off of question and answering models. Of course, there'll be security issues in which such coarse grained facilities exist. The other major benefit of having an enterprise knowledge graph is you could find out what sort of concepts are similar. For example, we had Acme Widgets Corporation, Acme Widgets, Acme Widgets Incorporated, and Acme Widgets Incorporated. They're spelled different ways, but the reality is they're equivalent to the same thing. In the previous slide, you saw we we had linked data that gave you the ability to perform federation amongst the data that exist. However, it didn't provide a model that gave similarity. It's true you could get a lot of value by by mapping and conventions. That said, what we could do is apply vector based models on top of the data, which would give us enhanced similarity. A lot of the recipe is taking relationships and text, creating chains of concepts within that the graph, and then seeing what is similar, making the matrix values that could be run with mathematical algorithms. So using graph embeddings and neural network based models would give you additional lift from the transformation. You want to be sure, though, to train the model correctly. My shirt says a clockwork onion. It should say a clockwork orange, but it says a clockwork onion. So the the the computer split this out wrong with a bad similarity model, or the human input the data incorrectly? It doesn't matter. You need to look at the data in the knowledge graph and the models, and that's example possible results. You could also perform all sorts of visualizations with your your with your knowledge graph. However, you probably don't wanna overwhelm the person who's looking at the graph with too much information. Instead, really focus on providing a user interface as meaningful to them and can be exploratory. There's a lot of tooling that provides data exploration and the ability to navigate the graph. However, typical end users are too busy to go through this sort of paradigm. Make the user interface clean and simple, and provide a selected slice of the graph that'll be meaningful to them. Also in the user interface, keep references to the knowledge graph to provide additional value. Thanks for your time, and I hope you enjoyed our presentation. If you're interested in best practices, please check out the Enterprise Knowledge Graph Foundation at e k g f dot org.",
    "transcript_length": 18287,
    "speaker": "David Newman",
    "tags": [
      "GraphQL",
      "RDFOWL",
      "SPARQL",
      "data fabrics",
      "deep learning",
      "graph algorithms",
      "graph databases",
      "graph neural networks",
      "knowledge driven data catalogs",
      "knowledge graph"
    ]
  },
  {
    "title": "The Enterprise Knowledge Graph",
    "description": "In this talk we\u2019ll canvas the Enterprise Knowledge Graph, and how you can apply it using its cornerstones of:\n\n- Foundational building blocks\n- Information model expressivity\n- Machine understandable representations\n- Transcending the relational model\n- How an EKG expands on a graph and a knowledge graph\n- Provides an infrastructure for Machine Learning\n- Contrasting an unlinked with linked data environment\n- Question and answering model emergence\n- Semantic similarity & embedding\n- Focused UI",
    "category": "Knowledge Graphs",
    "transcript": "Hi. I'm David Newman. I'm going to introduce you to enterprise knowledge graph capabilities. First of all, let me, describe this from a very high perspective. New frontiers and innovation require investments in new foundational building blocks. For example, the mapping of the human genome is a representation of a first of a kind model that's a prerequisite step in innovations. Secondly, novel technologies must be developed that know how to effectively exploit the model, such as DNA sequencing technologies that developed to exploit the mapping of the human genome, and their focus is on reintegrating fragmented parts of DNA so that novel innovations could emerge that reflect critical breakthroughs such as, the mRNA, vaccines for COVID, cancer genomics, personalized medicine. Similarly, in finance, we're doing the same basic process to innovate new capabilities. In this case, we're mapping the financial genome using knowledge graph models called ontologies, and new technologies have emerged to exploit these very powerful representational models. Knowledge graph technologies focus on reintegrating the fragmented parts of data. Examples of technologies, that we need to exploit for knowledge graph are graph and virtual graph databases, knowledge driven data catalogs, and data fabrics, and graph algorithms, and graph neural networks, just to name a few. And these will result in some new innovations, such as advanced ways of understanding, customer, customer three sixty, ways to better, solve financial crimes, make risk more visible to better manage it, improvements in regulatory compliance, and significant improvements in data management. Knowledge graph is really the way forward for data. So let's look at this chart. At the bottom are things that humans can understand, and at the top are things that machines can understand. In parallel is the expressivity of the information model. The more expressive the information model, the more powerful will its linkage and insights be. So let's look at this chronologically. So around nineteen seventy, the relational model, the prevailing model today emerged, which focuses on looking at, columns and tables, which describes, basically, information in terms of strings or numbers that are represented as attributes within entities, relational databases emerged in around nineteen eighty five. Fast forward to the twenty first century with the emergence of knowledge graph models and databases, we could look at this area as a subset of artificial intelligence known as knowledge representation and reasoning or ontologies, which looks at data as things and not as strings. So looking at this, graph, data is look is looked at as first class objects or nodes in the graph that are associated with other nodes in a graph that reflect relationships. That's what these lines or edges are, and more about that later. So around two thousand and twelve, knowledge graph models and databases were able to converge with machine learning where we can leverage new analytic capabilities in this area called knowledge graph completion, where the algorithms converge with the knowledge graph database can infer new relationships, new linkages, link prediction, classification, categorization in order to create new nodes and create linkages between these nodes to reflect new insights. And then we can further evolve forward around two thousand and sixteen with the emergence of graph neural networks, which represent the intersection of knowledge representation and reasoning with machine learning, deep learning, and natural language processing to generate what we would call knowledge graph embeddings or vectorized things, which are ways of representing, very concrete pieces of information alternatively in, numeric representation or vector representation so that it aligns much better with machine learning algorithms in order to apply, new techniques, new analytic techniques that can, understand and express data that would otherwise be represented in a more conventional database in order to gain new insights in very precise ways of clustering information. And this ultimately is, right now, a way for machines to understand information that humans could also understand. So knowledge graph really is a strategic capability that transcends relational data capabilities. So when we look at the prevailing relational data paradigm, one of the things that really stands out is what I would call it duality or dualistic, scenarios where the model is segregated from the physical model, which is segregated from the metadata, which is segregated from the data. These are all separate. These are all, nonintegratable. So this also results in, tremendous incongruency and fragmentation, which results in a lot of data management problems, a lot of, complexity, a lot of limitations in what the prevailing relational paradigm can, actually achieve in terms of what we really need to, to support the, you know, very complex data needs today. So the emergence of the knowledge graph paradigm positions us much better for the future because data, because the data the model around that data plus metadata plus knowledge are all linked and integrated together. So I call this nondualistic. They're not segregated. They're integrated in a way that allows us to connect data to knowledge about that data. It's connected through this new paradigm rather than segregated in a relational paradigm. So the ability to actually link knowledge to raw data, and I'll explain that a little further on, is a is a huge achievement. So linking the data to its model, to its metadata, and knowledge about that data is gonna stimulate some very new innovations and tremendous business value. Well, let's dive into first what a graph is so we could better understand what a knowledge graph is. So a graph is a unique form of data organization that describes networks of connected data. So we can look at this, example here of a network or a graph, which is reflected with nodes connected to other nodes by these lines, which we call edges. These are connectors. For example, we could say that person a is a node, person b, c, and d are examples of nodes. And we can label the edges, with the, predicate pays, and we could then get a visualization at a at a data structure that indicates that person a pays both person b and c, and b and c both pay person d. And this is a very powerful data structure that can be used to reflect, use cases such as, anti money laundering, payments networks, ways of better detecting fraud, technology asset management, where, again, the basic organization of the data for the use cases and network. So so the idea here is to use a data organization that better reflects the use case than alternatively slicing and dicing this into tabular views that don't reflect this holistic picture. So what is a knowledge graph? So a knowledge graph is a very highly expressive form of data organized in a graph, except that the knowledge graph allows us to describe concepts that describe the data itself. And this representation of concepts in a logical model is also known as an ontology. It's a way of representing knowledge and describing meaning. A very standardized form of representing ontologies in is a language called RDFOWL, and it's the global semantic modeling standard. It's the a modeling standard that's best used to represent knowledge graphs because it's of its expressivity, and it's linguistically aligned with how we think and speak. And, basically, what this will do is allow us to take unstructured content definitions and translate it into a structured form that not only humans can understand, but machines can understand. So as an example, we could describe a model where we have a customer, associated with a checking account and a credit card account, and we have this notion of this edge that we could label with the meaning has account so that this is very intuitively understandable to a human. We can describe concepts and other concepts and the and the linkage of these is what we would call a predicate. And these could be further defined in a very rich taxonomy where we could describe that checking and credit card accounts are kinds of accounts, and that checking and credit card accounts also have common behaviors or attributes. In this case, they both share common behavior that they will have transactions that associate to another first class object called the transaction, which will also have a fan out of relationships that describe the meaning of the transaction, that it's meaningful in that a transaction may reflect the purchase from a merchant, and it may reflect the purchase amount, which is some monetary value. And that this could be all again represented using a standardized common semantic language like RDFOWL that is also highly reusable, which we'll describe a little more, in a moment, and it's something that humans and machines can both understand. So a knowledge graph elevated to an enterprise level is an enterprise knowledge graph, and this is where the knowledge graph model, is standardized and is highly reusable across the enterprise, and it organically grows bottom up from use cases where new business elements and, business behaviors are represented in this model that has enterprise reuse reusability capability. And this also, positions us for, leveraging a common semantic language across the enterprise for better, data integration as well as for improved interoperability because this knowledge graph model leverages open web protocols. So essentially, we're superimposing the, power and behavior of the Internet across our data and the ability to link data and information in the Internet is very similar to how we link and integrate data across an enterprise knowledge graph. So let's take the model we described a little bit earlier, and we can now introduce data to that model where we can execute new transactions for in this case, a customer has a consumer credit card account where there are one to many transactions that all fan out to the same standardized description of data that reflects transactions, monetary values associated with merchants, and this fan out can further occur. Now with the semantic web, what we're doing is we're prelinking information together using the fan out of the, web of data that all aligns with a common conceptual model, and we can ask certain questions. So we may ask a, some information about a merchant, and we may ask for a given merchant, who are the customers across the enterprise that interface with this merchant even though some of the customers are domiciled in different lines of businesses. So for example, consumer credit card line of business may have different, databases than the consumer debit card line of business or the commercial card line of business or the business credit card line of business. But if these databases are represented semantically as graph databases, then we will be able to, engineer, federation and interoperability across these databases because we all aligned to the same common model and the same web enabled protocols where we could simply ask a question of a merchant, and that question will traverse these, web enabled interfaces and linkages over the web of data in order to basically, identify that the ultimate customers associated with this merchant can be identified even though they may be, stored in different databases across the enterprise. So we could better enable enterprise linkage using the semantic web, and the web protocol for advanced interoperability. So, one more, point I wanna make here is that knowledge graph can apply knowledge to data in order to infer new insights and relationships. For example, the concept of corporate control, Again, it's broken down into this notion of subject, predicate, object, which again is linguistically aligned with how we think and speak. So let's take this example. A corporation is an example of a subject, controls another corporation, predicate, and another object represents this concept of corporate control. And then we can weave more specificity into this concept by saying that controls majority voting shares is a kind of controls. We can add more specificity to subjects, predicates, and objects. And then we can introduce data and link it to the model where we could say that Global Bank, a data assertion, is a type of a corporation, and New York Bank is a type of corporation. Now once we've done this and this is actually a significant leap because within knowledge graph, data and the metadata around it and the conceptual model around that can all be interlinked within the web. So it's all operationalized. We can then say that Global Bank, a type of corporation, controls the majority voting shares of New York Bank. And then we can also introduce additional concepts that is majority controlled by, is the inverse of controls majority voting shares. They're the polar opposites of each other, and we could introduce this into our model. And now you can see we're introducing some knowledge into the model by virtue of how we introduce concepts. And then we may be able to say that New York Bank, because it, is, has its major is controlled by Global Bank. Global Bank controls the majority voting shares of New York Bank. Therefore, New York Bank is majority controlled by Global Bank. We could draw that inference. And then we could further draw the inference that once we model that a parent company, and a subsidiary company have a relationship such that the subsidiary company is majority controlled by the parent company given that New York Bank must play the role of the subsidiary company, and Global Bank must play the role of the parent company. And these are very powerful inferences that we could make by linking the data to the concepts, establishing and defining knowledge about those concepts. And the knowledge really emerges when we draw these inferences to basically infer that Global Bank must be a parent company and New York Bank must be subsidiary company of Global Bank. And having this data linked to concept and knowledge is really a very powerful enabler and bridge to more advanced machine learning and the introduction of more precise, insights. So with that, I'm gonna turn the presentation over to my colleague, Omar Khan, to continue with his, part of the presentation. Thank you very much. Hi. Thanks for joining David and me today to talk about the enterprise knowledge graph. I'd like to talk a little bit about the current unlinked data environment and how adding linked data approaches will help with better integration with an easier infrastructure consolidation. So when you have a linked data environment, what that allows you to do is have various views so that you can have the view of the data if it's materialized, or you can have it run ad hoc. For example, if you see the banking data here, it's named with various different names and different silos for for Acme Widgets. However, when we go to take a look at the data in a federated view, what we see is we can pull these names together and work with them with ease without having to go across different systems. This provides you with a common query facade that you could use with RESTful APIs, SQL, SPARQL, or GraphQL. This could be used to feed a dedicated EKG. Moving on to the next slide. If you have an, enterprise knowledge graph, you could do something even more powerful. You could tie in question answering mechanisms where it uses natural language where you built a model that humans can use to perform inquiries. One thing I believe will be emerging is where instead of creating specific restful interfaces or other sorts of APIs, you will in fact be using a bot using natural language that will talk to the knowledge graph and get information out of it based off of question and answering models. Of course, there'll be security issues in which such coarse grained facilities exist. The other major benefit of having an enterprise knowledge graph is you could find out what sort of concepts are similar. For example, we had Acme Widgets Corporation, Acme Widgets, Acme Widgets Incorporated, and Acme Widgets Incorporated. They're spelled different ways, but the reality is they're equivalent to the same thing. In the previous slide, you saw we we had linked data that gave you the ability to perform federation amongst the data that exist. However, it didn't provide a model that gave similarity. It's true you could get a lot of value by by mapping and conventions. That said, what we could do is apply vector based models on top of the data, which would give us enhanced similarity. A lot of the recipe is taking relationships and text, creating chains of concepts within that the graph, and then seeing what is similar, making the matrix values that could be run with mathematical algorithms. So using graph embeddings and neural network based models would give you additional lift from the transformation. You want to be sure, though, to train the model correctly. My shirt says a clockwork onion. It should say a clockwork orange, but it says a clockwork onion. So the the the computer split this out wrong with a bad similarity model, or the human input the data incorrectly? It doesn't matter. You need to look at the data in the knowledge graph and the models, and that's example possible results. You could also perform all sorts of visualizations with your your with your knowledge graph. However, you probably don't wanna overwhelm the person who's looking at the graph with too much information. Instead, really focus on providing a user interface as meaningful to them and can be exploratory. There's a lot of tooling that provides data exploration and the ability to navigate the graph. However, typical end users are too busy to go through this sort of paradigm. Make the user interface clean and simple, and provide a selected slice of the graph that'll be meaningful to them. Also in the user interface, keep references to the knowledge graph to provide additional value. Thanks for your time, and I hope you enjoyed our presentation. If you're interested in best practices, please check out the Enterprise Knowledge Graph Foundation at e k g f dot org.",
    "transcript_length": 18287,
    "speaker": "Omar Khan",
    "tags": [
      "GraphQL",
      "RDFOWL",
      "SPARQL",
      "data fabrics",
      "deep learning",
      "graph algorithms",
      "graph databases",
      "graph neural networks",
      "knowledge driven data catalogs",
      "knowledge graph"
    ]
  },
  {
    "title": "The game plan for your Knowledge Graph-driven FAIR data platform",
    "description": "This talk will highlight our lessons learned and best practices for building a Knowledge Graph-driven FAIR data platform to drive digital transformation.\n\nThe session will focus on agile data management and governance with Knowledge Graphs, following the FAIR Data principles.\n\nIt will cover topics such as data catalogues, ontology modelling, taxonomy integration, user experience development, and how low-code platforms allow you to get from idea to production in less than 2 weeks.",
    "category": "Knowledge Graphs",
    "transcript": "This is, the final day for the Knowles Connections conference brought to you, by Connected Data London and the knowledge graph, conference. This is our second track, innovators, and, have the great pleasure of welcoming, here today, Sebastian Schmidt, who's the co CEO of MetaFacts. Co CEO is actually not very familiar, title, not one many people have. Sebastian does. And he's here today to talk to us about, how knowledge graphs, can power FAIR, platform. And by FAIR, well, it's an acronym you may be or not be familiar with. Sebastian will explain it. I'll give the floor to him. Thanks a lot, George. So also welcome from from me on the presentation on your game plan for building a knowledge graph to a fair data platform. So, a few words on myself. My name is Sebastian Schmidt. As as George mentioned, I'm co CEO at MatterFacts. So I'm, running the company together with Peter Hasso, who founded it in two thousand fourteen. I actually come from a cloud and and, infrastructure background, but I've always been a big fan of knowledge graphs. And, I'm happy for being about one and a half years part of a truly digital business and working with a lot of knowledge graph experts. Keep people behind, for example, the the owl draft or also pushing on on new topics like, RDF style and sparkle style. So, who is MetaFacts? As I said, we started out in two thousand fourteen. We are based out of Waldorf, Germany, and, we have an international team really across multiple locations from Australia, Germany, Russia, Ireland, all over the globe. We have one product which is Meta Factory, our knowledge graph platform And we use that product, to drive digital transformation with our customers. And that's specifically around, unlocking the value of the data assets using knowledge graphs as the underlying technology stack. And what I want to present today is an approach, we have implemented with a number of customers in in different verticals. So we are working with customers in in cultural heritage and digital humanities and engineering, manufacturing, finance, insurance, pharma, and and life science. And we have built knowledge graph based applications and and tools from, a bit of material management to a digital museum, solutions like for for drug discovery and and drug repurposing. So, really a lot of different domains where we bring in the the expertise of how do you build a knowledge graph and, how can you do that really fast. And our customers bring in their specific domain expertise. So, what is it about digital transformation? Where do we see this demand coming from for building such knowledge graphs? We see that a number of drivers are behind why companies wanna establish a digital transformation strategy from, really just enabling innovation, bringing new products to the market, driving additional business, to increasing engagement with, the employees, internal customers, partners, to improving operations. So, very often, it's also just an an internally focused effort to make sure that internal processes once mover access to relevant information is available at the time it's needed and, is complete and is reliable. When talking about how to get to digital transformation, this is something you might have seen before. So there's this approach of, first talk to people, talk to your customers, talk to your partners, talk to your employees, and understand the gaps and the needs. Then it is finding the right data, using the data in the right way to drive digital processes and with those, digital processes to support them with digital tools. The approach we are taking on that is is a truly agile approach, and I'm gonna, highlight that a few times throughout the presentation. So it's all about planning, building, testing those individual steps and, iterations on each of those steps as needed. So wherever we are in that process, the idea is you can at any time go back to the previous steps, fill gaps that you identified later on, retest new hypothesis, and and make sure that you are actually achieving the the goal that you have set out. And it might even be adjusting that goal, as you are going along. This is a lot of ground to cover. So today I'm gonna focus really, purely on the the data aspect of this. So how do we build FAIR data? FAIR data that stands for findable, accessible, interoperable, and reusable data. And I will highlight three of the key steps we see which is describing your data, modeling your data, and then using your data. As mentioned, we are using the semantic web knowledge graph standard for that to derive knowledge from this data and to also further drive prediction with artificial intelligence and machine learning. So when we talk about data and digital transformation, most of our projects actually start from the point of looking at what kind of data is available. And I think there's no enterprise that has a lack of available data but the challenge is usually about, what can I actually do with this data? Is this relevant data for what I wanna achieve here? Who created it? So who can I reach out to to get more detail about it? Which processing steps have been applied to this data? Is this, in some way modified? How do I interpret that correctly? Is that, reliable data? So do I have contradicting data available or other data that is, supporting the information I would derive from it? Where and when was it created? So is it still relevant? And what was the intended use? So with what, idea was it initially created and and that's that properly applied to, what I'm planning to do with it. And there are a number of approaches we see that our customers have taken to answer those questions and get a better understanding of their data. The most successful approach at describing your data is is really taking all of your data, all of those digital assets in the same way. Meaning, that could be any kind of private data or public data that is not yet cataloged, that is just existing in in some way in your enterprise, or existing catalog solutions you might have already established, which we very often see are building on proprietary standards, are building mostly, as a siloed solution. So, most customers I'm working with have actually two or three catalog solutions in place. None of them covering, let's say, a very significant part of the data. It's it's usually in the range of twenty, maybe thirty percent. And the idea is bringing in documentation provenance and lineage. And what we are proposing for that is is building on those w three c standards. So building on the RDF data formats to build a semantic data catalog and using, vocabularies from from DCAD and BRAVO to, on the one side with DCAD model really What kind of data that is, where it comes from, and and how you can reach out to get more information about it, how you can get to the actual source of that data and combining that with Provo to understand the kind of transformations modifications that have happened to the data and, what happened before this data actually reached, the the system where you are interacting with it now. So to make it a little bit more visual, I have a demo. I have a fallback version as a video, but let's try if we can maybe have the actual demo system. So this is our, meta factory tool and, we have a number of asset management interfaces built in here. And I wanna start out from the data cataloging interface here. So what we can see is all of those different properties defined in in DCAD to filter the available datasets we have in the system. We, for example, can look at the link Nobel Prize dataset here. We can, look at that. And what we are doing is we we populate, a page in, some kind of a linked data browser kind of concept. So we are now on the on that individual dataset. We get information about this dataset. We can see the URI and the type of this dataset we have here. It is possible to modify the dataset. And all of those configurations are directly loaded from the DCAD model. The model that's defined with that DCAD standard, that open standard is directly driving the user interface and, how information is presented. From here, I can also explore the graph that's behind it. So I can, for example, see all of the incoming and outgoing links for this data set. I can see the the publisher. I can also look into the the model information of type data set. I can see there are number of other resources that are of that same type. So we can take one of those in as well and and see the the different kind of datasets we have in our knowledge graph and really navigate from one resource to the other and and get a better understanding. We have, auto populating knowledge panels that show you context information about everything in that knowledge graph. And through that, you can really browse your datasets and build a central system driven by an open standard by an open data model where you can learn about all the different datasets that are available to you and that you are working within your company. Now the next step is modeling your data. So now that we actually have information about where this data comes from, what kind of data that is, and, where we get to to more information about it, it is very important to put a model on. And I'm not talking about a classical relational database model or something like that. So this is not to describe the structure , the data types of my data, but this is about describing my domain model. Because it's very important if you wanna be effective in using the data at hand that you describe, what kind of data it actually is and how the different concepts and relations are interacting in your specific dataset. So what we are seeing here is a case from from pharma domain where we have a number of data sources in our data catalog and we are showing here how those public available data sources actually interlink on a on a higher level, model. So we see that we have genes and and proteins and and transcripts which are all, subclasses of biomolecular entities. And we see how a protein connects to a pathway. So without actually being in the pharma domain, I can get a basic understanding of of what kind of things are modeled in this data. What kind of information I can extract from that. And that's the idea of this approach. We are using, the OWL and and Shackle standards for that. So so most of what you're seeing here is actually modeled through OWL, But we see that, you also wanna model things like constraints. So you might wanna define, this relationship between gene and protein. So this encodes relationship we have here. How many, proteins can be encoded by a gene. And to define that, OLL doesn't give you any way to do that. So that's why we are combining it with Shackle to allow you to model those constraints as well. So, again, how does that work? So from the the perspective of metafactory, that's just another asset. So, in this case, we're looking at an ontology asset. You can also see that we have, an integration with with git here. So those are all not versioned in git right now but we have again the Nobel Prize dataset here. We see this is versioned in git with no active changes on this. And we can go into that now and we can really modify our model here. So what we see are, again, the the core concepts of this dataset. We are seeing, those constraints like there's an exact one relation between a word file and a file type. And one common step you would wanna do is, after you have actually built the ontology, which you can do in here, so you can can modify parts of the ontology. You can add relations. You can add additional classes. But you can also tie it into, other ontologies. So we see a lot of customers building kind of a more higher level ontology where all of the specific domain ontologies can tie into. And that's also why you're seeing such a lot of classes in here. So if you, for example, would only look at the local ontology you can see it's it's a lot less here. So let me quickly jump back to to all ontologies and, pick out something that does not come from this ontology itself but comes from another ontology. So for example, we pick out the the agent and, the document. We can see that for them, there are already relations defined. And now we can go here and say that an award file and take the subclass relation is actually a subclass of a document. Lawyer is actually a subclass of an agent. And now we can save this change. And we can also now do validation. So we can actually see how much of our data is conformed to this model. So if I'm going back to my, ontologies, I can do a validation step here and we should now get a new entry here for new data quality report. And we can see that, with our latest report, we have a few elements which are not conformant, with our model. So we can right away go in here and we can learn that the minimum cardinality for for file type and for year is not, correctly set for some of those elements. And we can now learn also which specific note does not provide it. So for example, here we see the device file. And that leads back again to our linked data browser. So the idea is really that everything is in the knowledge graph. The catalog is in the knowledge graph. The model is in the knowledge graph. And also the data validation information is part of that knowledge graph. And everything is interlinked and can be accessed from every point, of where you are in your dataset. The deontology or your schema, your model, that's one piece. The other one is, bringing that together with a controlled vocabulary. We are using SCOS for that. So you see an example where we are modeling, the the controlled vocabulary for diseases in this case. And then, we have built that system so that you have interfaces for specific user types. So we are seeing more and more customers adopting the the ideas of a semantic modeler being responsible for that overall modeling process, kind of the the gatekeeper to the model. Domain experts actively participating in modeling as well as, building out the vocabularies and data stewards that have some, interaction with those those models or ontologies, but are mostly active around building the data catalog, bringing the data into the system, mapping it into models and vocabularies, and making it available. To support that process, and and I don't really have time to talk about that a lot, but we have also implemented the W3C reconciliation service API draft. So you can use that to have a standardized interface to look up what is the idea the the IP for one of my, resources or entities. So every new dataset and all the new data you are bringing into your knowledge graph will correctly map into what's available already. So the third and final step is using your data. And that's interestingly one that's often overlooked when knowledge graphs are built in the beginning. And that has been, one of our focus points all along. So we, together with our customers, built, wireframes. So really discussing the the requirements from from data, people, processes for the user experience. And then take that into Meta Factory, which is also, a model driven local platform. So that that linked data browser you saw as well as all the interfaces, they are highly customizable which means they are just HTML pages, that are connected to a number of of templates where we use our, predefined and graph enabled components to put those visualizations and interfaces together. And everything that you would like to have in a different way you can modify. This all sits on top of a graph database where the data is stored. And all of the configuration like you saw it for the model, but also for those template pages is stored in a versioning system. Which makes ultimately the the platform itself a stateless component that's just putting the things together and creating the interface to interact with it. Or in the same way providing that through APIs to a larger ecosystem of analytics prediction, AI, ML solutions. We do have a public demo system based on Wikidata. And, if you if you open that system on wikidata. Matter of fact dot com, there's an example gallery of some of the components we provide. And I just wanna show you, two examples quickly. So there is one option here to combine a keyword search with a a tree visualization. So we are seeing the, the tree of, diseases here and and how that that connects in this taxonomy called MeSH. And here we can look behind the cover of how this is actually built. So what you see is it is our tree component. It has a query to find, parents and it has a query to do the the keyword lookup. And interestingly, you can also see that we are also federating here over a number of repositories combining mesh with Wikidata. So these few lines give you that user interface, that you're seeing here where you can freely interact with this data. Which means if you are, able to write a sparkle query, you are actually able to write a front end interface in meta factory. And if you are not able to write a sparkle query, we have an interactive query builder. So you can actually say I'm looking for a person connected to an organization, where that organization is the employer, and where that organization is related to a place, which is the headquarter location. And that might be London. And now we select the London we are interested in. And now we get a result and and we can extract the sparkle query from it. We can also combine it with and and or clauses and make it obviously a lot more complex query. But through that, I can derive my results that I can derive, an actual spark agreement that I can use then in those visualizations or I can even provide that as a query builder to my end users to interact with the dataset. So we have built a number of systems as I said. It's probably around one hundred applications we have built before our customers in the last years. Just some few examples here. And, as I mentioned it before, all of this is following this agile approach. So, that's also true for all of those assets, datasets, mappings, ontologies, vocabularies as well as the the UI templates. For all of them, we are following an agile approach with versioning in in Git and relevant governance processes around it. And the idea is that with that approach, you can really go from a proof of concept, in just one to two weeks to an an MVP, and a production system in a month or even less. So we have successfully implemented that for for some use cases in in just two weeks, getting something to the end user, being able to iterate in weekly sprints, getting feedback in getting additional data sources in expanding the model, expanding the vocabulary and so on. So, yeah. With that, I think I'm at the end of the the twenty five minutes. That that was the end of the twenty five minutes, and that was also me muting. What I wanted to do was actually enable my screen, Sebastian, for for setting up your your screen. At least it it happened after you had finished your your talk. So, it's a good time to open the floor to questions, and we already have one, actually. We've had that almost since the, the beginning of your talk. It's, one from Dean Aleman. It's a very specific one. So if, Dean will allow me, I will, broaden in the the scope a little bit. So, Dean's question is, if someone's already using Collibra for their data catalog, is there an easy way to migrate that into a knowledge graph like the one, you showed in your slides? I would expand the scope a little bit because, the question, excuse me, the question is specifically about one data catalog, solution. There are there are many in the market. So while it's generalized, like, is there a way for someone who's using some whatever other data catalog solution to to to transition, let's say, to what you're using. So we we are seeing that specifically, specific one quite a lot, actually. So I would say it's, it's part of, like, eighty percent of the solutions we have built around, data cataloging. And we are not yet where we wanna be with that. So, as we, for example, have also enhanced capabilities around the the visual ontology building just with the last release to the the point that you saw just now. We are also looking into how we can make that, onboarding of existing catalogs smoother and and easier. Right now there's there's some manual process still involved with that, but we we see that, over the I would say next year, this becomes a highly automated process to to speed up those projects, even further. Okay. Okay. Thank you. We also have, again, a rather specific question from from Daniel. I don't know if it's one you can answer, but I'm gonna pass it on anyway. He asked about pricing and whether we provide licenses for research purposes. Yes. So we, we are ourselves very active in research. So we also wanna support, research. And and therefore, if you are interested in an academic license or something like that, reach out. We have, under matter of fact dot com slash get started. We have a a public trial that's accessible to to everyone. You can run this in the cloud or on premises. And, if you have a research academic need, just put that into the, the comment field or reach out via email and we'll give you, directly access, to that. Okay. Actually, I also have a question for you, because at some point, you mentioned the agile approach and, iteration and versioning, but it's for data. And this is something that I see coming up actually a lot, increasingly, I would say, beyond the the confines of data cataloging or knowledge perhaps or what have you, it's something that's that's that's there is a need for, and it's getting more widespread. So even for people doing machine learning or any dealing with with data and data science in any other capacity, let's say. Because people are realizing it's a bit like code. So you do have iteration. So your data set will change. Your data set will evolve over time. So you need a way to keep track of that and different versions and which version everyone has and all of that. So how do you approach and resolve this issue in your platform? So for for UI templates, for ontologies, for vocabularies, it's fairly straightforward because the the amount of data is usually quite controllable and and the approach with using a versioning system like git for that is what's implemented most of the time. For data, it becomes a little bit more tricky because sometimes that's just not the approach that's that's relevant or interesting there. So we have a number of ways how we can do the the versioning there. We are, for example, working with, different, named graphs. So depending on the technology that's underneath in the database available to provide kind of a staging area for datasets, or other approaches around maintaining specific user changes. So we also have the the capabilities of providing those forms and every change is then tracked with provenance information from the user and versioning information. So you can actually play back all the changes that that users have entered entered into the system. And in the same way, you can also do that for for other ways how data might get into that system. So for for data, it's still a little custom based on the specific need. I think there's there's no one way to do it right, but we have, implemented that from just small scale to to large scale changes. And we we have approaches to to help with that. Okay. Yeah. I think the, the named graph solution that you mentioned makes makes sense. But as you also pointed out, there is can currently, to the best of my knowledge, at least not the standardized way to, to move between different name graphs. So I guess you have to somehow add some proprietary elements to that. It's it's actually we're not really applying a proprietary element, other than, you know, maybe a kind of a an extension to the vocabulary that we specifically use for that. It still stays RDF. It still stays, open and standardized. And what we can, for example, also employ for that is is LDP containers. So that's that's another technical approach, to help us, enable that. Okay. Okay. Yeah. I was just specifically interested in that because, yeah, as I said, and you also mentioned it's it's an issue, that many and many more organizations are facing. So I was just curious how you deal with it. Okay. So what yeah. We'll we'll have to be wrapping up so that, people can move on to, the next session. Thanks, everyone. Bye. Bye.",
    "transcript_length": 24557,
    "speaker": "Sebastian Schmidt",
    "tags": [
      "george",
      "peter hasso",
      "sebastian schmidt"
    ]
  },
  {
    "title": "Thrill-K: Rethinking knowledge layering and construction for higher machine cognition",
    "description": "The AI industry is now facing its next big challenge.\n\nWhat are the necessary properties of representational structures that could allow vast amounts of data become meaningful in the human sense of the word? How can knowledge architectures be constructed in a way that allows for both the efficiency and effectiveness of models they support?\n\nIn this keynote, Gadi Singer will discuss anthropomorphic conceptual structures and their benefits for enhancing Cognitive AI capabilities, as well as his model of the three levels of knowledge \u2013 Thrill-K \u2013 which can serve as a blueprint for building AI systems that are both efficient and scalable.",
    "category": "Knowledge Graphs",
    "transcript": "Greetings. I'm very pleased to be here in CDW twenty one and talk to you, knowledgeable people, about knowledge and how we should rethink it, restructure it, and prepare it for a better AI. The last decade has been phenomenal for AI, primarily because of deep learning. But there's a next wave that is coming. And whether use system two definition that Yoshua Bengio is doing based on Kanban's or use the DARPA definition of the third wave of AI, there's something new that's coming. AI that is more cognitive, that has better understanding of the world, that has higher intelligence. And this is going to be done through a combination of components. It's going to have neural networks in it. It's going to have symbolic representation and symbolic reasoning in it. And, of course, it's going to be based on deep knowledge. And when we have it, the value that is provided to individuals, to businesses, would be redefined and much enhanced compared to even the great things that we can do today. This will require new architectures to implement it, which we'll talk about later on. And with all that coming together, a new human centric cognitive AI is about to arrive. I mentioned deep knowledge. What do I mean by that? If you look at the nature of recognition versus higher level cognition, they are very different. In recognition, there's a world out there with relatively shallow data. If you look at pixels, you just have their location, the color, their intensity, but the data is relatively shallow. It's continuous. It's differentiable. And you need deep learning and other methods in order to structure it, find what is the meaning of that, and then make it into knowledge. Cognition is different. High cognition requires to to understand a very deep structure of knowledge. Let's take the example, is grandma well? In order to pass and understand what is grandma, what is this particular grandma, what is expected in any particular age, what's the history, and many other things. What is relatively well for this particular grandma. There's so much knowledge that's in there that all comes forth with a simple question. So deep knowledge are knowledge constructs that allow an AI to continuously acquire new information, organize its internal view of the world, comprehend the meaning of grandma and well in context and reason on its knowledge. Language models are great, and they are able to fill in word, generate new statements, and in them, there's a lot of knowledge. There's statistical information, of course. There's a lot of factual knowledge, and there even some common sense knowledge. And you can see it in some of the benchmarks like the natural questions or trivia QA, where models like t five that were trained through masking and other methods are able to give pretty good results. And in addition to just having those direct ways of assessing how much knowledge is in there, you can enhance it. For example, the work that was done in Eugene Choi's group on Atomic shows that you can extract from GPT three a lot of common sense, information and enhance a knowledge graph by ten x from common sense information that is within a large model, a GPT three. And GPT three by itself, by being prompted, can give you some really nice common sense answer like x starts running so x gets in shape and so on. So overall, it's good. Language models are one of option for good knowledge models. There's also some additional value that has been quoted on why there might be a very good knowledge model because, there's no schema engineering, there's no need to have preset relations and also there's no need for a person, for humans to provide supervision. So, are language models great knowledge models? And the answer, I believe, is not. Language models are not great knowledge models, and it's true both on the effectiveness, which is how well they perform, as well as their efficiency, which is how much compute do you need in order to perform. And let's take the same t five and the benchmarks that we just show, very good performance in. When you integrate retrieval, such as in RAM or RAG or more recently infusion and decoder, you can get a much better results like going from thirty six point six to fifty one point four on the actual f one or the exact MET scores. And you can do it with a much smaller model. By using information, in this case, that resides outside and effectively bring it in, you improve both efficiency and the quality of the result. Well, you're saying that language models are performing very well as a repository of knowledge, so what's the problem? Well, they were not designed for that and even though they do well, we need to think about what is really the optimal way of representing knowledge. And there are some basic limitations in the representation and other aspects that make language model good but not great for the task. And we should remember that sometimes there's so much information learned in those language models that we can extract and enhance dedicated knowledge models. But by themselves, there are several shortcomings, and let's look at what those are. So what makes for a great knowledge model? I would say five areas of capabilities, scalability, fidelity, adaptability, richness, and explainability. Let's talk a bit about each of them. The scale of information that we're dealing with is very high, and you can see that if you look just at the sizes of the recent models. How do systems deal with this tension? On one hand, you want to have the largest scale possible. On the other hand, you want expediency. You want the ability to do it in a very cost effective manner. There is a way that systems deal with that, and that is by creating tiers or a hierarchy. You want to differentiate between things that you access every tenth time to maybe items that you access every ten trillion times. And by providing this hierarchy such as in a computer system, you are able to separate the closest thing in the computer system. It would be within the registers or the cache. Can be easily accessible. There's a small scale, high cost, but you get it when you need it immediately. And then as you go to further and further tiers, you have a larger scale, more capacity. The speed is lower, and the cost is lower. So by having a full hierarchy system, you can have the scale of the largest tier, but you can do a much shorter access time and better cost than with a single tier. And many systems in nature are operating in the same manner. Let's take an example from a completely different space. Energy gets to our cells by transforming ATP to ADP, and it's just there. How do we get this energy? We get a glucose circulating in the bloodstream. So there's lots of energy going around but still needs a little bit of processing to become available. But all this glucose circulating is not enough. There's storage, there's backup, and that one is glycogen or others that is sitting in liver, muscle cells, fatty tissue, and can be turned into glucose. But there's the external, the largest tier is of course the outside world where you have all this energy that can be consumed, digested, and then turned. So the idea of having a tiered system is one that is in all environments and settings, which require a huge scale with very fast access. Another area is the richness of knowledge. The ability to capture all the complexities, all the relationship in both in the language world as well as in other modalities. And this is something that I'm, invited to read in a blog that I published on the dimensions of knowledge. But just in a nutshell, there's knowledge about the world that needs to be captured, and it could be descriptive like taxonomies or ontologies and and property inheritance. It could be either language or it could be a three d point cloud about some some, assets, some physical objects that you capture inside. The second is ability to capture models of the world. Causal models, procedural models, physical models, how things are related and moving in time, in space, in progression. And finally, scripts and stories. It's not just things that are point. There are complex story that we tell ourselves that we know things about the world which needs to be captured in this form. And then there's the meta knowledge, context, source attribution. So if you read something in a, in a magazine from the Newsstand or you read it from the, the New England Medical, Journal of Medicine, you will treat it differently because you understand the attribution to the sources. And then there's values and priorities which are so important as part of our knowledge structure. When is something more applicable? What's the value that we put on it? And then finally, concepts. Concepts are the most important construct that we use as humans to think and build views of the world, and concepts are the one that cutting across all those views. The next area is adaptability. A knowledge model must work with multitude of data sources, types, and update rates. So if I'm a dentist with a small office, I want to be able to work with my patient's information and be able to update it quickly and be able to have different types of information in there. If I'm a financial institution, it's very different types of data. Also, the rate of change and update of the model is very important. The model need to change as fast as the relevant part in the world changes. So if it's about tracking where people live and what's their phone numbers, that's one rate. If it's tracking the latest earthquake in California, it's a different rate. And there's no path where going through a retraining or even fine tuning with new data can refresh as quick as the world changes in some of those cases. The next area is explainability. When an AI interacts with humans, how can the AI explain itself, be interpretable and communicate in a very natural manner? And this is better structured when you have an explicit, intelligible source of knowledge. For one, all the structure is built in a way that is transparent. Whether it's a taxonomy or other relationship, it can be explained more simply. It also have all the constructs to do what if analysis counterfactual in a way that can be mapped back to this explicit structure. And finally, it gives a better foundation for a human AI communication where information can be exchanged, adopted, integrated, and communicated back. Finally, fidelity. Knowledge models must retain information in a form that allows faithful reproduction to the origin, whether it's facts, attributes, relationship. When you store them in a knowledge graph, you can store them in a way that is true to source. You can also store them in a way that is not dependent on statistical occurrence. So whether it happened once or it happened multiple times, you are much less sensitive to catastrophic forgetting or other types of decay. And also because you have the full structure, because you have the sources, you can diminish some of the effects of training based bias. We spoke about all those principles of having a great knowledge model, but how does it translate to an actual architecture that can support the next wave of AI? And I would posit that the architectures need to have knowledge and information at three levels. It can do more, but three levels is the necessary and sufficient partition. So I call it three levels knowledge or it sounds phonetically like thrill k. And the first level is for the most immediate knowledge, and that knowledge is, let's call it giga scale. And it should sit in the newer network. The next level of knowledge is the deep knowledge base. So this could be a knowledge graph, for example. This is where you have intelligible, structured, explicit knowledge. And this one could be at the terror scale. And this is available whenever is needed by the neural network. And finally, there's the world information. And the world knowledge or world data is considered to be zeta scale. You've got things today in terms of the flow of information that are reaching that level of ten to the twenty one. And we need to plan our system for the Yetta scale, the ten to the power of twenty four. So we have those three levels of knowledge, The immediate, the one that is associated in the knowledge base, and the one that is supporting out there in the world. How do we put it together as an architecture? So how does the three k architecture look like? It has those three elements. It has the neural network with a lot of knowledge within its parametric memory, and simple flows could just go from input through neural network to output. But it also has the knowledge base, structured, explicit, intelligible knowledge base. So when needed, you can have the extraction into the neural network. And I'm saying reason because there's not only the symbolic representation which is in the deep knowledge base, but also the ability to do symbolic reasoning. And then, since you cannot have all the information of the world inside your model updated at the right rate, there needs to be access to the outside world and all the corpus of data, documents, images, earthquake information, financial, everything that's happening out there based on the application. And there needs to be the retrieval mechanisms, obviously, but there also needs to be a knowledge acquisition that continuously updates an ever growing consistent view of the world within the machine. And this is how we can avoid going to some of the shortfalls of handmade expert systems of the past. So how will this new AI look like? What is this new cognitive AI that will emerge? Well, you have to imagine beyond statistical correlation, beyond the type of things that we are very impressed by today. You need to imagine an AI that actually understands language by understanding concepts and relationship, not just predicts the next word or the next phrase. You need to imagine an AI that is inherently multi sourced so it can converge multiple sources into its knowledge. And multi model, because the world around us is multi model and rich. It integrates common sense knowledge, and common sense knowledge is the basis for making reasoned intelligent decisions. It can adapt to new circumstances and new tasks because the data and the knowledge is not structured for a particular task, but it's there with all its richness and expressivity to be used for new tasks. It explains itself and communicates better based on what we discussed earlier. And importantly, it is more robust and customizable. For the long tail of use for AI, the AI needs to adapt to particular environments, to your car, to your home so that it learns the specifics, it builds it into its knowledge, and it can interact with you in the most appropriate way for you. So those competencies together create a anthropocentric cognitive AI that can be achieved only if we use knowledge underlying, which is explicit, intelligible, and well structured. So what is needed to make cognitive AI a reality? Well, it takes more than a village. It takes the whole community to do it. And some of the things that are required are the capabilities to access and to mine diverse sources of knowledge and be able to bring them into the knowledge model. We need to have strong semantic parsers and knowledge representations for all this diverse multimodal knowledge. We need to have hardware and software that is able to do all this, hierarchically, on the fly, in a cost efficient manner. We need to be able to have some apps that are really showcasing this to drive the need and make all the necessary improvements on those workloads. And finally, I expect to have a new marketplace created since this is the age of knowledge emerging. Knowledge structure or knowledge basis can be sold, communicated, and used as other precious resource that we have around us today. We are doing our part of this within Interlapse at my emergent AI research organization. We are looking at multiple aspects of this, including NLP, multimodality, common sense reasoning, neuromorphic computing. And this is just an example of demonstrating the match that is being created between a word being addressed by the language transformer, part of the image that's corresponding, and there's also a data structure that is mapped into that. And this work is featured in visual comic leaderboard and other places. Let's bring it all together. So when is it going to happen and how is it going to look like? I expect that it will take about the same duration as deep learning took if you compare two thousand twenty one to twenty eleven. So this year, cognitive AI is still a nascent technology. By twenty twenty five, I expect it to be already in some commercial use. And by the end of the decade, I expect it to be a mainstay. And Thrill k promises to be the blueprint of how it's going to be implemented, And it needs to bring together new network with symbolic representation and with symbolic reasoning. It needs to be able to adapt and discover information from the outside world so it has a continuously growing knowledge base. It has to be able to provide support at scale to a diversity of users as well as usages through its flexible knowledge structure. And it's gonna build around deep knowledge that is conceptualized, multimodal, ontology based, and it's ready for the unfolding future. Thank you very much.",
    "transcript_length": 17427,
    "speaker": "Gadi Singer",
    "tags": [
      "atomic",
      "darpa",
      "decoder",
      "gpt three",
      "infusion",
      "interlapse",
      "kanban",
      "nlp",
      "rag",
      "ram"
    ]
  },
  {
    "title": "UBS Helix Knowledge Graph: The Power of Information",
    "description": "UBS Helix Knowledge Graph was created to unify and streamline the information about our technical estate to maximize the impact of our technology spend :\n\n- Break the silos of our applications\n- Reduce redundancies and duplications in the structure and vocabulary of our applications\n- Enable a 360o view of technical assets.\n- Reduce complexity and eradicate undocumented data definitions\n- Make it easier to rapidly answer questions about our technical estate\nEnable a 360o view of technical assets.\n\nReduce complexity and eradicate undocumented data definitions\n\nMake it easier to rapidly answer questions about our technical estate\n\nHelix provides Business, IT Managers and every single user of data, the power of knowledge by answering questions common not only in a financial institution but in every organization that has an IT Department:\n\n- How many application do I have?\n- What infrastructure do they leverage?\n- Where do I have Technical Debt and how can I reduce it;\n- Where do I invest my money?\n- What does an application cost me?\n- For which business capability am I spending the most dollars/have the most opportunity for consolidation?\n- Where am I spending too much money on applications I should be decommissioning?\n- What data do each of my system has and how does it flow?\nHow many application do I have?\n\nWhat infrastructure do they leverage?\n\nWhere do I have Technical Debt and how can I reduce it;\n\nWhere do I invest my money?\n\nWhat does an application cost me?\n\nFor which business capability am I spending the most dollars/have the most opportunity for consolidation?\n\nWhere am I spending too much money on applications I should be decommissioning?\n\nWhat data do each of my system has and how does it flow?\n\nHelix is based on open and standard technologies and model-driven architecture.\n\nProvides an open framework for data publishers to integrate with Helix and also unlimited possibilities for users to create their own search for information in an intuitive and web-like manner.\n\nNo more spreadsheets, no more predefined APIs and reports that need programmatical effort.\n\nUBS Helix Knowledge Graph is the new generation and the technical landscape is only the beginning.",
    "category": "Knowledge Graphs",
    "transcript": "Hello. My name is, Natasha Varitimou, and I'm a data architect in UBS. More specifically, I'm the modeler in the UBS knowledge graph. So, we will discuss today what were the issues and requirements that we had in UBS and how the UBS knowledge graph facilitated the way that we deal with these requirements. So, every organization in order to increase its revenue has two ways. One way is to increase its customer base, which is something that every organization is trying to do, but you cannot do it multi fold over time. The second way is to decrease its debt. So the question that we had in UBS is how do I decrease my debt, and more specifically, how do I reduce my debt in the IT world, in the technical landscape? So in order to do that, we had to answer some questions. What are my applications in my IT landscape? What is the software and hardware they leverage? How much does an application cost me? How can I reduce this cost? Do I invest too much money in applications that I should be decommissioning? There are a lot of questions where the answers were not that straightforward. It wasn't easy. It wasn't we couldn't give answers immediately. They were it was very time consuming to actually search for the data that will actually help me facilitate me to answer these questions. That is because our IT landscape, the view for our IT landscape wasn't very clear. The data that would help us answer these questions were hidden between behind different applications, so they were hidden in sign laws. They were named differently. We had many duplicated information from application to application, and, of course, we had redundancies. And, all these questions, these challenges were not only we don't, face them only in UBS. I have seen this is actually a data definition and data integration problem, and I have seen it in many organizations and independent the size. I've seen it in many sectors. I have worked in governmental, projects, in my previous roles, governmental projects in Brussels. I have worked in oil and gas projects in Norway. I have worked for life sciences and pharma. I've worked for consumer goods. The latest years, I have been focusing in the financial sector, but as I said, independent the sector, independent the size, independent the project, the challenges were always the same. The data questions were always the same. So the solution, in a few words, is federated and model driven layered architecture, and this is what I'm going to present to you today, how we solved how we created this in UBS. So So I'm gonna structure the presentation by breaking down the data challenges one by one and giving and presenting the solution that we actually gave for each challenge. So the first question, and it seems very simple, while it is not is what is my data? We need a consistent description and representation of the data. Sometimes the the same real world concept in the same organization, but in different parts of the organization or maybe in the same part of the organization between different teams, the the same real world concepts are called differently. So some people might call something a system, for example, and then other teams might call it infrastructure, and other teams might call it, hardware. So we don't know that we indeed, we are talking about the same thing. We need common terms to the organization . We need descriptions for these common terms, and we need to know what is the what are the relations between them. So what we actually need is a model. This is what a model brings to the table. So we need an enterprise agreed model so everybody would refer to the same real world concepts the same way, and we need to document it in a model. Now, if this model is a UML model, is a model documented in a picture, in a conference page, in a document anywhere, then people could understand and see this model and maybe see the descriptions as well in the relations, but machines cannot understand it. So machines cannot answer the same questions that people can. So in a question, a person could go and search based on this model, but the machine cannot a software agent, an application cannot search for this, based on this model. So we need machine readable models. So the same way that a person, a human can understand it, a machine or an application can understand it as well. And we also need the data to live together with these machine readable models. This is what Tim Berners Lee called knowledge graph. Machine readable models and data living together so everybody can search for the information they need, not only, as I said, humans, but also applications and software agents can navigate through these models, get the data they want, can discover new information, follow their nose, and, do more complex queries. Right. Some organizations have, reached this level of maturity, but, at least to have an enterprise agreed model, but just a few and probably, UBS is the first that is doing it in such a large scale. We are doing it actually we have a a knowledge graph in the middle of our architecture as you see here in our in this, diagram. So in the middle of this pink part is actually our models, which are we call them upper ontologies, which are normalized, consistent, clear descriptions of our data. Actually, we have gone one step further. So they are not just ontologies. They are Shackle models, which is Shackle is a a state of the art, modeling language where you not only describe in a machine readable way your model, but you actually can use this model to validate the data against this model and report some inconsistencies, if any. So in this middle layer in our knowledge graph, we have our upper anthologies, and we have our data living together consistently. In our upper ontologies, in our upper shackle models, we are trying to utilize as many standards as we can so we will, we don't have to reinvent the wheel, and we'll also, succeed interoperability. So we are utilizing we are using as much as we can Dublin Core. We're using consents from Fibo. We are using, Prov for provenance. We are using TCAT to describe datasets. We are using DataCube and SDMX to describe statistics, metrics, and dimensions. We are trying to reuse as many standards, especially w three c standards, as we can. As I said in the beginning, we began by trying to describe our technical landscape, so we needed a technical ontology. However, this was, surprising, I would say. We couldn't find something to reuse. There wasn't a standard technical ontology out there that we could just reuse and extend if necessary. I see surprising because every organization has an IT department. I I suppose that every organization wanted to wants to describe its applications, its how hardware, its software, all the cloud capabilities they have, how they are all tied together. However, as I said, we couldn't find any, so we created our own ontology. So that is our model layer in our architecture. But then the second challenge is how do I bring my data in and describe it with these models? Where is my data? Where is this data coming from? How do I track their provenance? How do I track how the data flow, in order for me to have a lineage, aspect on my universe? For example, if I change something in my data, what does it affect downstream? Or if I have a change somewhere, what does, if I have something, how do is it affected from any chains they do upstream? This is what we call lineage. So this is a second this was the second challenge. So the the solution to this one is federated architecture. We don't have we we utilize microservices to actually bring the data in from its data sources. The data sources themselves are also described very clearly with taxonomies. We have people and roles around these microservices, so we have a well documented, ecosystem of each data that are coming from this microservice. Again, for the microservices and for our publishing, we didn't create a wrong technology. We didn't use a proprietary technology. There was again, we didn't reinvent the wheel because there was there were technologies out there, very well known ones, tried ones, and guaranteed. It was the HTTP protocol that is used in the web. It was the URI's URI techniques. It was, as I said, vocabularies like d cat where we will describe the datasets as they were coming, as you see here, from the different publishing microservices to our knowledge graph. And, of course, we we also utilize Prove. So every time a bunch of data, what we call dataset, is coming in the knowledge graph, we always track and we see an example here. We always track what this dataset is, what are the persons, and what are the roles around this dataset, and we always track where they are coming from. So a dataset might be generated by an ingest activity coming from an original source, from an application, but it could also be generated by a transform activity derived, as you see here in the example, from two other datasets. These datasets, in their term, are generated by a transfer or by an ingest activity. That way, we, track the lineage of our data with datasets being first class citizens. Third challenge, I have now a very good architecture. I have my data very well published, described, consistent, and, with usability and, guaranteed data quality. How do not, how can I now use this data? Different teams have different needs on the data. For example, there might be, one part of the organization, one team that would, need just a specific glance, a specific lens on the data and not see the whole universe, a knowledge graph can be, intimidating sometimes because it has a lot of information. So someone would just want to see just a lightweight, simple, view of the data. There might be other teams that want some of the data, so cherry pick some of the data, and they want it in specific formats, other teams in other formats. So how do I cover all these cases? We have different ways to deal with these challenges, so our ways vary for facilitating different, extension lighter models or our on our, bigger, fuller knowledge graph to having, different types facilitating different types of queries and rest APIs. So in our architecture, this is described here in the upper layer. So as you see, although this middle layer, our knowledge graph creates the integration platform, we can have multiple ways to interact with it. That will vary. We have a a very nice UI, so it's like a a web browser search on our knowledge graph. So someone could just, put a term in it. And a lot of links appear where you can follow your nose and discover more information just like you would do, in a search engine. We have a a Quibi interface, a sparkle endpoint, so someone could write very, more complicated queries. And when you save them, they are immediately saved as REST APIs, so they could be, invoked from any other application or invoked from any user in the UI and see the results in the UI. We have, what we call guided search where it is again a web browser, just all the other browsers, but you can, you can cherry pick specific properties, specific information on your on your concepts, to see. So, actually, under the hood, that creates a sparkle query, but it this is under the hood, and guided search provide to users that are not familiar with SPARQL. Most of the users are not familiar with SPARQL, an intuitive way of creating complex queries in the knowledge graph. We always we also have, of course, free text search, which will make more efficient, based on elastic search, and we are going to create a faceted search. So someone could search and filter based on specific properties on the concepts. This is the more, direct way to interact with a knowledge graph. Sorry. Go back. But, there will we also have other teams that they interact with a knowledge graph, with a consistent integrated middle layer that we see here. They interact in their own way based on different the different ways that I said before. But, also, there is a way, and I think that in the future, it would be more obvious as we bring more data in the knowledge graph and we have more use cases. There will be the need of lighter, extended models, as we call them, where the data are also described with these lighter models, and we need to transform the data from the fuller, more, heavyweight knowledge graph to these lighter small pods. Of course, the trick here is, always be synchronized and consistent. That is the trick. And, otherwise, you can interact in many simpler ways, but the with the middle integration layer. There are also, though, times, and this is the last challenge, that, you cannot bring your data in the knowledge graph. This might be for time or priority or resources restrictions, but it is not, always possible immediately to publish the data in the knowledge graph. In this case, if I cannot integrate the actual data, what I do is I can integrate the metadata and that is what we call dataset registry. So dataset registry, I have seen it with different terms in different companies. Other companies call it metadata repositories, metadata registries, dataset catalog, data dictionaries. What they all mean and what we actually do in our dataset registry is in order to achieve, again, the integration and facilitate integrated information across our data sources when we cannot bring the data itself in the knowledge graph, we are bringing the description of our datasets. So we are not bringing the datasets in the Knowledge Graph, We are bringing their descriptions saying that this is the physical structure. A dataset actually although you don't see for example, if it is a tabular dataset, maybe we don't see its rows, but I'm going to describe and say that this dataset has, actually, these tables and these columns in it, and I we map these tables and columns, these physical structures, again, to the enterprise agreed logical model. That way, you can query over your universe if you manage to map it to the logical model. You can query your universe just as if data were in your knowledge graph. However, in this case, just because you don't have the data, you will need a resolution service here to say, what is it that you want? Do you want applications in software and hardware? These datasets have them in. We can see it from its physical structure, and this is how you can get it. But you will need now the resolution service based on these instructions to actually go and get the data. So our architecture in our dataset registry is actually to describe the datasets again, their physical structures, the way they map to the logical enterprise agreed model, and, that way, you can actually get the information again that you need. Just want to emphasize again here that dataset registry is a very nice first step to integrate your universe, but it is just the first step. It just just a small part of your entire knowledge graph. And the biggest benefits, the greatest arch data architecture that you can have is when your data is in your universe. But as I said, sometimes you have to do things, step by step, and the dataset registry is, for certain, a very well way to start, getting to integrate your, data landscape. So, we are actively working on our dataset registry, which is the catalog of catalogs. We are bringing we will we will bring all datasets in, and we are going to have an explorer where someone could go and search the datasets, see its usability, and structures, in the same way that, our explorer would be the same way that Google had, its dataset explorer. We are also using d cat, to describe our datasets, but we extend it, as we're saying, to describe their physical structures and their mappings. I think that is, what I wanted to share today, and, hope, I gave you an overview of the federated and model driven layered architecture that we are establishing in UBS. And, looking forward, for stimulating discussions in the context of the conference. Thank you very much, and, thank you.",
    "transcript_length": 16020,
    "speaker": "Natasa Varytimou",
    "tags": [
      "data dictionaries",
      "datacube",
      "dataset catalog",
      "dataset registry",
      "dcat",
      "dublin core",
      "elastic search",
      "enterprise agreed model",
      "federated architecture",
      "fibo"
    ]
  },
  {
    "title": "Van Gogh Worldwide: Constructing and searching a knowledge graph of linked art",
    "description": "Van Gogh Worldwideis a platform that brings together professional information about the work and life of Vincent van Gogh.\n\nIn this presentation we share how Spinque realized the platform using information from cultural heritage institutions worldwide.\n\nOn the knowledge construction side we touch upon the lessons learned to construct a knowledge graph in a loosely distributed fashion, to capture the sweet spot in data modeling complexity and the challenges to motivate a community.\n\nOn the knowledge access side we demonstrate how Spinque\u2019s probabilistic graph database is used to implement both the graph queries as well as the full-text search.",
    "category": "Knowledge Graphs",
    "transcript": "Well, hello again, everyone. And, hopefully you managed to find your way in. Hope we didn't keep you waiting too long. It's a pleasure today to be introducing in, knowledge connection twenty twenty, someone I know for, well, over a decade, I guess. We have, the pleasure of having, Mihir, with us today from, Spinken. And Michiel and I, actually were colleagues for a brief while, back in Amsterdam, like I said, over over ten years ago. We were both in related areas in knowledge graph, what today actually is called knowledge graph. Back then, it wasn't called that, but you know, same same difference. The topic was still the same. His was more on the visualization and UI UX side, and I guess he's he's kept to that. And this is what he's here to talk about today as well. So I'll get out of the way and let me hear it. The only thing to mention to you, if you want to share anything, like questions, comments or anything else, by now I guess you know the process, this little chat box. I'll keep an eye on it and in the end of the presentation address your questions and comments to be here. So the floor is yours. Thank you. Welcome, everybody. Nice to be here and nice to see some familiar people in the participants , list. Alright. Can you put on my my screen share? I should pause that and then quickly switch to this one. You should be able to see my slides now. Right? So let's put it in full screen. So for Fog Worldwide, I'm going to talk about so this is a project you have been working on the last months. I work for a company called Spink. You see the logo here at the bottom left. We are actually in the business of creating search solutions for for knowledge graphs. And we're gonna have a look at how we apply that here for this project for worldwide. So before I say anything else, I should really give credit to the organizations and the people that were involved in this project. So here at the top, you see three cultural heritage organizations in the Netherlands. The one on the right, you're probably familiar with. And here at the bottom, you see some other companies, more technology companies, design companies involved in this project. And I myself, as I said here, at SPYNC. So there's three things that I would like to do with you today in this talk. Of course, we're gonna have a look at for a whole wide and, why are we doing this and how. Next, I want to make a, a claim for cultural heritage organizations to connect their data. And finally, we're gonna have a look at how we employ the concept of search design to access knowledge graphs. So let's start with Vincent. So physics for growth really is a worldwide phenomenon by now. So this is a visualization of his artworks where they are currently located. And as you you might wonder what what is here in the Pacific Ocean? Well, this is actually Honolulu in in Hawaii. Right? So it's really spread out to all continents all the way to Sydney, Australia. And what we do in projects like this is that we usually start from the end user perspective. So what are the search tasks that people are having for this type of data. Now you see here a picture, a little bit empty, as you probably can expect. During this presentation, we'll start filling the rest of this picture. But we start here from the right side. So what are the users here? Well, in this case, these are really professional users. So these are researchers, in the art humanities domain. And you have a couple of examples of research questions that, that they have from actual people. For example, Verhoeff painted several artworks from the family Houlain in France. So which members of the family did he actually paint? A much more complicated question is this one. So, for several artworks of van Gogh, it's not exactly known when they were painted. And sometimes also the order of in which they are painted was this one before the other one is not known. And finding this out is a big topic. And they use a lot of information from that. From this range is really from analyzing the paint, the actual material to the canvas, that it was was used for. For example, it's two paintings use canvas from the same role. This is, of course, an indication that they were created close by in time. Another picture, another question is actually, I think, for someone else attending now, is more sort of a social aspect, right? So how are the paintings of a book perceived throughout time, right? They are exhibited for more than a century. And how were they perceived? How were they exposed together? How was it how did people write about those paintings? And how did that change throughout the decennial? All right. So that should have given you an idea what we're trying to do here. So let's just go there. So you should see now, the website, and, let's try to start with the the simple question that you just saw. Let's search for, Now what you see here is six paintings made by. The first thing that I have to say is that these are actually not all the paintings by this family. Because at the moment, we have collected all the paintings, or all artworks, I should say, in in the Netherlands, from a dozen institutions. The second phase is, is currently starting. And in this phase, we're going to collect all the information from the other institutions. But already plenty to see here. But of course, this is not so spectacular yet. I mean, this you can also see see on Wikipedia. So let's dive into it a little bit more. So for example, this, this cute little boy, again, becomes more interesting because already here on the right side, you see images and becomes even more interesting if I open this because these are actually available in high resolution. So this means I can can zoom in and and look at these boys' eyes. And this becomes even more interesting if I start looking at more technical recordings of, of this painting. So this is what is called breaking light, where the light is shared on the painting from the side. And this really gives a good view on how Vergoc used his paint to to get all kinds of effects, right, the thickness of the of the paint here. Another thing that's very interesting is this canvas is, of course, stretched on a frame. And this frame contains all kinds of information. Right? So there are are label stickers on it that indicated who has owned this painting and, where it was, for example, exhibited. So here is actually one from nineteen twenty eight from an exhibition in Germany. Later in time, I guess they wanted to protect this frame and they added a cover on top of it, which when you borrow the the painting, you're not allowed to take off. So the more recent exhibitions actually have their, their stickers here on the on on this back panel. Okay, so let's close this high resolution few and scroll down a bit. Of course, there is object metadata about the artwork. But there's also various other things. For example, van Gogh has written a lot of letters to mainly to his brother, but also some to some colleagues. In his letters, he he often mentions his artwork. So the same for for this artwork. So you can read about, what he said about that. Who has previously owned these artworks, going from his brother to, to where it's currently located. And here we see those, those exhibitions. So what we saw there, the sticker at the background, on the on the frame in nineteen twenty eight is actually this exhibition. So we can click this exhibition and now see which other artworks were also exposed there. And so that gives you, a completely different perspective, on the artworks. Now there are various other things that you can, can explore yourself, after this presentation. So let's get back, to the presentation. So what we've done now, we've we've seen sort of the user side. So what's happening in the background? Of course, there's all kinds of data coming from, from institutions, and somehow we want to create a knowledge graph out of this. Now I want to approach this part of the talk from a sort of personal insight that I gained during this project. I've been working in in Credoheads already for, for about fifteen years coming from this technical perspective. But what I realized in this project is that there really is a wealth of knowledge in these this organization. So you can truly call them knowledge institutes. But what surprised me is that only a small part of this knowledge is actually available online. So, of course, these numbers are speculative that I that I present here. And this will not be the same for all. For some, this will be a lot more. But in general, there's really a lot of stuff that is not available online yet. What we managed to do with is to increase this. And so, for example, these exhibitions that I showed you in the high address, photos are now made available. So in that sense, FOCO worldwide has has helped to reach the full potential for for cultural organizations. But what I truly believe is that, for the cultural heritage organizations themselves, there is so much potential to gain if they start to, organize themselves around connected data. So not only just for sharing it to other portals, but also for themselves because it will empower them to do so many more things and truly be a a knowledge institute on the web. So, that's almost, almost a call out to these organizations. So how should they do this then? Well, this should start with within their organization. And so there's not just this collection management system from which you see a screenshot here on the left with this typical object metadata. That was actually the stuff that I have known for years. But there is so much more. And so you have already seen some of those assets. And some of the things you haven't seen is also x rays or infrared photography. There's really a wealth of information there. Then there was documents, literature, research reports, publications, all discussing and providing interesting perspectives. So the organization should really organize this and start connecting this data. Secondly, they should connect to external authorities. And there is a lot of knowledge inside, but outside, there is a lot that can complement this. And here you see a couple of examples for people in the cultural heritage domain, probably familiar with something known as the art and architecture or in short, AT. This provides very rich information, for example, about materials, type of paintings, etcetera. Of course, literature, for example, as as modeled in Fiaf. And in the Netherlands, is a very interesting source collecting information about artists, museums, owners, etcetera. Right? So all this information provides a way of integrating data and at the same time providing additional backgrounds information. Okay, so given these two things, connecting inside and connecting, adding external information to the inside. How to do that? Well, very good news that since, since a while now, there is a community effort in this domain, on a shared data model. And they call this linked art. And this is a very nice initiative because what it does, it really tries to find a sweet spot, on top of a very expressive model, which is called CDOS AM, but which is maybe a bit complicated to use. And what LinkedAR managed to do is really reach the sweet spot that made it possible for organizations to start using to start express all this which which information that they have. And, now here you see a snippet of that. Of course, we're not go gonna go into detail. But I do wanna point out a couple of things here. So for example, the reference to an AT concept here and the reference to a place. And And also interesting here is the modeling of dates, which actually allows you to model vagueness in dates. So that brings me to the last one is that the quality of data is really a big thing. So a lot of time during this project was actually spent by the data owners in improving their quality. And so in the collection management systems, they often, for example, use not complete dates. And this this is the case because they don't they don't know the exact exact date. It's either in a period that is made or they don't know exactly when it started or ended. And by allowing them to actually, model this fakeness, we can then start doing things like sorting updates, which is of course not possible if you don't have a well formed date. But there are various other issues in data quality as well. Now what we notice in this project is, expressed here by this, by this quote. So you can read it yourself. So what really happened is that once they saw information, their information being used on the web and seeing, hey. This is actually complete or this is wrong. That really was a big incentive to start improving the data. One of the examples, if you then have good data, you have it connected to external authorities. For example, here in for wide, you see these these filters on on the left side that you're obviously familiar with from all kinds of search applications. Now here, this gives a very nice overview of the high level what kind of different types of objects there are. If you've seen any website that aggregates, especially culture data from different sources, usually these things are a mess, Right? So what we do here, we use controls for gamblers for all of that. Meaning also that if you don't use them, you will not end up in these filters. But we provide some sort of flexibility because although we show these high level categories here, which was one of the wishes of the users, We allow also institutions to provide many more specific things. But by using the relations in this thesaurus, we can then provide the level that we need here in the interface. So we use the background knowledge for that. All right. Let's come back to this picture. So we've seen now, coming from the right side, that we start to understand the search tasks that are involved here. We have seen how organizations can capture their knowledge, and they've actually been doing that themselves. Right? So this is not that we start harvesting their data and do this. They actually have been doing this themselves, which was really cool for us. But clearly in this picture, there's there's a gap in the middle. Right? So how are we going to solve the search task using using this knowledge graph? That brings me to this part. And, we obviously, we need some sort of API, right, that goes to the website. And but what happens before that is what we call relevance models. And I'll explain to you why we call it relevance models. Because what it has to do with for us is actually three type of queries. And if you're familiar with with graph data, you're familiar with the first Right? So structured graph queries, for example, give me all the artworks, and and sort them by by the production date. Now that's a very valid question. But in applications like this, there's usually a full text component as well. So what about the documents that mentioned for folks health? And in fact, in most cases, they combine these things, right? So there is structured and unstructured information combined, right? So in particular, this unstructured part, we're starting to deal with relevance, right, aboutness. It's not just true or false. And what I like to show you is how we solve these things in SpeedDesk. So what we're trying to do is to really allow people to create these relevance models themselves. So what you should see now is, is being desk, which should be run as a as a cloud service. And, here, the the full API of the of the application is, shown here. This is basically the whole API, of the application. And for each endpoint, they are implemented as what we call strategies or in other words, relevance models. And what I want to do with you is to actually create one. Not exactly what we use in these applications, but just to illustrate how we do that. And we do that in a visual programming interface. So here on the left side, we have a library of building blocks. And what we can do is we can drag those blocks onto the convos, and there we can start creating relevance models. So to start with, I'm just gonna take all the data that we have. So basically you can say this is this is the graph that we have. And what I want to do here is I want to make a search engine for all the letters that Verkhof wrote. So I'm not gonna take all the data, but actually dragged in another block. And I'm going to filter my data on messages. So as you notice here, these are modeled with schema dot org. So I select the messages. Now I can inspect the output here of this block. And we see here on the right side information. Let me increase this a little bit. So here are, the number of letters. Now this is not a lot. This is nine hundred three. But let's let's create a search engine for this. For example, we're using the text of those letters. So what I can do is I can actually extract this text. There we go. And I configured this to actually take the text that we want to use. And now we can say, let's build a document search engine for this. So this is a particular relevance model. And in fact, this is actually the model that is used by by most, off the shelf search engine systems like Solr, Elasticsearch. So, basically, what we're doing, we're building that here on the spot. So we just miss one component here, namely the input, And let's say for now that we want to search for mental health. Maybe we configure our search engine a bit. I know that we're talking about English here, so let's apply some stemming. And now the magic happens because when I start inspecting the outputs, what the system does for you is actually create the indexes that are required. Right? So and in this way, you as a user don't need to think about what are the the technical aspects that I need to realize, what kind of index do I need. But you can think at a much higher abstraction level, what should my relevance model actually look like. And what you see now that we actually find all kinds of results, and these are ranked as you can see here in this probability distribution. And at the beginning we see here, maybe we're not maybe we're actually not gonna expect these results, but what if we start analyzing them? So we go sort of back to our graph. And let's say now we're interested in which periods these letters were written. So I'm going to extract actually the date that they were sent. There we go. And here we are at eighteen eighty nine and eighteen eighty eight is the most relevant years. Now it turns out that actually cut off his ear, in December eighty eight. So it's not surprising that around this time, he was writing mostly about about this mental health. We can also do something else and say, hey, let's now look to something completely different. So from those letters, let's move to the artworks. And the artworks that he has mentioned in those letters. So what are actually the artworks most involved in this whole topic of mental health health? Now this is probably a bit difficult to read for you. On the website they're nicely formatted. But here you see that painting of Madame Goulain, which was actually painted at the end of eighty eight. And here's another painting of of the sunflowers all sort of in that that period. Now I hope this gave you an idea how you can play with these things, create relevance models over over both the text and the structured structured part. If I am correct, I have, like, one minute left. So let's briefly then look at how we are doing this. So if you're sort of in the in the knowledge graph domain, you're you're familiar with this picture. Right? So what we have at the bottom, oh, sorry. At the top, the top of the iceberg is some sort of API that is used by, by an application. Underneath that, there is some sort of graph database and and a graph query language. And what's what we find very interesting is to look at what are now the rules that are needed to solve these things. Right? We need some developer to create these things. And this is typically, yeah, just below the surface, just above the surface. Now if you need full tech support, then typically this is added on the site. And although there are solutions here, it's not very easy to combine these things. And what you need of course is someone that now has knowledge of both these things. So what we're trying to do, we actually push the technology all the way down. So at the lowest level, we actually implemented these things in one system, which is a probabilistic graph database. On top of that, we have a probabilistic relation to algebra that can deal with these things. And if you're interested in that, you can read about that in the literature . Oh, sorry about that. But most importantly for us is that all of this is now under the surface. But above the surface, we now have a way that we can support information specialists to deal with accessing knowledge graphs and text in the same same way to create relevance models that are needed for, for an application. And that is what I just just showed to you. So to wrap this up, a lot of knowledge potential in corporates organizations. But for that, they really need to understand the users. They need to capture their knowledge and finally model the relevance that is needed for the applications. Great. Thanks. Thanks, Miguel. Well, great, great slides. We have some some questions from, people, but, you know, I'll seamlessly take advantage of my position here, and I'll I'll make the the first question because, well, you you triggered my interest with something you said. So you mentioned you're using a probabilistic graph database. I'm supposed to be the expert on graph databases, but I've never heard about it. Would you like to say, like, a couple of words of What it is? Who built it? How it works? Yeah. So so the work originated actually in database or actually to be more precise is the merger of two fields, relational databases and information retrieval. At the end of the 1990s, work was done there. And And basically what it means is adding a score column to, to a triple or quadruple. So we have lifted it from the database world to, to the graph, world. So, where we add scores. And now what do you do with these scores? The interesting thing is that the research that I showed you there, there are various there is an algebra to deal with these scores. Right? How how to combine them, how to add them, how to multiply them for for different operations. So at the lowest level, there is actually graph scores, but more importantly is at the top level is then how you how you use those scores for certain operations. Okay. Okay. Well, yeah, you you provided the reference, and I guess, I or anyone else who's interested will will have to look it up. But, yeah, it's it's a very interesting concept. And, let's see what people have been asking. K. Another question is, which is very specific I guess, what is the rank by text block? And I get it's tied to one specific slide you had. Yeah, yeah. So I use the rank by text or more specifically, I use the rank by BN twenty five, which is a relevance model. And so so this is used by Elasticsearch as well. And, what it does, it implements this for for you. Right? So the the actual query that you need, is implemented in that path. So basically, you can grab whatever entities you have in your Knowledge Graph and say, I need this text and sort of throw it in Elasticsearch as you want. But the nice thing is you don't need to because the system does it for you. So we actually create the indexes that you need to create that search engine. So it all runs into the same system. Okay. Well, like many people are saying in in the comments, that sounds really, really cool, actually. Another question and again from from me, you mentioned that certain vocabulary at some point linked the art, if I recall, And you said it's kind of good, sweet spot, middle ground, between, different expressivity, methods. And you also mentioned that it allows you it's kind of it allows to deal with vagueness and specifically vagueness in dates. So would you like to expand a little bit on the expressivity part and then maybe specifically on the on the vagueness? Yeah. Yeah. So what I specifically said is that it finds the right sweet spot between expressivity and ease of use. And so you could almost position it between, schema dot org, which is relatively easy to use. But for something rich and heterogeneous as we needed it for cohort wide, it was it was just not enough. And at the other hand, there is something known as CDOC CRM, which is an event based model, which allows you to model a lot of stuff. But, to figure out how to use it is a bit complicated. So what Linked Art has done is what they call an application profile. So under the hood, they actually use Cedarc Serum, the user classes, the user relations, the actual stuff. But they created what they call an application profile in JSON LD. So you can actually write it down quite simple in simple terms. And this is a community effort. So it's full of documentations, the patterns that you need come from actual use cases from actual institutions. And, we had we had some discussion with them. There were a couple of things that that we needed and things were changed. So this is still ongoing, but this really suited our needs. So if that would not have been that this this would be quite a quite a drama to to do. So, so that that's that sweet spot. And the other question, yeah, about the dates. And so in this domain, people say, late late autumn, eighteen eighty eight or October nineteen hundred. And what it allows you to do is basically define the vagueness, of a period. So there are things like the begin of the begin, the end of the begin, the beginning of the end, and the end of the end. And so you describe vagueness of periods where the where but you use actual excess dates. So for a system, you now have proper dates that you can use. They might not be the exact date, but at least you can, for example, sort on something. And typically this is accompanied then by textual description, something like late autumn eighty eight, which you can display in the interface. So it sort of gives you the best of both worlds. Okay. Yeah. I don't know. I mean, both of those things that you described, well, the, the way that, people have worked with elite, art vocabulary to me sounds, you know, exemplary, basically. So making it easy for, developers to use, by by leveraging JSONRT and doing these different profiles that you mentioned. And then on the dates, this is, like, a, you know, notoriously difficult issue, how to manage overlapping dates and even more back dates. So if they manage to to address that, that's that's super cool. Yeah. Yeah. Of course, they need to create this data. Right? And, what you saw in this project is that the the larger institutions like the Hoch Museum, the Rijksmuseum, they were doing these things themselves. So they have their own pipelines for creative data mappings. For smaller institutions, we actually help them with it. I mean, smaller in sense of for whole, right? So people providing like ten artworks. So what I didn't show you that in Spring Desk, there is a part where you can actually create those mappings as well. So we help them with that part. Okay. Let's see. I think we may have one last question. I'm getting to actually show. Says, technically, using the SPYNC tool requires to upload data into a proprietary database. Yes. I mean, we're coming a bit from the search area. So often we actually call it an index. Yeah. Right. So it's sort of a layer on top of your data. So we're not the data management system. We interface or we connect with existing systems. So, yeah, but that is required. So we're not doing federated search. That that that that is clearly, something different. Okay. Yeah. That's that's that's clear. Alright. Well, thanks, thanks once again, to you, Michiel, and to everyone who attended. We'll have to wrap up here so we can move to the next session. So, see you there, and see you also in the next session. Bye for now.",
    "transcript_length": 28191,
    "speaker": "Michiel Hildebrand",
    "tags": [
      "goulain",
      "michel",
      "mihir",
      "van gogh",
      "verhoeff",
      "vincent"
    ]
  }
]